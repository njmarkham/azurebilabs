# Module 6: Implementing Custom Operations and Monitoring Performance in Azure Data Lake Analytics

# Lab: Implementing custom operations and monitoring performance in Azure Data Lake Analytics

### Scenario

You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For this phase of the project, you are going to use ADLA to perform a range of analyses on data that has been captured and saved into the traffic surveillance system by using tools such as Azure Stream Analytics.

### Objectives

After completing this lab, you will be able to:
-   Use a custom extractor, read JSON file data, and use a custom outputter to XML.
-   Optimize a table in the ADLA catalog.
-   Implement a custom processor in ADLA.
-   Use existing analytics, developed in R, in an ADLA solution.

### Lab Setup
Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Use a custom extractor, read JSON file data, and use a custom outputter to XML

### Scenario

The Azure Stream Analytics job that you worked with in previous labs saves speed camera data to ADLS in JSON format. You want to be able to read and analyze this data using ADLA. You also want to be able to write a summary of the data to XML files for use by other tools.

In this exercise, you will create and use a custom extractor, to read JSON file data into ADLA, and a custom formatter to output data to XML.

The main tasks for this exercise are as follows:
1. Deploy and test a JSON extractor
2. Deploy and test an XML outputter

#### Task 1: Deploy and test a JSON extractor
1.  Use Visual Studio to open the **CustomFunctions** solution in the **E:\\Labfiles\\Lab06\\Exercise1\\Starter\\CustomFunctions** folder.
2.  Open **JsonExtractor.cs**, and browse the code, noting the following points:
    1.  The JsonExtractor class extends the IExtractor base class.
    2.  The Extract function is an override; this override uses functions in the Newtonsoft JSON library to read the JSON data from the input line (this contains a single line from the input file).
    3.  After the JSON data has been retrieved, and the fields in the input line extracted into an output row, this output row is returned to USQL.
    4.  The class is tagged with the [SqlUserDefinedExtractor(AtomicFileProcessing = true)] attribute; this is because the input data consists of a single object (a JSON array), and so its contents cannot be read by separate vertices in parallel.
    5.  The constructor specifies the name of the array to parse in the input; this is because there could possibly be multiple arrays in the same JSON file.
    6.  This class contains many other supporting functions that do all the parsing, and so on; you should ignore these functions for the purposes of this exercise.
3.  Rebuild the solution.
4.  Register an assembly for the **CustomExtractors** project.
5.  Before submitting the **Assembly Registration**, do the following:
	1.  Set the ADLA Account to the name of your ADLA account in Azure (speedsdla&lt;your name&gt;&lt;date&gt;).
	2.  Leave the Database set to master.
	3.  Select "Replace assembly if it already exists".
	4.  Expand Managed Dependencies, and select Newtonsoft.Json (this is the assembly containing the JSON Newtonsoft library).
	5.  Using Wordpad, open the E:\\Labfiles\\Lab06\\Exercise1\\SpeedData.json file.
	6.  Note that this file contains speed camera data formatted as a JSON array; there are more than a million records in the file, which is why you are not using Notepad to view it, because it would take too long to load.
6.  Using the Azure portal, upload the **SpeedData.json** file to the root folder of the ADLS account (overwrite any existing file with the same name). This file represents a JSON array as written by the ASA jobs from earlier labs.
7.  Open another instance of Visual Studio 2017, and create a new **USQL** project called **JsonExtractorTest** to test the extractor.
8.  In the **USQL** script, add the following statements (you copy this code from **E:\\Labfiles\\Lab06\\Exercise1\\JsonExtractorTest1.usql**):
	```
	// Assembly references for the JSON extractor and Newtonsoft assembly (required by the extractor) stored in the catalog
	REFERENCE ASSEMBLY [Newtonsoft.Json];
	REFERENCE ASSEMBLY [CustomExtractors];

    // Use the Json extractor to read the data from the SpeedData.json file. The fields of interest in the file are CameraID string, SpeedLimit int, Speed int, VehicleRegistration string, Date string (there is also the Month field that you can ignore for this lab):
	@speedData =
	EXTRACT CameraID string, SpeedLimit int, Speed int, VehicleRegistration string, Date string
	FROM "/SpeedData.json"
	USING new CustomExtractors.JsonExtractor();

    // Save the data to a CSV file named ConvertedSpeedData.csv:
	OUTPUT @speedData
	TO "/ConvertedSpeedData.csv"
	USING Outputters.Csv(quoting: false, outputHeader: true);
	```
9.  Run the USQL script using your ADLA account in Azure.
10.  Examine the **ConvertedSpeedData.csv** file by using Data Explorer in the Azure portal.

#### Task 2: Deploy and test an XML outputter
1.  Return to the instance of Visual Studio that displays the **CustomFunctions** solution.
2.  Open **XmlOutputter.cs**, and note the following points:
    1.  The SimpleXMLOutputter class extends the IOutputter base class.
    2.  The Output function is an override that formats a line of data as a simple XML object; the object tag to use is specified by the constructor.
    3.  The class is tagged with the [SqlUserDefinedOutputter(AtomicFileProcessing = false)] attribute; each line of output is a separate XML document, so the output is generated by using multiple vertices running in parallel.
    4.  The Close method flushes and closes the output stream.
3.  Rebuild the solution.
4.  Register an assembly for the **CustomOutputters** project.
5.  Before submitting the **Assembly Registration**, do the following:
    1.  Set the **ADLA Account** to **speedsdla*&lt;your name&gt;&lt;date&gt;***.
    2.  Leave the **Database** set to **master**.
    3.  Select the **Replace assembly if it already exists** check box.
6.  Switch to the instance of Visual Studio 2017 that displays the **JsonExtractorTest** solution.
7.  Switch to the instance of Visual Studio 2017 that has the **JsonExtractorTest** solution open.
8.  In the **USQL** script, add the following statements to the end of the script (you copy this code from **E:\\Labfiles\\Lab06\\Exercise1\\JsonExtractorTest2.usql**):
	```
	// Assembly reference for the XML outputter
	REFERENCE ASSEMBLY [CustomOutputters];

    // Modify the statement that outputs the data to use the XmlOutputter. Save the data as ConvertedSpeedData.xml
	OUTPUT @speedData
	TO "/ConvertedSpeedData.xml"
	USING new CustomOutputters.XmlOutputter();
	```
9.  Run the **USQL** script using your ADLA account in Azure.
10.  Using Data Explorer in the Azure portal, download the **ConvertedSpeedData.xml** file, and then examine it using Visual Studio 2017.
    **Note**: If you try to view the data using Data Explorer, the data will not be displayed correctly because Data Explorer does not understand the XML format.

>**Result**: At the end of this exercise, you will have deployed and tested a custom extractor, and deployed and tested a custom outputter.

## Exercise 2: Optimize a table in the ADLA catalog

### Scenario

You want to save speed camera data to the ADLA catalog, but need to ensure that the data is structured to optimize the analytics that your ADLA jobs perform. The most common types of query that are currently run are primarily focused on identifying patterns related to speed camera locations, such as average speeds at each camera location. Therefore, you want to ensure that tables in the ADLA catalog are optimized for such queries.

In this exercise, you will first create a table in the ADLA catalog, then use a U-SQL job to analyze data at a specific camera location; finally, you will redistribute the data in the catalog to optimize query performance.

The main tasks for this exercise are as follows:
1. Create the SpeedData table in the catalog
2. Create a U-SQL job that analyzes data for a specific camera
3. Redistribute the data to optimize data retrieval

#### Task 1: Create the SpeedData table in the catalog
1.  Using Visual Studio, create a new USQL project named **SpeedCameraAnalytics2**.
2.  Change the name of the **Script.usql** file to **CreateTable.usql**, and in the **CreateTable.usql** script, add the following statement to create a new database called **VehicleData** (if it doesn't already exist); you copy the code for steps 2 to 5 from **E:\\Labfiles\\Lab06\\Exercise2\\CreateTable1.usql**:
	```
	CREATE DATABASE IF NOT EXISTS VehicleData;
	```
3.  In CreateTable.usql, add the following statements to create a table for holding speed camera information, and to index the data by camera id, and hash the data by the vehicle registration:
	```
	DROP TABLE IF EXISTS VehicleData.dbo.SpeedCameraData;
	CREATE TABLE VehicleData.dbo.SpeedCameraData(
	CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	Date DateTime,
	INDEX cameraidx
	CLUSTERED(CameraID ASC)
	DISTRIBUTED BY
	HASH(VehicleRegistration)
	);
	```
Note that the rationale behind this hashing strategy is to distribute the load across the database, to try and avoid hotspots in the data. The index is intended to support fast lookup of speed data by camera ID.
4.  In CreateTable.usql, add the following statements to reference the assemblies required by the JSON extractor:
	```
	REFERENCE ASSEMBLY [Newtonsoft.Json];
	REFERENCE ASSEMBLY [CustomExtractors];
	```
5.  In CreateTable.usql, add the following statements to read the data from the SpeedData.json file:
	```
	@speedData =
	EXTRACT CameraID string, SpeedLimit int, Speed int, VehicleRegistration string, Date string
	FROM "/SpeedData.json"
	USING new CustomExtractors.JsonExtractor();
	```
6.  The dates in the JSON input are strings in the format "dd/mm/yy hh24:mi"; therefore, you need to add a user-defined function to convert a string in this format into a DateTime object.
7.  Edit the code-behind C\# file, by replacing the entire contents of **CreateTable.usql.cs** with the following code (you copy this code from **E:\\Labfiles\\Lab06\\Exercise2\\CreateTable.usql.cs**):
	```
	using System;
	namespace SpeedCameraAnalytics2
	{
	public class UDFs
	{
	public static DateTime ConvertStringToDate(string date)
	{
	// The input string is in the form "dd/mm/yyyy hh24:mi"
	char[] delimiters = { ' ', '/', ':' };
	string[] dateBits = date.Split(delimiters);
	int day = Convert.ToInt32(dateBits[0]);
	int month = Convert.ToInt32(dateBits[1]);
	int year = Convert.ToInt32(dateBits[2]);
	int hour = Convert.ToInt32(dateBits[3]);
	int minute = Convert.ToInt32(dateBits[4]);
	DateTime dt = new DateTime(year, month, day, hour, minute, 0);
	return dt;
	}
	}
	}
	```
This code defines a function called ConvertStringToDate; this function parses the input string, and uses it to construct a DateTime object.
8.  In the **CreateTable.usql** script, add the following statement to the end of the existing code (you copy this code from **E:\\Labfiles\\Lab06\\Exercise2\\CreateTable2.usql**):
	```
	INSERT INTO VehicleData.dbo.SpeedCameraData (CameraID, VehicleRegistration, Speed, SpeedLimit, Date)
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, SpeedCameraAnalytics2.UDFs.ConvertStringToDate(Date) AS Date
	FROM @speedData;
	```
This statement inserts the data read by using the JSON extractor into the SpeedCameraData table; notice that this statement calls the ConvertStringToDate function for each row of input.
9.  Run the USQL script using your ADLA account in Azure.
10.  Switch to the Azure portal, and go to the **speedsdla*&lt;your name&gt;&lt;date&gt; ***blade.
11.  Go to Job Management, and open the topmost job (it will probably be marked with a status of Preparing or Queued, but if you have not been too quick to get to this step, it could be marked as Running).
12.  Wait while the job runs; when it has finished, with a status of Succeeded, open the SpeedCameraData table at the bottom of the graph, and then submit the command to query the table.
13.  Wait while the query job runs; when it has completed, open the **VehicleData.dbo.SpeedCameraData.tsv** file shown at the bottom of the graph.
14.  Browse the data; you should see rows for Camera 0 (the browser only shows the first few rows), and the data is not in any specific order.

#### Task 2: Create a U-SQL job that analyzes data for a specific camera
1.  Using Visual Studio, add a new U-SQL script called **AnalyzeSpeedsByCamera.usql** to the **SpeedCameraAnalytics** project.
2.  In AnalyzeSpeedsByCamera.usql, add the following statements to perform a query and generate a summary for Camera 121 (you copy this code from **E:\\Labfiles\\Lab06\\Exercise2\\AnalyzeSpeedsByCamera.usql**):
	```
	// Find the statistics for a specific camera
	DECLARE @camera = "Camera 121";

    @speedSummary =
	SELECT CameraID,
	MAX(SpeedLimit) AS SpeedLimit,
	COUNT(*) AS NumberOfObservations,
	MIN(Speed) AS Lowest,
	MAX(Speed) AS Highest,
	AVG(Speed) AS Average
	FROM VehicleData.dbo.SpeedCameraData
	WHERE CameraID == @camera
	GROUP BY CameraID;

    // Save the results to SpeedSummary.csv::
	OUTPUT @speedSummary
	TO "/SpeedSummary.csv"
	USING Outputters.Csv(outputHeader: true, quoting: false);
	```
3.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***.
4.  In the **Job View** pane, examine the SV1 Extract stage. It should report that it is running 10 vertices and reading 40 MB of data. It outputs 10 rows (there is one row of summary data generated by each vertex). These 10 rows are passed to the SV2 Aggregate stage, which combines them together to produce a single row.
5.  Open the **SpeedSummary.csv** file to see the following results:
	```
	CameraID: Camera 121
	SpeedLimit: 30
	Number of Observations: 6057
	Lowest: 0
	Highest: 116
	Average: 7
	```
6.  Load the job profile, and open the **Vertex Execution View** for the SV1 Extract stage. The job was run with the default number of AUs (5), and you can see how the job was performed, with five vertices running first, and the next five running as the first five completed. The vertex for the Aggregate stage ran when the vertices for the Extract stage had all finished.
7.  Open the **Stage Scatter View** for the SV1 Extract stage.
8.  In the Stage Scatter Chart, clear the Output in the legend to focus on the volume of data and time spent performing input. Each vertex read approximately the same amount of data (about 4 MB), but the time spent is grouped into two distinct sets. This is because the second five vertices had to wait for a vertex to become available while the first five were running.
9.  Open the Vertex Operator View for the SV1 Extract stage.
10.  This view shows the logical data flow in terms of fields and numbers of rows. The extractor read 6,057 rows but, because the data is spread across the database, it created a separate vertex to handle the rows from each chunk of data that it read. Each vertex (Process\_1 in the graph) performs the aggregation over the data from that chunk, and sends the results to the second stage (Process\_2). Process\_2 does the final aggregation and sends the results to the output.

#### Task 3: Redistribute the data to optimize data retrieval
One way to optimize the data would be to partition it by camera ID. However, there are currently 500 cameras in this dataset, so this process would involve creating 500 partitions, which is probably not feasible (note that U-SQL only supports partitioning by ID, and does not support user-defined partitioning functions that are available in SQL Server). Another solution is to distribute the data by Camera ID rather than VehicleRegistration. In this way, the observations for a specific camera should be grouped close together in the database, and will hopefully reduce the amount of I/O that the analysis needs to perform.
1.  In Visual Studio, return to the **CreateTable.usql** job, and in the **CreateTable.usql** script, edit the CREATE TABLE command to distribute data by hashing the CameraID; replace the existing DISTRIBUTED BY HASH(VehicleRegistration) string with the following:
	```
	DISTRIBUTED BY
	HASH(CameraID)
	```
2.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***. Verify that the script completes successfully before continuing.
3.  Switch to the **AnalyzeSpeedsByCamera.usql** job.
4.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***.
5.  When the script has completed, examine the graph in the **Job View** pane. You should now see that the job comprised a single stage that required only a single vertex. This stage read 3.93 MB of data (rather than 10 stages each reading nearly 4 MB each) and, because the data was all available in the same "chunk", it could be processed and aggregated in a single stage.
6.  Open the **SpeedSummary.csv** file and verify that the results are the same as before:
	```
	CameraID: Camera 121
	SpeedLimit: 30
	Number of Observations: 6057
	Lowest: 0
	Highest: 116
	Average: 7
	```
>**Result**: At the end of this exercise, you will have created a table in the ADLA catalog, analyzed data in this table, and redistributed data in the catalog for optimal retrieval.

## Exercise 3: Implement a custom processor in ADLA

### Scenario

You need to find the proportion of stolen vehicles passing each speed camera; this will then help you identify the locations that are likely to be "criminal" hotspots, where incidences of vehicle theft are particularly high.

You have stolen vehicle data covering eight years, organized in folders by year/month/day, with a total of 2,914 separate CSV files. The data contains the vehicle registration, date stolen, and date recovered (which could be empty). The same vehicle could be reported stolen and recovered several times in these records. Additionally, the police aren't always informed when a vehicle is recovered, so some records might have an empty date recovered, even if the vehicle is no longer missing. This means that a vehicle could be reported as stolen on several dates, but not recovered in the intervening period. The vehicle might be recovered later. Therefore, to determine whether a vehicle should be considered as stolen, you need to look at the most recent record in the history. If the vehicle has a recovery date, it is not missing, regardless of what any previous records might infer. If no recovery date is shown on this record, the date stolen should be used.

Because of these peculiarities in the data concerning stolen and recovery dates, it’s difficult to process this data using regular U-SQL operators; you will, therefore, use a custom reducer that performs the necessary magic.

In this exercise, you will first upload the dataset to ADLS. You will deploy and test the custom reducer, and then perform some analyses to identify stolen vehicles in the speed camera data.

The main tasks for this exercise are as follows:
1. Preparation: upload stolen vehicle data to ADLS (using AzCopy and Adlcopy)
2. Examine and deploy a custom reducer
3. Test the custom reducer
4. Analyze the speed camera data to check for stolen vehicles

#### Task 1: Preparation: upload stolen vehicle data to ADLS (using AzCopy and Adlcopy)
1.  Use the Azure portal to create a new Blob storage account, using the following details:
 -   **Name**: vehicledata*&lt;your name&gt;&lt;date&gt;*
 -   **Resource group (use existing)**: CamerasRG
 -   **Location**: select the same location as you used for the data warehouse in Exercise 1
 -   Leave all other details at their defaults
2.  Wait until the storage account has been successfully created before continuing with the exercise.
3.  Add a new blob container named **stolen**.
4.  Make a note of the storage access key for the storage account.
5.  View the **E:\\Labfiles\\Lab06\\Exercise3\\StolenVehicles** folder, and verify that it contains eight years of stolen vehicle data, organized in subfolders by year/month/day; there are 2,914 separate CSV files.
6.  Use the AzCopy command to upload the files and folders under the **E:\\Labfiles\\Lab06\\Exercise3\\StolenVehicles** folder to the blob container. Use the **/S** parameter to indicate that AzCopy should recursively traverse subfolders; you copy the command from **E:\\Labfiles\\Lab06\\Exercise3\\AzCopyCmd1.txt** (replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the key you noted previously:
	```
	azcopy /Source:"E:\Labfiles\Lab06\Exercise3\StolenVehicles" /Dest:https://<storage account name>.blob.core.windows.net/stolen /DestKey:<storage key> /S
	```
7.  The copy process might take several minutes to complete; wait until all files have been copied before continuing with the exercise.
8.  Use Cloud Explorer in Visual Studio to examine the stolen blob container.
9.  In Cloud Explorer, if you do not have an Azure Pass folder, you will need to add the Microsoft account that is associated with your Azure Learning Pass subscription.
10.  Verify that the files and subfolders have been uploaded.
11.  Use the Azure portal to create a new folder called **Stolen** in your Data Lake Store (adls&lt;your name&gt;&lt;date&gt;).
12.  Use the AdlCopy command to transfer the files from Blob storage to the Stolen folder in your Data Lake Store; you copy the following command from **E:\\Labfiles\\Lab06\\Exercise3\\AdlCopyCmd.txt** (replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, replacing **&lt;Data Lake Store name&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the blob store key you noted previously):
	```
	adlcopy /source https://<storage account name>.blob.core.windows.net/stolen/ /dest adl://<Data Lake Store name>.azuredatalakestore.net/Stolen/ /sourcekey <storage key>
	```
13.  If you get a **Visual Studio** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
14.  Note: Depending on the region and the location of the Blob storage account (ideally they should be in the same region, but might not be), AdlCopy will take from two to 20 minutes to copy the data. Ignore the stats that indicate the percentage of files copied—it sits at 0.00 percent until complete then jumps to 100 percent, and might copy the data in three phases (files 1 to 1,000, then files 1,001 to 2,000, and then the remainder).
15.  Use Data Explorer in the Azure portal to examine the Data Lake Store and verify that all the files and folders have been copied across.

#### Task 2: Examine and deploy a custom reducer
1.  Using Visual Studio, open the **CustomReducers** solution in the **E:\\Lab06\\Exercise3\\Starter\\CustomReducers** folder.
2.  Open **StolenVehicleReducer.cs**, and note the following points:
    1.  The ReduceByRecoveredVehicles class extends the IReducer base class.
    2.  The class is tagged with the [SqlUserDefinedReducer(IsRecursive = true)] attribute; you can reduce each group of data in parallel.
    3.  The Reduce function is an override that examines each row of input; the input contains a set of stolen vehicle records (VehicleRegistration, DateStolen, DateRecovered) that will be grouped by registration (you will specify this when you call the reducer from the U-SQL job).
    4.  The Reduce function only outputs rows for vehicles that it considers to be still missing (the most recent record does not have a recovery date).
3.  Rebuild the solution.
4.  Register an assembly for the **CustomReducers** project.
5.  Before submitting the Assembly Registration, do the following:
    1.  Set the **ADLA Account** to **speedsdla*&lt;your name&gt;&lt;date&gt;***.
    2.  Leave the **Database** set to **master**.
    3.  Select the **Replace assembly if it already exists** check box.

#### Task 3: Test the custom reducer
1.  In the instance of Visual Studio 2017 that has the **SpeedCameraAnalytics2** solution open, add a new U-SQL script called **AnalyzeSpeedsByVehicle.usql**.
2.  In AnalyzeSpeedsByVehicle.usql, add the following statements (you copy this code from **E:\\Labfiles\\Lab06\\Exercise3\\AnalyzeSpeedsByVehicle1.usql**):
	```
	// Assembly references for the reducer:

    REFERENCE ASSEMBLY CustomReducers;

    // Use a CSV extractor to read the stolen vehicle data from the files under the Stolen folder in ADLS file; the fields of interest in the file are VehicleRegistration string, DateStolen string, and DateRecovered string:

    @stolenVehicleHistory =
	EXTRACT VehicleRegistration string,
	DateStolen string,
	DateRecovered string // Can be empty
	FROM "/Stolen/{*}/{*}/{*}/VehicleData.csv"
	USING Extractors.Csv(skipFirstNRows: 1);

    // Call the reducer to reduce the rowset to only those vehicles that are currently marked as stolen; group the data by VehicleRegistration as it is passed to the reducer:

    @stolenVehicles =
	REDUCE @stolenVehicleHistory
	ON VehicleRegistration
	PRODUCE VehicleRegistration string,
	DateStolen DateTime
	USING new CustomReducers.ReduceByRecoveredVehicles();

    // Save the results to StolenVehicleSpeedAnalysis.csv:

    OUTPUT @stolenVehicles
	TO "/StolenVehicleSpeedAnalysis.csv"
	USING Outputters.Csv(quoting : false, outputHeader : true);
	```
3.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***.
4.  Examine the **StolenVehicleSpeedAnalysis.csv** file by using Data Explorer in the Azure portal (if you download the file, it should contain 241,244 records, plus the header). The data will be sorted by VehicleRegistration as a by-product of the reduction process.

#### Task 4: Analyze the speed camera data to check for stolen vehicles
1.  In the instance of Visual Studio 2017 editing the **SpeedCameraAnalytics2** project, in **AnalyzeSpeedsByVehicle.usql**, before the OUTPUT statement, add the following SELECT statement to join the data in the **VehicleData.dbo.SpeedCameraData** table in the catalog with the stolen vehicle data returned by the reducer across the VehicleRegistration column. Count the number of rows for each camera (you copy this code from **E:\\Labfiles\\Lab06\\Exercise3\\AnalyzeSpeedsByVehicle2.usql**):
	```
	@stolenVehicleAnalysis =
	SELECT C.CameraID,
	COUNT(C.VehicleRegistration) AS NumStolenVehicles
	FROM VehicleData.dbo.SpeedCameraData AS C
	JOIN
	@stolenVehicles AS V
	ON C.VehicleRegistration == V.VehicleRegistration
	GROUP BY C.CameraID;
	```
2.  In AnalyzeSpeedsByVehicle.usql, before the OUTPUT statement, add another SELECT statement that simply finds the total number of vehicles that have passed each camera (you copy this code from **E:\\Labfiles\\Lab06\\Exercise3\\AnalyzeSpeedsByVehicle3.usql**):
	```
	@vehicleAnalysis =
	SELECT CameraID,
	COUNT(VehicleRegistration) AS NumVehicles
	FROM VehicleData.dbo.SpeedCameraData
	GROUP BY CameraID;
	```
3.  In AnalyzeSpeedsByVehicle.usql, before the OUTPUT statement, join the results of the previous two SELECT statements across the CameraID column, and calculate the percentage of cars passing each camera that are stolen (you copy this code from **E:\\Labfiles\\Lab06\\Exercise3\\AnalyzeSpeedsByVehicle4.usql**):
	```
	@speedCameraAnalysis =
	SELECT C.CameraID,
	C.NumVehicles,
	V.NumStolenVehicles,
	((double)V.NumStolenVehicles / C.NumVehicles) * 100 AS PercentStolen
	FROM @vehicleAnalysis AS C
	JOIN
	@stolenVehicleAnalysis AS V
	ON C.CameraID == V.CameraID;
	```
4.  In AnalyzeSpeedsByVehicle.usql, replace the existing OUTPUT statement to save these results, and sort the data in descending order of PercentStolen (you copy this code from **E:\\Labfiles\\Lab06\\Exercise3\\AnalyzeSpeedsByVehicle5.usql**):
	```
	OUTPUT @speedCameraAnalysis
	TO "/StolenVehicleSpeedAnalysis.csv"
	ORDER BY PercentStolen DESC
	USING Outputters.Csv(quoting : false, outputHeader : true);
	```
5.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***.
6.  Examine the **StolenVehicleSpeedAnalysis.csv** file by using Data Explorer in the Azure portal, and note that the data is now sorted in descending order of PercentStolen.
7.  This time, if you download the file, it should contain 500 records (one record for each camera), plus the header; the data should indicate that between 2.45 percent and 3.76 percent of all cars passing speed cameras are stolen.

>**Result**: At the end of this exercise, you will have uploaded speed camera data to ADLS, examined the code in the custom reducer, deployed and tested the custom reducer, and then used the reducer to attempt to identify stolen vehicles in the speed camera data.

## Exercise 4: Use existing analytics, developed in R, in an ADLA solution

### Scenario

You want to determine if there is any correlation between whether a vehicle is caught speeding and it possibly being stolen. You are familiar with R, and want to use an analytics package provided by either of these languages to help with this analysis.

In this exercise, you will call an R script from a U-SQL job to determine if there is any correlation between a vehicle being identified as speeding and being identified as being stolen; you will then repeat this analysis using a Python script in a U-SQL job.

The main tasks for this exercise are as follows:
1. Determining correlations by using R

#### Task 1: Determining correlations by using R
1.  Install the U-SQL Extensions to **speedsdla*&lt;your name&gt;&lt;date&gt;***.
2.  Using Notepad, examine the R code in the script **E:\\LabFiles\\Lab06\\Exercise4\\SpeedAnalytics.R**. This is an R script that uses the ScaleR package to tidy up the input data, calculate the correlation between a vehicle speeding and being stolen, and returns a data frame containing the data from the correlation matrix. Remember that the reducer will call this code once for each camera (the reducer will group the data by camera ID).
3.  Upload the R script to the root folder of **adls*&lt;your name&gt;&lt;date&gt;***.
4.  Add a new USQL script called **AnalyzeUsingR.usql** to the **SpeedCameraAnalytics** project.
5.  In AnalyzeUsingR.usql, add the following statements to add references to the ExtR and CustomReducers assemblies (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR1.usql**):
	```
	REFERENCE ASSEMBLY [ExtR];
	REFERENCE ASSEMBLY CustomReducers;
	```
6.  In AnalyzeUsingR.usql, add the following statements to retrieve the stolen vehicle history data, and use the ReduceByRecoveredVehicles to return only vehicles that are currently marked as stolen (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR2.usql**):
	```
	@stolenVehicleHistory =
	EXTRACT VehicleRegistration string,
	DateStolen string,
	DateRecovered string // Can be empty
	FROM "/Stolen/{*}/{*}/{*}/VehicleData.csv"
	USING Extractors.Csv(skipFirstNRows: 1);

    @stolenVehicles =
	REDUCE @stolenVehicleHistory
	ON VehicleRegistration
	PRODUCE VehicleRegistration string,
	DateStolen DateTime
	USING new CustomReducers.ReduceByRecoveredVehicles();
	```
7.  In AnalyzeUsingR.usql, add the following statement to fetch the speed camera data from the catalog (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR3.usql**):
	```
	@speedData =
	SELECT CameraID,
	VehicleRegistration,
	Speed,
	SpeedLimit
	FROM VehicleData.dbo.SpeedCameraData;
	```
8.  In AnalyzeUsingR.usql, add the following statements to combine the rowsets over the vehicle registration column, and to perform a LEFT join because you want to include all rows from the speed camera data, regardless of whether or not the vehicle snapped was speeding (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR4.usql**):
	```
	@sourceData =
	SELECT C.CameraID AS CameraID,
	C.VehicleRegistration AS VehicleRegistration,
	C.Speed AS Speed,
	C.SpeedLimit AS SpeedLimit,
	V.DateStolen.ToString() AS DateStolen
	FROM @speedData AS C
	LEFT JOIN
	@stolenVehicles AS V
	ON C.VehicleRegistration == V.VehicleRegistration;
	Note that this statement will cause the DateStolen column to have a null value in the resulting rowset for all cars that are not stolen.
	```
9.  In AnalyzeUsingR.usql, add the following statement to deploy the script containing the R code to be run (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR5.usql**):
	```
	DEPLOY RESOURCE @"/SpeedAnalytics.R";
	```
10.  In AnalyzeUsingR.usql, add the following statement to call the R script to generate a correlation matrix for each camera, showing whether there is any correlation between cars speeding and being stolen (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR6.usql**):
	```
	@RScriptOutput =
	REDUCE @sourceData
	ON CameraID
	PRODUCE CameraID string, Correlation double
	USING new Extension.R.Reducer(scriptFile:"SpeedAnalytics.R", rReturnType:"dataframe", stringsAsFactors:false);
	```
11.  In AnalyzeUsingR.usql, add the following statement to save the results (you copy this code from **E:\\Labfiles\\Lab06\\Exercise4\\AnalyzeUsingR7.usql**):
	```
	OUTPUT @RScriptOutput
	TO "/CorrelationMatrixData.csv"
	ORDER BY CameraID
	USING Outputters.Csv(quoting : false, outputHeader : true);
	```
12.  Run the USQL script using **speedsdla*&lt;your name&gt;&lt;date&gt;***.
13.  Examine the **CorrelationMatrixData.csv** file by using Data Explorer in the Azure portal. You should see that there appears to be little correlation between cars being stolen and being caught speeding. This is actually not surprising for this dataset because all of the data is generated randomly. However, there might be more of a correlation in the real world.

>**Result**: At the end of this exercise, you will have used an R script script in a U-SQL job.

**Question**: Why do you need to deploy your own custom extractor for JSON file data?

**Question**: Why is it important to optimize the table structure in your ADLA catalog?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
