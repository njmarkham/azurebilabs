# Module 5: Processing big data using Azure Data Lake Analytics

# Lab: Processing big data using Data Lake Analytics

## Exercise 1: Create and test a Data Lake Analytics job

#### Task 1: Prepare data files for a local Data Lake Analytics instance
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  On the taskbar, click **File Explorer**.
3.  In File Explorer, go to **E:\\Labfiles\\Lab05\\Exercise1**, and copy the **speeds** folder tree to the clipboard.
4.  In File Explorer, go to **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, and paste the speeds folder tree.
    Note that this folder contains sample speed camera data for three hours—each hour's data is in a separate folder.
5.  In File Explorer, go to **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot\\speeds\\2017\\08\\30\\16**, and double-click **speeds1**.
    Note that, for each hour, the speeds1 file contains the camera ID, vehicle registration, recorded speed, speed limit at the camera location, camera location (latitude and longitude), and time for each record.
6.  Close Microsoft Excel, without saving any changes.

#### Task 2: Create a new Data Lake project
1.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
2.  In Visual Studio, on the **File** menu, point to **New**, and then click **Project**.
3.  In the **New Project** dialog box, in the **Templates** list, expand **Azure Data Lake**, and then click **U-SQL (ADLA)**.
4.  In the **Name** box, type **CameraSpeeds**, and then click **OK**.
5.  In Solution Explorer, double-click **Script.usql**.
6.  In the **Script.usql** pane, type the following code (this code can be copied from **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob1.usql**):
	```
	@cameraData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime
	FROM "/speeds/2017/08/30/16/speeds1.csv"
	USING Extractors.Csv(skipFirstNRows: 1);

    @avgspeeds =
	SELECT CameraID, AVG(Speed) AS AvgSpeed
	FROM @cameraData
	GROUP BY CameraID;

    OUTPUT @avgspeeds
	TO "/AverageSpeeds.csv"
	USING Outputters.Csv(outputHeader: true);
	```
Note that, in the first statement (@cameraData), because the source data first line contains headers, you need a command to skip over it. In the second statement (@avgspeeds), the code creates a rowset that reads the file, and calculates the average speed reported by each speed camera. In the third statement (@avgspeeds), the code saves the results to a CSV file named AverageSpeeds.csv, and includes the column header data.

#### Task 3: Run a Data Lake Analytics job locally
1.  To submit the job using the (Local) ADLA account, click **Submit**.
2.  The job will open a command window; when the job has completed, note the **Success** status, and then press Spacebar to close the command window.
3.  Examine the simple job graph by hovering the mouse over the SV1 Extract step.
4.  In the **Local Run Results** pane, double-click **AverageSpeeds.csv**.
    Note that this file contains the data generated by the job; the file should contain 50 lines (one for each camera), displaying the camera ID and the average speed reported.

#### Task 4: Create a Data Lake Analytics account
1.  On the Start menu, type **Internet Explorer**, and then press Enter.
2.  In Internet Explorer, go to **http://portal.azure.com**, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
3.  In the Azure portal, click **+ New**, click **Data + Analytics**, and then click **Data Lake Analytics**.
4.  On the **New Data Lake Analytics Account** blade, in the **Name** box, type **speedsdla*&lt;your name&gt;&lt;date&gt;***.
5.  Under **Resource Group**, click **Use existing**, and then click **CamerasRG**.
6.  In the **Location** list, select the same location as you used for the Data Lake Store that you created in Lab 2.
7.  Click **Data Lake Store**.
8.  On the **Select Data Lake Store** blade, click **adls*&lt;your name&gt;&lt;date&gt;***.
9.  Leave all other settings at their defaults, and click **Create**.
10. Wait until the account has deployed before continuing with the lab.

#### Task 5: Prepare data files for a cloud Data Lake Analytics instance
1.  In the Azure portal, click **All resources**, and then click **adls*&lt;your name&gt;&lt;date&gt;***.
2.  On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **Data Explorer**, and then click **New Folder**.
3.  In the **Create new folder** dialog box, type **speeds**, and then click **OK**.
4.  On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **speeds**, and then click **New Folder**.
5.  In the **Create new folder** dialog box, type **2017**, and then click **OK**.
6.  On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **2017**, and then click **New Folder**.
7.  In the **Create new folder** dialog box, type **08**, and then click **OK**.
8.  On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **08**, and then click **New Folder**.
9.  In the **Create new folder** dialog box, type **30**, and then click **OK**.
10. On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **30**, and then click **New Folder**.
11. In the **Create new folder** dialog box, type **16**, and then click **OK**.
12. On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **New Folder**.
13. In the **Create new folder** dialog box, type **17**, and then click **OK**.
14. Repeat steps 12-13 to create a new folder called **18**.
15. On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, click **16**, and then click **Upload**.
16. On the **Upload files** blade, click the **file** button.
17. In the **Choose File to Upload** dialog box, go to **E:\\Labfiles\\Lab05\\Exercise1\\speeds\\2017\\08\\30\\16**, click **speeds1.csv**, and then click **Open**.
18. On the **Upload files** blade, click **Add selected files**.
19. When the file has uploaded, close the **Upload files** blade.
20. In the breadcrumb trail, click **30**.
21. Repeat steps 15-20 to upload the **speeds2.csv** and **speeds3.csv** files to folders **17** and **18** respectively.

#### Task 6: Run a Data Lake Analytics job in the cloud
1.  Switch to Visual Studio.
2.  In the **Script.usql** pane, in the toolbar, in the **(Local)** list, click **speedsdla*&lt;your name&gt;&lt;date&gt;***.
    Note, if your Azure Data Lake Analytics account is not shown, on the **View** menu, click **Server Explorer**, and then enter the credentials of your Azure account.
3.  Click **Submit**.
4.  When the job has completed, examine the simple job graph by hovering the mouse over the **SV1 Extract** step to see the processing details.
5.  Switch to the Azure portal.
6.  In Azure Data Lake Explorer, in the breadcrumb trail, click your root folder (**adls*&lt;your name&gt;&lt;date&gt;***).
7.  Verify that **AverageSpeeds.csv** has been created.
8.  Click **AverageSpeeds.csv** and verify that it contains the same data as before.
9.  Close the **File Preview** blade.

#### Task 7: Edit a Data Lake Analytics job to add rolling averages
1.  Switch to Visual Studio.
2.  In the **Script.usql** pane, at the end of the existing script, type the following code (this code can be copied from **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob2.usql**):
	```
	@rollingAvgSpeeds =
	SELECT CameraID, Time, Speed, AVG(Speed) OVER (PARTITION BY CameraID ORDER BY Time ROWS 25 PRECEDING) AS RollingAverage
	FROM @cameraData;

    OUTPUT @rollingAvgSpeeds
	TO "/RollingAverageSpeeds.csv"
	USING Outputters.Csv(outputHeader: true);
	```
Note that the first statement (@rollingAvgSpeeds) creates another rowset that calculates the average speed over the last 25 observations for each speed camera.
The second statement (OUTPUT @rollingAvgSpeeds) outputs the CameraID, observation time, and speed observed at that time—in addition to the rolling average—and writes the results out to another CSV file (with header).

#### Task 8: Run the Data Lake Analytics updated job
1.  In the second list box, ensure that **speedsdla*&lt;your name&gt;&lt;date&gt;*** is shown, and then click **Submit**.
2.  Note that, this time, the job graph is more complex, and that the two rowsets are generated in parallel.
3.  When the job has completed, examine the job graph; you will now see two parallel processes.
4.  Hover the mouse over the **SV2 Process** and **SV3 Process** steps to see the processing details.
5.  Close the **Job View** pane.
6.  On the **File** menu, click **Close Solution**.
7.  Switch to the Azure portal.
8.  On the **adls*&lt;your name&gt;&lt;date&gt;*** blade, verify that **RollingAverageSpeeds.csv** has been created.
9.  Click **RollingAverageSpeeds.csv** and view the file contents.
10. Close the **File Preview** blade.

>**Result**: At the end of this exercise, you will have prepared data files for a local Data Lake Analytics instance, created a new Data Lake project, and run this Data Lake Analytics job locally. Also, you will have created a Data Lake Analytics account, prepared data files for a cloud Data Lake Analytics instance, run this Data Lake Analytics job in the cloud—then edited the job to add rolling averages, and run the updated job.

## Exercise 2: Use Data Lake Analytics with data joins

#### Task 1: Prepare data files for a local Data Lake Analytics instance
1.  In File Explorer, go to **E:\\Labfiles\\Lab05\\Exercise2**, and copy **ownerdata.csv** to the clipboard.
2.  In File Explorer, go to **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, create a new folder called **vehicles**, and paste **ownerdata.csv** into the vehicles folder.
3.  Double-click **ownerdata.csv**, and note that this file contains the names and addresses of the owner for each vehicle.
4.  Close Microsoft Excel, without saving any changes.

#### Task 2: Create a new Data Lake project

1.  Switch to Visual Studio.
2.  On the **File** menu, point to **New**, and then click **Project**.
3.  In the **New Project** dialog box, in the **Templates** list, expand **Azure Data Lake**, and then click **U-SQL (ADLA)**.
4.  In the **Name** box, type **SpeedAnalyzer**, and then click **OK**.
5.  In the **Script.usql** pane, type the following statement to create a new database named **VehicleData** (if it doesn't already exist); all the code for this exercise can be copied from **E:\\Labfiles\\Lab05\\Exercise2\\VehiclesJobCmds.txt**:
	```
	CREATE DATABASE IF NOT EXISTS VehicleData;
	```
6.  In the **Script.usql** pane, add the following statement to create a table for holding speed camera information, and to index and hash the data by the speed camera ID:
	```
	CREATE TABLE IF NOT EXISTS VehicleData.dbo.Speeds
	(
	CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	INDEX speedidx
	CLUSTERED (CameraID ASC)
	DISTRIBUTED BY HASH (CameraID)
	);
	```
7.  In the **Script.usql** pane, add the following statement to remove any existing data from the table:
	```
	TRUNCATE TABLE VehicleData.dbo.Speeds;
	```
8.  In the **Script.usql** pane, add the following statement to declare a variable that uses pattern matching to specify the file name:
	```
	DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{Filename}";
	```
9.  In the **Script.usql** pane, add the following statement to create an extractor to retrieve speed data from the selected CSV file; this requires defining virtual fields for the date, hour, and filename components of the file, in addition to the fields within the file (the first line contains headers, so skip over it):
	```
	@speedData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	Date DateTime,
	Hour int,
	Filename string
	FROM @speedCameraData
	USING Extractors.Csv(skipFirstNRows: 1);
	```
10.  In the **Script.usql** pane, add the following statement to declare variables that specify the date and hour for the data to process (there are three hours of data—16-18, for August 30):
	```
	DECLARE @selectedDate DateTime = new DateTime(2017, 8, 30);
	DECLARE @hour int = 16; // Could also be 17 or 18
	```
11.  In the **Script.usql** pane, add the following statement to create a rowset that fetches the data for the specified date and time, and to ensure that only CSV files are read (this is specified using the Filename variable):
	```
	@speedRecords =
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
	FROM @speedData
	WHERE Date == @selectedDate AND Hour == @hour AND Filename LIKE "%.csv";
	```
12.  In the **Script.usql** pane, add the following statement to save the data to the Speeds table in the database for further analysis later:
	```
	INSERT INTO VehicleData.dbo.Speeds (CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time)
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
	FROM @speedRecords;
	```

#### Task 3: Test the Data Lake Analytics job locally
1.  Click **Submit**, to submit the job using the (Local) ADLA account in Visual Studio.
2.  The job will open a command window; when the job has completed, note the **Success** status, and then press Spacebar to close the command window.
3.  Examine the job graph by hovering the mouse over the **SV1** – **SV3** steps to see the processing details.
4.  In Server Explorer, expand **Azure**, expand **Data Lake Analytics**, expand **(Local)**, expand **U-SQL Databases**, expand **VehicleData**, expand **Tables**, right-click **dbo.Speeds**, and then click **Preview**; you might need to click **Refresh** to see **VehicleData**.
5.  Note that the preview has run another Data Lake Analytics job that fetches the data in the table and returns it as a TSV file.
6.  Close the **Table Preview** pane.

#### Task 4: Modify the Data Lake Analytics job to use joins

1.  In the **Script.usql** pane, add the following statement to create a table for recording fines and summonses:
	```
	CREATE TABLE IF NOT EXISTS VehicleData.dbo.Fines
	(
	CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	TimeCaught DateTime,
	PenaltyType string, // "P" for a Fixed Penalty (£100), "S" for a Summons (for excessive speeds, fine amount to be determined by a magistrate)
	PenaltyIssuedWhen DateTime,
	PenaltyIssuedTo string, // Full name of owner, including title
	OffenderAddress string, // Full address of the owner
	INDEX finesidx
	CLUSTERED (CameraID ASC)
	DISTRIBUTED BY HASH (CameraID)
	);
	```
2.  In the **Script.usql** pane, add the following statement to define a rowset that fetches the vehicle owner data from the ownerdata.csv file (and skip the headers in the first line):
	```
	@vehicleOwnerData =
	EXTRACT VehicleRegistration string,
	Title string,
	Forename string,
	Surname string,
	AddressLine1 string,
	AddressLine2 string,
	AddressLine3 string,
	AddressLine4 string
	FROM "/vehicles/ownerdata.csv"
	USING Extractors.Csv(skipFirstNRows: 1);
	```
3.  In the **Script.usql** pane, add the following statement to create a rowset that joins the speed data with the vehicle owner data, so that the application will know where to send fines/summonses. The code combines owner title and names into a single string, and concatenates the address information into another single string (with semicolons between each line of the address):
	```
	@speedsAndOwners =
	SELECT S.CameraID AS CameraID, S.VehicleRegistration AS VehicleRegistration, S.Speed AS Speed, S.SpeedLimit AS SpeedLimit,
	S.Time AS TimeCaught, O.Title + " " + O.Forename + " " + O.Surname AS PenaltyIssuedTo,
	O.AddressLine1 + "; " + O.AddressLine2 + "; " + O.AddressLine3 + "; " + O.AddressLine4 AS OffenderAddress
	FROM @speedRecords AS S
	JOIN @vehicleOwnerData AS O
	ON S.VehicleRegistration == O.VehicleRegistration;
	```
4.  In the **Script.usql** pane, add the following statement to process each row of data from the speed camera as follows:
 -   If the speed shown is less than the speed limit plus 5 percent, do not raise a fixed penalty or summons.
 -   If the speed shown is between the speed limit plus 5 percent and the speed limit plus 30 mph, generate a fixed penalty fine.
 -   If the speed is greater than the speed limit plus 30 mph, then issue a summons to appear in court.
	```
	@fines =
	SELECT *, DateTime.UtcNow AS PenaltyIssuedWhen,
	Speed >= SpeedLimit * 1.05 && Speed <= SpeedLimit + 30 ? "P" :
	Speed >= SpeedLimit + 30 ? "S" : "" AS PenaltyType
	FROM @speedsAndOwners;
	```
5.  In the **Script.usql** pane, add the following statement to save the details for fines and summonses to the Fines table in the database, and do not record data for legal traffic speeds:
	```
	INSERT INTO VehicleData.dbo.Fines (CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress)
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
	FROM @fines
	WHERE !String.IsNullOrEmpty(PenaltyType);
	```
6.  In the **Script.usql** pane, add the following statement to copy the data to an output file; note that you cannot read data from the VehicleData.dbo.Fines table because it could be mutating:
	```
	@results =
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
	FROM @fines
	WHERE !String.IsNullOrEmpty(PenaltyType);

    OUTPUT @results
	TO "/FinesAndSummonses.csv"
	USING Outputters.Csv(outputHeader: true);
	```

#### Task 5: Test the modified Data Lake Analytics job locally
1.  Click **Submit**, to submit the job using the (Local) ADLA account.
2.  The job will open a command window; when the job has completed, note the **Success** status, and then press Spacebar to close the command window.
3.  Examine the job graph and note that there are now more processing steps shown.
4.  In Server Explorer, under **Azure\\Data Lake Analytics\\(Local)\\U-SQL Databases\\VehicleData**, right-click the **Tables** folder, and then click **Refresh**.
5.  Right-click **dbo.Fines**, and then click **Preview**.
6.  Verify that the **Fines** table contains the details of any fines and summonses.
7.  Close the **Table Preview** pane.

#### Task 6: Run the updated Data Lake Analytics job in the cloud
1.  Switch to the Azure portal.
2.  Click **All resources**, click **adls*&lt;your name&gt;&lt;date&gt;***, and then click **Data Explorer**.
3.  In Data Explorer, ensure you are at the root folder, and then click **New Folder**.
4.  In the **Create new folder** dialog box, type **vehicles**, and then click **OK**.
5.  Click **vehicles**, and then click **Upload**.
6.  On the **Upload files** blade, click the **file** button.
7.  In the **Choose File to Upload** dialog box, go to **E:\\Labfiles\\Lab05\\Exercise2**, click **ownerdata.csv**, and then click **Open**.
8.  On the **Upload files** blade, click **Add selected files**.
9.  When the file has uploaded, close the **Upload files** blade.
10. Switch to Visual Studio.
11. In the **Script.usql** pane, in the toolbar, in the **(Local)** list, click **speedsdla*&lt;your name&gt;&lt;date&gt;***, and then click **Submit**.
12. When the job has completed, examine the job graph by hovering the mouse over the processing steps.
13. Switch to the Azure portal.
14. In Azure Data Lake Explorer, in the breadcrumb trail, click **adls*&lt;your name&gt;&lt;date&gt;***.
15. Verify that **FinesAndSummonses.csv** has been created.
16. Click **FinesAndSummonses.csv** and verify that it contains the same data as before.
17. Close the **File Preview** blade.
18. Switch to Visual Studio.
19. In Server Explorer, under **Azure**, under **Data Lake Analytics**, under **speedsdla*&lt;your name&gt;&lt;date&gt;***, expand **U-SQL Databases**, expand **VehicleData**, and then expand **Tables**; note that you might need to click **Refresh** to see **VehicleData**.
20. In the **Tables** folder, right-click **dbo.Speeds**, and then click **Preview by Running a Job**.
21. In the **Preview by Running a Job** dialog box, click **Submit**.
22. When the preview job has completed, click the **Data** tab, and in the **Job Outputs** pane, double-click **Speeds\_Preview.tsv**.
23. Verify that the Speeds table contains the same data as when the job was run locally.
24. Close the **File Preview** and **Job View** panes.
25. In Server Explorer, right-click **dbo.Fines**, and then click **Preview by Running a Job**.
26. In the **Preview by Running a Job** dialog box, click **Submit**.
27. When the preview job has completed, click the **Data** tab, and in the **Job Outputs** pane, double-click **Fines\_Preview.tsv**.
28. Verify that the table contains the same data as when the job was run locally.
29. Verify that the **Fines** table contains the same details of fines and summonses.
30. Close Visual Studio without saving any changes.

>**Result**: At the end of this exercise, you will have prepared data files for a local Data Lake Analytics instance, created a new Data Lake project and tested this job locally, then modified the job to use joins, and tested the job locally before running it in the cloud.

## Exercise 3: Use Data Lake Analytics with SQL Database

#### Task 1: Create a SQL Database
1.  In the Azure portal, click **+ New**, click **Databases**, and then click **SQL Database**.
2.  On the **SQL Database** blade, in the **Database name** box, type **VehicleDB**.
3.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4.  In the **Location** list, select the same location as you used for the Data Lake Store.
5.  In **Select source**, click **Blank database**.
6.  Click **Server**.
7.  On the **New server** blade, enter the following details, and then click **Select**:
 -   **Server name**: camerasdbs*&lt;your name&gt;&lt;date&gt;*
 -   **Server admin login**: student
 -   **Password**: Pa55w.rd
 -   **Confirm password**: Pa55w.rd
 -   **Location**: select the same location as you used for the Data Lake Store that you created in Exercise 1
 -   Ensure that the **Allow azure services to access server** check box is selected
8.  On the **SQL Database** blade, click **Pricing tier**.
9.  On the **Configure performance** blade, click **Basic**, and then click **Apply**.
10.  On the **SQL Database** blade, leave all other settings at their defaults, and click **Create**.
11.  Wait until the database and server have deployed before continuing with the lab.
12.  In the Azure portal, click **All resources**, and then click **camerasdbs*&lt;your name&gt;&lt;date&gt;***.
13.  On the **camerasdbs*&lt;your name&gt;&lt;date&gt; ***blade, under **SETTINGS**, click **Firewall/Virtual Networks**, and then click **Add client IP**.
14.  Ensure that **Allow access to Azure services** in set to **ON**, and then click **Save**.
15.  In the **Success** dialog box, click **OK**.

#### Task 2: Upload data to SQL Database and add an index

1.  On the Start menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
2.  In the **Connect Server** dialog box, in the **Server name** box, type **camerasdbs*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
3.  In the **Authentication** list, click **SQL Server Authentication**.
4.  In the **Login** box, type **student**.
5.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6.  In SQL Server Management Studio, in Object Explorer, expand **Databases**, right-click **VehicleDB**, point to **Tasks**, and then click **Import Data**.
7.  In the **SQL Server Import and Export Wizard** dialog box, on the **Welcome to SQL Server Import and Export Wizard** page, click **Next**.
8.  On the **Choose a Data Source** page, in the **Data source** list, click **Flat File Source**, and then click **Browse**.
9.  In the **Open** dialog box, go to **E:\\Labfiles\\Lab05\\Exercise3**, change the file type to **CSV files (\*.csv)**, click **ownerdata.csv**, and then click **Open**.
10. On the **Choose a Data Source** page, leave the **Column names in the first data row** check box selected, and then click **Next**.
11. On the **Choose a Data Source** page, click **Next**.
12. On the **Choose a Destination** page, in the **Destination** list, select **SQL Server Native Client 11.0**.
13. Ensure that the **Server name** is set to **camerasdbs*&lt;your name&gt;&lt;date&gt;*.database.windows.net**, and then click **Use SQL Server Authentication**.
14. In the **User name** box, type **student**, in the **Password** box, type **Pa55w.rd**, and then click **Next**.
15. On the **Select Source Tables and Views** page, in the **Destination** column, accept the default table name **[dbo].[ownerdata]**, and then click **Next**.
16. On the **Save and Run Package** page, leave the settings at the defaults, and then click **Next**.
17. On the **Complete the Wizard** page, click **Finish**.
18. Verify that the data has been uploaded successfully (the operation should upload 1782 records to the database). If you get data truncation warnings, you can ignore them.
19. On the **The execution was successful** page, click **Close**.
20. In Object Explorer, under **Databases**, expand **VehicleDB**, expand **Tables**, right-click **dbo.ownerdata**, and then click **Select Top 1000 Rows**.
21. Verify that the data has been uploaded to the table.
22. In Object Explorer, expand **dbo.ownerdata**, right-click **Indexes**, and then click **New Index**.
23. In the SQL command pane, replace the template index with the following SQL command (the SQL code can be copied from **E:\\Labfiles\\Lab05\\Exercise3\\Index.txt**):
	```
	CREATE INDEX RegIdx
	ON [dbo].[ownerdata]
	(
	[Vehicle Registration]
	)
	GO
	```
24.  Click **Execute**.
25.  Close SQL Server Management Studio, without saving any scripts or SQL files.

#### Task 3: Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell
1.  In File Explorer, go to **E:\\Labfiles\\Lab05\\Exercise3**, right-click **Setup.cmd,** and then click **Run as administrator**.
2.  In the **User Account Control** dialog box, click **Yes**.
3.  On the Start menu, type **Windows PowerShell ISE**, right-click **Windows PowerShell ISE**, and then click **Run as administrator**.
4.  In the **User Account Control** dialog box, click **Yes**.
5.  In Windows PowerShell ISE, on the **File** menu, click **Open**.
6.  In the **Open** dialog box, go to **E:\\Labfiles\\Lab05\\Exercise3**, click **CreateCredential.ps1**, and then click **Open**, to open the following script:
	```
	# Install Azure PowerShell modules
	Install-Module AzureRM -AllowClobber;
	Import-Module AzureRM;

    # Login to Azure
	Login-AzureRmAccount;

    # Create credential
	New-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -CredentialName "VehicleOwnerDataCredential" -Credential (Get-Credential) -DatabaseHost "<name of your Azure SQL Database Server>.database.windows.net" -Port 1433;

    # Remove credential
	# Remove-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -Name "VehicleOwnerDataCredential";
	```
7.  Edit the script, replacing **&lt;name of your ADLA account&gt;** with **speedsdla*&lt;your name&gt;&lt;date&gt;***.
8.  Edit the script, replacing **&lt;name of your Azure SQL Database Server&gt;** with **camerasdbs*&lt;your name&gt;&lt;date&gt;***.
9.  On the **File** menu, click **Save**.
10.  Highlight lines 1-3, and then on the toolbar, click **Run Selection**, to install the latest AzureRM cmdlets.
11.  If you get a warning message about installing modules from an untrusted repository, click **Yes to All**.
12.  Highlight lines 5-6, and then on the toolbar, click **Run Selection**, to log in to Azure.
13.  In the **Sign in to your account** dialog box, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Sign in**.
14.  Highlight lines 8-9, and then on the toolbar, click **Run Selection**, to create a credential in your ADLA catalog that contains the connection information for connecting to your Azure SQL Database.
15.  In the **cmdlet** **Get-Credential at command pipe** dialog box, in the **User name** box, type **student**, in the **Password** box type **Pa55w.rd**, and then click **OK**. These are the credentials for accessing the database server—these credentials are stored, encrypted, as part of the credential in the catalog.
16. Note that the **-DatabaseName** parameter is the name of the database in the ADLA catalog that will hold the credential, and is *not* the name of the SQL database to which the credential connects!
17. Note also that the script shows the **Remove-AzureRmDataLakeAnalyticsCatalogCredential** command (commented out); this is how you remove a credential from the catalog. You could use this command if you make a mistake when creating the credential; you can then recreate the credential with the correct information.

#### Task 4: Configure a Visual Studio-based Data Lake Analytics job to use stored credentials
1.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
2.  In Visual Studio, click **Open Project / Solution**.
3.  In the **Open Project** dialog box, go to the **E:\\Labfiles\\Lab05\\Exercise3\\Starter-SpeedAnalyzer** folder, click **SpeedAnalyzer.sln**, and then click **Open**.
    Note that this project is a sample solution for the U-SQL job created in Exercise 2.
4.  If the **Select Azure Account** dialog box appears, select your Azure subscription, and then click **OK**.
5.  In Solution Explorer, double-click **Script.usql**.
6.  In the **Script.usql** pane, locate the comment **// TODO: Switch the context to the VehicleData database in the catalog**, then on the next line, add the following USE statement for the credential store (the U-SQL commands can be copied from **E:\\Labfiles\\Lab05\\Exercise3\\UsqlCmds.txt**):
	```
	USE DATABASE VehicleData;
	```
7.  In the **Script.usql** pane, locate the comment **// TODO: Drop the VehicleOwner Data Source if it already exists**, then on the next line, add the following statement:
	```
	DROP DATA SOURCE IF EXISTS VehicleOwner;
	```
8.  In the **Script.usql** pane, locate the comment **// TODO: Create the VehicleOwner data source**, then on the next line, add the following statement to create the data source:
	```
	CREATE DATA SOURCE VehicleOwner
	FROM AZURESQLDB
	WITH (
	PROVIDER_STRING = "Database=VehicleDB;Trusted_Connection=False;Encrypt=True;",
	CREDENTIAL = VehicleOwnerDataCredential ,
	REMOTABLE_TYPES = (bool, byte, sbyte, short, int, long, decimal, float, double, string, DateTime)
	);
```
9.  In the **Script.usql** pane, locate the comment **// TODO: Fetch the vehicle owner data from the SQL database**, then on the next line, add the following statement to create a rowset from the dbo.ownerdata table held by the SQL database, using the data source you just created:
	```
	@vehicleOwnerData =
	SELECT [Vehicle Registration] AS VehicleRegistration, [Title] AS Title, [Forename] AS Forename, [Surname] AS Surname,
	[Address Line 1] AS AddressLine1, [Address Line 2] AS AddressLine2, [Address Line 3] AS AddressLine3, [Address Line 4] AS AddressLine4
	FROM EXTERNAL VehicleOwner
	LOCATION "dbo.ownerdata";
	```
10.  Leave the remainder of the script as it is; it is unchanged from Exercise 2, and performs the same tasks, but now uses the data retrieved from the SQL database instead of a flat file.

#### Task 5: Run a Data Lake Analytics job using stored credentials to access data in SQL Database
1.  In the **Script.usql** pane, in the toolbar, click the **(Local)** list, click **speedsdla*&lt;your name&gt;&lt;date&gt;***, and then click **Submit**.
    Note that this script must be run in the cloud, because the credential was created in the ADLA account, not locally.
2.  When the job has completed, examine the job graph by hovering the mouse over the processing steps.
3.  Switch to the Azure portal.
4.  Click **All resources**, click **adls*&lt;your name&gt;&lt;date&gt;***, and then click **Data Explorer**.
5.  In Data Lake Explorer, in the root folder, verify that **FinesAndSummonses.csv** has been created.
6.  Click **FinesAndSummonses.csv** and verify that it contains the same data as before.
7.  Close the **File Preview** blade.
8.  Switch to Visual Studio.
9.  In Server Explorer, expand **Azure**, expand **Data Lake Analytics**, then expand **speedsdla*&lt;your name&gt;&lt;date&gt;***, expand **U-SQL Databases**, expand **VehicleData**, and then expand **Tables**; note that you might need to click **Refresh** to see **VehicleData**.
10. In the **Tables** folder, right-click **dbo.Speeds**, and then click **Preview by Running a Job**.
11. In the **Preview by Running a Job** dialog box, click **Submit**.
12. When the preview job has completed, on the **Data** tab, in the **Job Outputs** section, double-click **Speeds\_Preview.tsv**.
13. Verify that the Speeds table contains the same data as when the job was run locally.
14. Close the **File Preview** and **Job View** panes.
15. In Server Explorer, right-click **dbo.Fines**, and then click **Preview by Running a Job**.
16. In the **Preview by Running a Job** dialog box, click **Submit**.
17. When the preview job has completed, on the **Data** tab, in the **Job Outputs** section, double-click **Fines\_Preview.tsv**.
18. Verify that the table contains the same data as when the job was run locally.
19. Verify that the **Fines** table contains the same details of fines and summonses.
20. Close Visual Studio without saving any changes.

>**Result**: At the end of this exercise, you will have created a SQL Database, uploaded data to SQL Database and added an index. You will also have stored a SQL Database credential in the Data Lake Analytics catalog using PowerSheII, configured a Data Lake Analytics job to use stored credentials, and run this job using stored credentials to access data in SQL Database.

## Exercise 4: Use Data Lake Analytics to categorize data and present results using Power BI

#### Task 1: Create a new Data Lake project

1.  On the Start menu, type Visual Studio 2017, and then press Enter.
2.  In Visual Studio, on the **File** menu, point to **New**, and then click **Project**.
3.  In the **New Project** dialog box, in the **Templates** list, expand **Azure Data Lake**, then click **U-SQL (ADLA)**.
4.  In the **Name** box, type **SpeedCameraAnalytics**, and then click **OK**.
5.  In Solution Explorer, double-click **Script.usql**.
6.  You will now define a rowset variable that extracts the data from the speed camera data files (uploaded in Exercise 1) under the /speeds folder. These files are organized by date and time, but you should ensure that you read every available file. Remember that each file is in CSV format, and has a header.
7.  In the **Script.usql** pane, type the following code (all the code in this exercise can be copied from **E:\\Labfiles\\Lab05\\Exercise4\\CameraJobCmds.txt**):
	```
	// Capture the historical and current data from all available files containing speed data
	DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{*}.csv";

    // Specify the format of the data in the CSV files
	@speedData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	Date DateTime,
	Hour int
	FROM @speedCameraData
	USING Extractors.Csv(skipFirstNRows: 1);
	```
8.  You will now define a rowset that retrieves the data in the CameraID and Speed fields from these files; in the **Script.usql** pane, type the following code:
	```
	// Generate a rowset that reads the relevant data from the CSV files
	@cameraData =
	SELECT CameraID, Speed, LocationLatitude, LocationLongitude
	FROM @speedData;
	```
9.  You will now define a rowset that, for each camera, calculates the number of vehicles that were travelling at less than 10 mph (this is the first "bucket"); in the **Script.usql** pane, type the following code:
	```
	// Define a rowset that calculates the number of vehicles for each camera that were travelling at less than 10 mph (this is the first "bucket")
	@lessThan10 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS LessThan10
	FROM @cameraData
	WHERE Speed < 10
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
10.  You will now define another rowset that, for each camera, calculates the number of vehicles that were travelling at between 10 and 29 mph (this is the second "bucket); in the **Script.usql** pane, type the following code:
	```
	// Define a rowset that calculates the number of vehicles for each camera that were travelling at between 10 and 29 mph (this is the second "bucket)
	@between10And29 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between10And29
	FROM @cameraData
	WHERE Speed BETWEEN 10 AND 29
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
11.  You will now repeat the previous code for the following speed ranges—30-49 mph, 50-69 mph, 70-99 mph, 100 mph and more; in the **Script.usql** pane, type the following code:
	```
	// Define rowsets for the following speed ranges; 30-49 mph, 50-69 mph, 70-99 mph, 100 mph and over
	@between30And49 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between30And49
	FROM @cameraData
	WHERE Speed BETWEEN 30 AND 49
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @between50And69 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between50And69
	FROM @cameraData
	WHERE Speed BETWEEN 50 AND 69
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @between70And99 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between70And99
	FROM @cameraData
	WHERE Speed BETWEEN 70 AND 99
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

    @over99 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Over99
	FROM @cameraData
	WHERE Speed > 99
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
12.  You will now define another rowset that joins the buckets together over the CameraID column. You must use an OUTER join in case the bucket for a particular speed camera is empty (you don't want to discard all the data in the other buckets for that camera); in the **Script.usql** pane, type the following code:
	```
	// Join the speed buckets together, this requires an OUTER join, as one or more buckets for a camera could be null (empty)
	@speedAnalysis =
	SELECT a.CameraID, a.LocationLatitude, a.LocationLongitude, a.LessThan10, b.Between10And29, c.Between30And49, d.Between50And69, e.Between70And99, f.Over99
	FROM @lessThan10 AS a
	LEFT JOIN @between10And29 AS b
	ON a.CameraID == b.CameraID
	LEFT JOIN @between30And49 AS c
	ON a.CameraID == c.CameraID
	LEFT JOIN @between50And69 AS d
	ON a.CameraID == d.CameraID
	LEFT JOIN @between70And99 AS e
	ON a.CameraID == e.CameraID
	LEFT JOIN @over99 AS f
	ON a.CameraID == f.CameraID;
	```
13.  Finally, you will save the results to a CSV file named **/SpeedAnalysis.csv**, and include a header; in the **Script.usql** pane, type the following code:
	```
	OUTPUT @speedAnalysis
	TO "/SpeedAnalysis.csv"
	USING Outputters.Csv(outputHeader: true);
	```

#### Task 2: Test the Data Lake Analytics job locally then run it in the cloud
1.  Click **Submit**, to submit the job using the (Local) ADLA account in Visual Studio.
2.  The job will open a command window; when the job has completed, note the **Success** status, and then press Spacebar to close the command window.
3.  Examine the job graph by hovering the mouse over the various processing steps.
4.  In File Explorer, go to **C:\\Users\\AdatumAdmin\\AppData\\Local\\USQLDataRoot**, then double-click **SpeedAnalysis.csv** to open the file in Excel.
5.  Verify that the results show a tabular list of cameras, their locations, and the number of cars detected in each speed bucket.
6.  Close Microsoft Excel, without saving any changes.
7.  Switch to Visual Studio.
8.  In the **Script.usql** pane, in the toolbar, in the **(Local)** list, click **speedsdla*&lt;your name&gt;&lt;date&gt;***, and then click **Submit**, to now run the job in the cloud.
9.  When the job has completed, examine the job graph by hovering the mouse over the various processing steps.
10. Close Visual Studio, without saving any changes.

#### Task 3: Use Power BI to visualize the data
1.  On the Start menu, type **Power BI Desktop**, and then press Enter.
2.  In the **Power BI Desktop welcome** pane, click **Get data**.
3.  In the **Get Data** dialog box, click **Azure**, click **Azure Data Lake Store**, and then click **Connect**.
4.  In the **Azure Data Lake Store** dialog box, type the following URL, replacing **&lt;name of your Data Lake Store&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***, and then click **OK**:
	```
	adl://<name of your Data Lake Store>.azuredatalakestore.net/SpeedAnalysis.csv
	```
5.  In the **Azure Data Lake Store** dialog box, if there is a **You aren't signed in** message, click **Sign in**, and authenticate using the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Connect**.
6.  In the **adl://adls*&lt;your name&gt;&lt;date&gt;*.azuredatalakestore.net/SpeedAnalysis.csv** dialog box, click **Edit**.
7.  In the Query Editor, in the **Content column**, click **Binary**; if the data in the file does not appear, click **Binary** again.
8.  In the Query Editor, click **Close & Apply**.
9.  In the **Visualizations** pane, click **ArcGIS Maps for Power BI**.
10.  In the **Welcome to ArcGIS Maps for Power BI** dialog box, click **OK**.
11.  In the **Fields** pane, drag the **LocationLatitude** field to the **Latitude** box in the **Visualizations** pane.
12.  In the **Fields** pane, drag the **LocationLongitude** field to the **Longitude** box in the **Visualizations** pane.
13.  In the **Fields** pane, drag the **CameraID** field to the **Location** box in the **Visualizations** pane.
14. To make the names more meaningful, in the **Fields** pane, point to **LessThan10**, click the ellipses (**...**), and then click **Rename**.
15. Type **Less than 10 mph**, and then press Enter.
16. Repeat steps 14 and 15 to rename each of the **BetweenXAndY** fields to **Between X and Y mph** (for example, Between10And29 will be renamed to Between 10 and 29 mph).
17. Repeat steps 14 and 15 to rename **Over99** to **Over 99 mph**.
18. In the **Fields** pane, drag the **Less than 10 mph** field to the **Tooltips** box in the **Visualizations** pane.
19. In the **Fields** pane, drag the **Between 10 and 29 mph** field to the bottom of the **Tooltips** box in the **Visualizations** pane.
20. Repeat the previous step for each of the remaining speed fields, finishing with the **Over 99 mph** field.
21. You should now have all the speed fields listed under **Tooltips**, and in ascending speed order.
22. On the map object, click **Focus mode**, and view the map.
23. Zoom in and hover over any speed camera to see the detected speed values for that camera.
24. On the **File** menu, click **Save**.
25. In the **Save As** dialog box, go to **E:\\Labfiles\\Lab05\\Exercise4**, in the **File name** box, type **SpeedCameraAnalysis.pbix**, and then click **Save**.
26. Close Power BI Desktop.

#### Task 4: Lab closedown
1.  Close Windows PowerShell ISE.
2.  Do not remove the Azure resources (resource group, Stream Analytics jobs, event hub, IoT hub, and storage); these resources will be used in later labs.
3.  Close Internet Explorer.

>**Result**: At the end of this exercise, you will have created a new Data Lake project, tested the job locally, run it in the cloud, and then used Power BI to visualize the data.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
