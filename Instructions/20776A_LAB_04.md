# Module 4: Managing Big Data in Azure Data Lake Store

# Lab: Managing big data in Data Lake Store

### Scenario
You work for Adatum as a data engineer, and have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data—captured from speed cameras and automatic number plate recognition (ANPR) devices—and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For the next phase of the project, you will use a range of tools to enable batch mode, and automated operations, with Data Lake Store. You will also add security to your Data Lake Store, using custom ACLs and data encryption that uses your own managed key.

### Objectives
After completing this lab, you will be able to:
-   Perform Data Lake Store operations using PowerShell.
-   Upload bulk data to Data Lake Store.
-   Set ACLs on Data Lake Store folders and files.
-   Encrypt Data Lake Store data using your own key.

### Lab Setup
Estimated time: 90 minutes

Virtual machine: **20776A-LON-DEV**

User name: **ADATUM\\AdatumAdmin**

Password: **Pa55w.rd**

## Exercise 1: Perform Data Lake Store operations using PowerShell

### Scenario
To enable automated and batch mode operations with Data Lake Store, you will first investigate the use of PowerShell cmdlets for Data Lake Store. In this exercise, you will use PowerShell to deploy a new Data Lake Store, and to manage folders and files in this store.

The main tasks for this exercise are as follows:
1. Install AzCopy
2. Install AdlCopy
3. Prepare the PowerShell environment
4. Use PowerShell to create a new Data Lake Store
5. Use PowerShell to manage files and folders in Data Lake Store

#### Task 1: Install AzCopy
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  Install **AzCopy** from **https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy**.
3.  Add: **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy** to the system path.

#### Task 2: Install AdlCopy
1.  Install **AdlCopy** from **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
2.  Add **%HOMEPATH%\\Documents\\AdlCopy** to the system path.

#### Task 3: Prepare the PowerShell environment
1.  Run the **E:\\Labfiles\\Lab04\\Setup.cmd** as administrator.
2.  Start **PowerShell ISE as Administrator**, and open **E:\\Labfiles\\Lab04\\ADLS\_commands.ps1**.
3.  In the following script, replace **&lt;name of new Data Lake Store&gt;** with **lakestore*&lt;your name&gt;&lt;date&gt;***, and **&lt;location&gt;** with **Central US**, **East US 2**, or **North Europe**:

    ```
    # Log in to Azure
Login-AzureRmAccount

    # Register the Azure Data Lake Store provider
Register-AzureRmResourceProvider -ProviderNamespace "Microsoft.DataLakeStore"

    # Create a new Resource Group
New-AzureRmResourceGroup -Location "<location>" -Name "LakeStoreRG"

    # Create a new Data Lake Store Account
$resourceGroupName = "LakeStoreRG"
$dataLakeStoreName = "<name of new Data Lake Store"
New-AzureRmDataLakeStoreAccount -ResourceGroupName $resourceGroupName -Name $dataLakeStoreName -Location "<location>"

    # Verify that the Data Lake Store account has been created
Test-AzureRmDataLakeStoreAccount -Name $dataLakeStoreName

    # Create a folder named VehicleOwnerData
$folder="/VehicleOwnerData"
New-AzureRmDataLakeStoreItem -Folder -AccountName $dataLakeStoreName -Path $folder

    # Upload the VehicleOwners.csv file to the VehicleOwnerData folder
Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path "E:\Labfiles\Lab04\\Exercise1\VehicleOwners.csv" -Destination $folder\VehicleOwners.csv

    # List all files and folders in the VehicleOwnerData directory
Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path $folder
```
4.  Save the updated script.

#### Task 4: Use PowerShell to create a new Data Lake Store
1.  Execute lines 1-13 to create a new Data Lake Store, and then on the toolbar, click **Run Selection**.
2.  In the **Sign in to your account** dialog box, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription, and then click **Sign in**.
3.  Execute lines 15-16 to verify that the new Data Lake Store has been created.

#### Task 5: Use PowerShell to manage files and folders in Data Lake Store
1.  Execute lines 18-20 to create a folder called **VehicleOwnerData**.
2.  Execute lines 22-23 to upload a file called **VehicleOwners.csv** to the **VehicleOwnerData** folder.
3.  Execute lines 25-26 to list all files and folders in the **VehicleOwnerData** directory.

>**Result**: At the end of this exercise, you will have used PowerShell cmdlets to:
-   Create a new Data Lake Store (in a new resource group).
-   Create a folder in this store.
-   Upload a file to the folder.
-   Display to files in the folder.

## Exercise 2: Upload bulk data to Data Lake Store

### Scenario
To further enhance automated and batch mode operations with Data Lake Store, you will next investigate the command-line tools that enable bulk data upload to Azure Blob storage, and bulk transfers of data from Azure Blob storage to Data Lake Store. In this exercise, you will use AzCopy to upload data to Blob storage, and then use AdlCopy to transfer this data to a Data Lake Store.

The main tasks for this exercise are as follows:
1. Create a new Storage account and Blob container
2. Upload files to the new Blob container
3. Verify the file upload using Cloud Explorer
4. Use AdlCopy to copy the files from Blob storage to Data Lake Store
5. Verify the data copy using Data Explorer

#### Task 1: Create a new Storage account and Blob container
1.  Open the Azure portal, and sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
2.  Create a new Storage account, with the following details:
-   **Name**: blob*&lt;your name&gt;&lt;date&gt;*
-   **Resource group (Use existing)**: LakeStoreRG
-   **Location**: select the same location as you used for the Data Lake Store in Exercise 1
-   Leave all other details at their defaults
3.  Wait until the storage account has been successfully created before continuing with the exercise.
4.  Create a new Blob container in your storage account called **vehiclefeed**.
5.  Copy **key1** for the Storage account to the clipboard.

#### Task 2: Upload files to the new Blob container
1.  Go to **E:\\Labfiles\\Lab04\\Exercise2**, and note that there are multiple folders and subfolders containing vehicle registration data, which are organized by year and month.
2.  In an Admin-level Command Prompt, execute the following command, (replacing **&lt;storage account name&gt;** with the name of the account that was previously created—that is, **blob*&lt;your name&gt;&lt;date&gt;***), and replacing **&lt;access key&gt;** with the key you copied to the clipboard:
```
azcopy /Source:"E:\Labfiles\Lab04\Exercise2" /Dest:https://<storage account name>.blob.core.windows.net/vehiclefeed /DestKey:<access key> /S
```
You copy the preceding command from **E:\\Labfiles\\Lab04\\AZCopyCmd.txt**.

#### Task 3: Verify the file upload using Cloud Explorer
1.  In Visual Studio, open Cloud Explorer.
2.  In Cloud Explorer, if you do not have your Azure Learning Pass subscription folder, add the Microsoft account that is associated with your Azure Learning Pass subscription.
3.  Use Cloud Explorer to examine the contents of **vehiclefeed**, and verify that all the files and subfolders have been uploaded.

#### Task 4: Use AdlCopy to copy the files from Blob storage to Data Lake Store
1.  Execute the following command (replacing **&lt;name of your Data Lake Store&gt;** with **lakestore*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;access key&gt;** with the blob store key you copied to the clipboard:
```
adlcopy /source https://<storage account name>.blob.core.windows.net/vehiclefeed/ /dest adl://<name of your Data Lake Store>.azuredatalakestore.net/VehicleRegistrations/ /sourcekey <access key>
```
You copy the preceding command from **E:\\Labfiles\\Lab04\\AdlCopyCmd.txt**.
1.  If the **Visual Studio** dialog box appears, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
2.  Note that both the source and destination URIs must end with a **/**; AdlCopy is automatically recursive, and there is no **/S** parameter.
3.  When the copy is complete, close the command prompt.

#### Task 5: Verify the data copy using Data Explorer
-   Use Data Explorer to view the contents of the **VehicleRegistrations** folder, and verify that the files that were uploaded to Blob storage have now been copied to your Data Lake Store.

>**Result**: At the end of this exercise, you will have:
-   Created a new Azure storage account and Blob container.
-   Uploaded a set of files and folders to the container using AzCopy.
-   Verified the upload using Cloud Explorer in Visual Studio.
-   Copied the data from Blob storage to Data Lake Store using AdlCopy.
-   Verified the copy using Data Explorer in the Azure portal.

## Exercise 3: Set ACLs on Data Lake Store folders and files

### Scenario
Adatum’s new traffic surveillance system will use Data Lake Store for primary data storage; therefore, it’s essential that this data is protected. In this exercise, you will add security to your Data Lake Store, using custom ACLs. To test and evaluate the security model in Data Lake Store, you will use a guest user account.

The main tasks for this exercise are as follows:
1. Create a Microsoft account for a Data Lake guest user
2. Add the guest user account to Azure Active Directory
3. Set guest user permissions for a Data Lake Store folder
4. Start a PowerShell session as the guest user
5. Use PowerShell to attempt to list folder contents
6. Set permissions for the root folder
7. Use PowerShell to list the folder contents after a permissions change
8. Use PowerShell to attempt to read a file
9. Set permissions for a file
10. Use PowerShell to read a file after a permissions change
11. Use PowerShell to attempt to upload a new file
12. Edit folder permissions to enable file upload
13. Use PowerShell to overwrite a file
14. Use PowerShell to attempt to overwrite a file after a folder permissions change

#### Task 1: Create a Microsoft account for a Data Lake guest user
1.  Use an InPrivate browser session to go to **https://signup.live.com**, and create a new Microsoft account with the following information:
 -   A username, such as Course20776aLab*-&lt;date&gt;-&lt;your name&gt;*@outlook.com.
 -   A password.
 -   A first name and last name.
 -   Your local country/region.
 -   A date of birth.
2.  When the account has been created, open the mail inbox, and click **Let's go**.

#### Task 2: Add the guest user account to Azure Active Directory
1.  In the Azure portal, open Azure Active Directory, and add a guest user, using the email address of the guest user account.
2.  In the mail inbox for the guest user account, open the **Microsoft Invitations** email, and click **Get Started**.

#### Task 3: Set guest user permissions for a Data Lake Store folder
1.  In the Azure portal, on the **Azure Active Directory** blade, copy the **Directory ID** to the clipboard.
2.  Use the Data Explorer to add the following permissions to the **VehicleOwnerData** folder for the guest user account:
-   **Read** and **Execute** as **An access permission entry and a default permission entry**
3.  Verify that your guest account is shown under Assigned Permissions, and with the correct permissions.

#### Task 4: Start a PowerShell session as the guest user
1.  Start Windows PowerShell, and at the PowerShell command prompt, execute the following command:
```
Login-AzureRmAccount -TenantID <DirectoryID>
```
Note that the PowerShell commands used in this exercise can be copied from **E:\\Labfiles\\Lab04\\PSCmds.txt**.
2.  If a data collection warning appears, type **n**.
3.  Sign in using the details of the guest user account.
4.  If the **You're already signed in** dialog box appears, use the **Sign out and sign in with a different account** option.

#### Task 5: Use PowerShell to attempt to list folder contents
1.  Execute the following PowerShell command, replacing **&lt;name of your Data Lake Store&gt;** with **lakestore*&lt;your name&gt;&lt;date&gt;***:
```
$dataLakeStoreName = "<name of your Data Lake Store>"
```
2.  Execute the following PowerShell command:
```
Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
```
3.  This command will return the following error: Operation returned an invalid status code 'Forbidden'.

#### Task 6: Set permissions for the root folder
1.  In the Azure portal, use Data Explorer to set the following permissions to the root folder for the guest user account:
-   Read and Execute
2.  Verify that your guest account is shown under Assigned Permissions, and with the correct permissions.

#### Task 7: Use PowerShell to list the folder contents after a permissions change
1.  Execute the following PowerShell command:
```
Get-AzureRmDataLakeStoreChildItem -AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
```
2.  This command should now successfully list the contents of the **VehicleOwnerData** folder.

#### Task 8: Use PowerShell to attempt to read a file
1.  Execute the following PowerShell command:
```
$path= "/VehicleOwnerData/VehicleOwners.csv"
```
2.  Execute the following PowerShell command:
```
$download= "E:\Labfiles\Lab04\Exercise1\VehicleOwners-downloaded.csv"
```
3.  Execute the following PowerShell command:
```
Export-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $path -Destination $download
```
This command will return an error.
4.  Execute the following PowerShell command:
```
DIR E:\Labfiles\Lab04\Exercise1\
```
5.  Note that the **Export-AzureRmDataLakeStoreItem** cmdlet has created the **VehicleOwners-downloaded.csv** file (0 bytes), but was unable to read the file's contents. This is because the file existed before permissions were granted to your guest account and, therefore, this account does not have permission to read the contents of the file.

#### Task 9: Set permissions for a file
1.  In the Azure portal, use the **Data Explorer** blade to set the following permissions to **VehicleOwners.csv** for the guest user account:
-   Read
2.  Verify that your guest account is shown under Assigned Permissions, and has the correct permissions.

#### Task 10: Use PowerShell to read a file after a permissions change
1.  Execute the following PowerShell command:
```
Export-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $path -Destination $download -Force
```
This command should now successfully read and download the **VehicleOwners-downloaded.csv** file.
2.  Execute the following PowerShell command:
```
DIR E:\Labfiles\Lab04\Exercise1\
```
3.  Verify that the **VehicleOwners-downloaded.csv** file has been downloaded to **E:\\Labfiles\\Lab04\\Exercise1**.

#### Task 11: Use PowerShell to attempt to upload a new file
1.  Execute the following PowerShell command:
```
$upload= "/VehicleOwnerData/VehicleOwnersUpdated.csv"
```
2.  Execute the following PowerShell command:
```
$fileName = "E:\Labfiles\Lab04\Exercise1\VehicleOwners.csv"
```
3.  Execute the following PowerShell command:
```
Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload
```
This command will return the following error: Operation returned an invalid status code 'Forbidden'.

#### Task 12: Edit folder permissions to enable file upload
1.  In the Azure portal, use the **Data Explorer** blade to add the following permissions to the **VehicleOwnerData** folder for the guest user account:
 -   Write
2.  Execute the following PowerShell command:
```
Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload
```
3.  Switch to the Azure portal, and on the **Data Explorer** blade, verify that **VehicleOwnersUpdated.csv** has now been successfully uploaded.
4.  On the **Data Explorer** blade, click the ellipses (**...**) for **VehicleOwnersUpdated.csv**, and then click **Access**.
5.  Note that your guest user account has been assigned the permissions that you set in the Default Permissions Entry for the VehicleOwnerData folder in Task 3.
6.  Close the Access blade.

#### Task 13: Use PowerShell to overwrite a file
1.  Execute the following PowerShell command:
```
Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload -Force
```
2.  This command should successfully overwrite the **VehicleOwnersUpdated.csv** file.
3.  To verify the upload, execute the following PowerShell command:
```
Get-AzureRmDataLakeStoreChildItem –AccountName $dataLakeStoreName -Path "/VehicleOwnerData"
```
4.  The **LastWriteTime** for **VehicleOwnersUpdated.csv** should have changed to the current system time (minus the few seconds you took to initiate the previous command).

#### Task 14: Use PowerShell to attempt to overwrite a file after a folder permissions change
1.  Use the **Data Explorer** blade to remove the **Write** permission to the **VehicleOwnerData** folder for your guest account.
2.  Execute the following PowerShell command:
```
Import-AzureRmDataLakeStoreItem -Account $dataLakeStoreName -Path $fileName -Destination $upload -Force
```
This command will return the following error: Operation returned an invalid status code 'Forbidden'.
This is because, although your guest user account has Write permission on the file, it does not have Write permission on the folder. Write privileges on a file only enable the file to be appended; they do not enable deletion or overwrite. Without Write access to the folder, the attempted overwrite fails and the old file is left intact.

>**Result**: At the end of this exercise, you will have created a guest user account in Azure AD, and then tested the ability of this account to view folder contents, open files, and upload and update files in Data Lake Store, depending on the specific permissions that are set. You will use the Azure portal to manage the permissions, and use PowerShell as the guest user environment.

## Exercise 4: Encrypt Data Lake Store data using your own key

### Scenario
As you have already seen, Adatum are building a traffic surveillance system that will use Data Lake Store for primary data storage. Therefore, you will also configure additional security for your Data Lake Store by setting up data encryption using your own managed key. In this exercise, you will create a new Key Vault and key, use this key to protect a new Data Lake Store, and then investigate the effects of key deletion and key restoration on the ability to access data.

The main tasks for this exercise are as follows:
1. Create a new Key Vault account and key
2. Configure a new Data Lake storage account to use Key Vault encryption
3. Use Data Explorer to upload a file and verify its contents
4. Back up the Key Vault key, and then delete the key
5. Attempt to access encrypted data without a key
6. Attempt to upload data to encrypted storage without a key
7. Restore a deleted key and verify data access
8. Lab closedown

#### Task 1: Create a new Key Vault account and key
1.  Use the Azure portal to create a new **Key Vault**, with the following details:
-   **Name**: VehiclesKV
-   **Resource Group (Use existing)**: LakeStoreRG
-   Leave all other settings at their defaults
2.  Wait until the Key Vault has deployed before continuing with the exercise.
3.  Add a **key** called **Key1** to the Key Vault.

#### Task 2: Configure a new Data Lake storage account to use Key Vault encryption
1.  Use the Azure portal to create a new **Data Lake Store**, with the following details:
-   **Name**: lakestore2*&lt;your name&gt;&lt;date&gt;*
-   **Resource Group (Use existing)**: LakeStoreRG
-   **Location**: Select the location you used for the Data Lake Store in Exercise 1
-   **Encryption Settings**: Use Key1 from your Key Vault
2.  Wait until the storage has deployed before continuing with the lab.
3.  Grant your new Data Lake Store permissions to your Key Vault.

#### Task 3: Use Data Explorer to upload a file and verify its contents
1.  Use Data Explorer to upload **E:\\Labfiles\\Lab04\\Exercise4\\testdata1.json** to your new Data Lake Store.
2.  View the contents of **testdata1.json**, and note that the file contains some data about fictitious criminal offenses.

#### Task 4: Back up the Key Vault key, and then delete the key
1.  Use the Azure portal to create a backup of **Key1** in **E:\\Labfiles\\Lab04\\Exercise4**.
2.  Delete **Key1** from your Key Vault.
3.  Wait 2-3 minutes before continuing with the exercise.

#### Task 5: Attempt to access encrypted data without a key
1.  Use Data Explorer to attempt to view the contents of **testdata1.json**.
2.  Note the **RuntimeException** and **Azure Key Vault key not found** message.

#### Task 6: Attempt to upload data to encrypted storage without a key
1.  Use the **Data Explorer** blade to attempt to upload **E:\\Labfiles\\Lab04\\Exercise4\\testdata2.csv**.
2.  Note the Upload Error message and note that the failed upload has created a 0 byte file.

#### Task 7: Restore a deleted key and verify data access
1.  Restore **Key1** to your **Key Vault** from your local backup.
2.  Use Data Explorer to view the contents of **testdata1.json**.
3.  Verify that you can now open the file.
4.  Use Data Explorer to retry the upload of **E:\\Labfiles\\Lab04\\Exercise4\\testdata2.csv**, ensuring that you allow the overwriting of existing files.
5.  Note that you can now upload files.
6.  View the contents of **testdata2.csv**, and note that you can now view the file contents (some data about vehicle registrations).

#### Task 8: Lab closedown
-   In the Azure portal, delete the **LakeStoreRG** resource group.

>**Result**: At the end of this exercise, you will have created a new Key Vault and key, used this key to encrypt a new Data Lake Store, uploaded data to the store, created a key backup and then deleted the key. You will also have attempted to access and upload data after key deletion, and restored the deleted key and verified data access.

**Question**: Why might you set a default permission entry on a folder in Data Lake Store?

**Question**: Is encryption using Key Vault the only way to encrypt data at rest properly in Data Lake Store?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
