# Module 7: Implementing Azure SQL Data Warehouse

# Lab: Implementing SQL Data Warehouse

## Exercise 1: Create and configure a new SQL Data Warehouse

#### Task 1: Install AzCopy and AdlCopy
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, **20776A-LON-SQL**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  On the Start menu, type **Internet Explorer**, and then Press Enter.
3.  In Internet Explorer, go to **https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy**.
4.  On the **Transfer data with the AzCopy on Windows** page, click the **Download and install AzCopy on Windows** link, and then click the **latest version of AzCopy on Windows** link.
5.  In the Internet Explorer message box, click **Run**.
6.  In the **Microsoft Azure Storage Tools – v6.2.0 Setup** dialog box, on the **Welcome to the Microsoft Azure Storage Tools – v6.2.0 Setup Wizard** page, click **Next**.
7.  On the **End-User License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Next**.
8.  On the **Destination Folder** page, click **Next**.
9.  On the **Ready to install Microsoft Azure Storage Tools – v6.2.0** page, click **Install**.
10. In the **User Account Control** dialog box, click **Yes**.
11. On the **Completed the Microsoft Azure Storage Tools – v6.2.0 Setup Wizard** page, click **Finish**.
12. Right-click the Start button, click **System**, and then click **Advanced system settings**.
13. In the **System Properties** dialog box, click **Environment Variables**.
14. In the **Environment Variables** dialog box, under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
15. In the **Edit User Variable** dialog box, in the **Variable value** box, at the end of the existing text, type **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy;**, and then click **OK**.
16. In the **Environment Variables** dialog box, click **OK**.
17. In the **System Properties** dialog box, click **OK**.
18. In Internet Explorer, go to **https://www.microsoft.com/en-us/download/details.aspx?id=50358**.
19. On the **AdlCopy** page, click **Download**.
20. In the Internet Explorer message box, click **Run**.
21. In the **AdlCopy** dialog box, on the **Welcome to the AdlCopy Setup Wizard** page, click **Next**.
22. On the **License Agreement** page, click **I Agree**, and then click **Next**.
23. On the **Select Installation Folder** page, click **Next**.
24. On the **Confirm Installation** page, click **Next**.
25. In the **User Account Control** dialog box, click **Yes**.
26. On the **Installation Complete** page, click **Close**.
27. Close Internet Explorer.
28. In the **System** window, click **Advanced system settings**.
29. In the **System Properties** dialog box, click **Environment Variables**.
30. In the **Environment Variables** dialog box, under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
31. In the **Edit environment variable** dialog box, click **New**.
32. In the text box, type **%HOMEPATH%\\Documents\\AdlCopy**, and then click **OK**.
33. In the **Environment Variables** dialog box, click **OK**.
34. In the **System Properties** dialog box, click **OK**.
35. Close the System window.

#### Task 2: Create a new database server
1.  In the Azure portal, click **+ New**, then in the **Search marketplace** box, type **sql server**, and then click **SQL server (logical server)**.
2.  On the **SQL server (logical server)** blade, click **Create**.
3.  On the **SQL Server (logical server only)** blade, enter the following details:
 -   **Server name**: trafficserver*&lt;your name&gt;&lt;date&gt;*
 -   **Server admin login**: student
 -   **Password**: Pa55w.rd
 -   **Confirm password**: Pa55w.rd
4.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
5.  In the **Location** list, select the same location as you have used for Data Lake Stores in previous labs.
6.  Ensure that the **Allow azure services to access server** check box is selected.
7.  Leave all other settings at their defaults, and click **Create**.
8.  Wait until the server has deployed before continuing with the lab.
9.  In the Azure portal, click **All resources**, and then click **trafficserver*&lt;your name&gt;&lt;date&gt;***.
10.  On the **trafficserver*&lt;your name&gt;&lt;date&gt;*** blade, under **SETTINGS**, click **Firewall / Virtual Networks**, and then click **Add client IP**.
11.  Ensure that **Allow access to Azure services** in set to **ON**, click **Save**, and then click **OK**.

#### Task 3: Create a new SQL Data Warehouse
1.  In the Azure portal, click **+ New**, click **Databases**, and then click **SQL Data Warehouse**.
2.  On the **SQL Data Warehouse** blade, in the **Database name** box, type **trafficwarehouse**.
3.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4.  In the **Select source** list, click **Blank database**.
5.  Click **Server**.
6.  On the **Server** blade, click **trafficserver*&lt;your name&gt;&lt;date&gt;***.
7.  On the **SQL Data Warehouse** blade, set the **Performance level** to **100** DWU.
8.  Leave all other settings at their defaults, and click **Create**.
9.  Wait until the data warehouse has deployed before continuing with the lab.

#### Task 4: Explore the SQL Data Warehouse using SQL Server Management Studio
1.  On the Start menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
2.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
3.  In the **Authentication** list, click **SQL Server Authentication**.
4.  In the **Login** box, type **student**.
5.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6.  In Object Explorer, expand **Databases**, and verify that the **trafficwarehouse** data warehouse is listed.
7.  Right-click **trafficwarehouse**, and then click **New Query**.
8.  In the SQL Editor, type the following statement, and then click **Execute** (you copy the SQL statements in this exercise from **E:\\Labfiles\\Lab07\\Exercise1\\Exercise1.sql**):
	```
	SELECT *
	FROM sys.dm_pdw_nodes
	GO
	```
9.  Verify that this select statement lists a single **CONTROL** node and a single **COMPUTE** node.
10.  Make a note of the value in the **pdw\_node\_id** column of the **COMPUTE** node.
11.  In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:
	```
	SELECT *
	FROM sys.pdw_distributions
	GO
	```
12.  Verify that you see 60 distributions (databases) listed; these distributions should all belong to the **COMPUTE** node (use the value in the **pdw\_node\_id** column to verify this).

#### Task 5: Use scaling with SQL Data Warehouse
1.  Switch to the Azure portal.
2.  In the Azure portal, click **All resources**, and then click **trafficwarehouse**.
3.  On the **trafficwarehouse** blade, under **COMMON TASKS**, click **Scale**.
4.  On the **trafficwarehouse - Scale** blade, set the **Performance level** to **400**, click **Save**, and then click **Yes**.
5.  Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes.
6.  Switch to Microsoft SQL Server Management Studio.
7.  In Object Explorer, click the **Connect Object Explorer** button.
8.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
9.  In the **Authentication** list, click **SQL Server Authentication**.
10. In the **Login** box, type **student**.
11. In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
12. In Object Explorer, click **trafficwarehouse**.
13. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:
	```
	SELECT *
	FROM sys.dm_pdw_nodes
	GO
	```
14.  Verify that this time you see four **COMPUTE** nodes, because you have scaled performance by a factor of four.
15.  In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:
	```
	SELECT *
	FROM sys.pdw_distributions
	GO
	```
16.  Verify that you still see 60 distributions, but they are now spread across the compute nodes (15 distributions per node).
17.  Close the SQL Editor, without saving any changes.
18.  Switch to the Azure portal.
19.  On the **trafficwarehouse - Scale** blade, set the **Performance level** to **100**, click **Save**, and then click **Yes**.
20.  Wait until the data warehouse has resumed after the scaling operation before continuing with the lab; this might take several minutes. It’s important to use an appropriate performance level because SQL Data Warehouse is an expensive resource, and should only be scaled as needed.
21.  Close the **trafficwarehouse - Scale** blade.

>**Result**: At the end of this exercise, you will have created a new database server, created a new data warehouse, explored the data warehouse using SQL Server Management Studio, and used scaling with data warehouse.

## Exercise 2: Design and configure SQL Data Warehouse tables

#### Task 1: Design tables and indexes for a SQL Data Warehouse application
The traffic monitoring system uses the following information:    
**Traffic camera locations**. There are 500 traffic cameras, recording traffic speeds and capturing vehicle registrations. The location of each camera rarely changes.    
**Vehicle speeds**. This data is continually streamed from the traffic cameras. It comprises the identity of the camera, speed limit (this can vary with time), vehicle speed, and vehicle registration. This data is used to perform analysis of traffic patterns, identify hotspots, and so on. It’s also used to send fixed penalty fines and summonses to owners of vehicles caught speeding, and to notify traffic patrols if a suspect (stolen or false) vehicle registration is captured.    
**Vehicle/owner data**. This dataset contains the current vehicle ownership records for every registered vehicle. This data includes the registration number, together with the name (title, forename, surname), and address (lines 1-4) for every vehicle. Currently, there are approximately 7.7 million vehicles registered, but new vehicles are registered often and older vehicles unregistered as they are written off.    
**Stolen vehicles**. This dataset contains the registration number, date reported stolen, and date recovered (or null if the vehicle is still missing) for every vehicle reported stolen. This data is historical—it’s appended to (and updated as vehicles are recovered), but data is never deleted. There are currently 1.2 million stolen vehicle records (approximately 2 percent of vehicles [150,000] are reported stolen each year, and there are currently eight years of data on record—2010-2017 inclusive). The data is expected to grow at a similar rate in the future.    

***Question 1:***
What distribution policy would be most appropriate for each type of data?    
Answer:    
**Vehicle speeds**. Hashed by camera ID. This is high volume data that is likely to grow quickly. Hashing by camera ID will help to spread the load across the data warehouse but ensure that it is found quickly by analytics that examine traffic patterns.    
**Traffic camera locations**. Replicated (small amounts of data that might need to be joined frequently with vehicle speed data).   
**Vehicle/owner data**. Round robin. This data has a relatively slow-moving footprint. It is often retrieved by vehicle registration (making an argument for hashing by registration number), but queries that determine ownership by address are also frequently performed.    
**Stolen vehicles**: Hashed by registration number. Most of the queries that reference this data retrieve it by registration number.    

***Question 2:***
How might your partition the data (if at all)?    
Answer:    
**Vehicle speeds**. Partition by month. Most queries to this data are likely to be about recent incidents, so the current month's data (and possibly the previous month) are the most likely to be accessed. The data also grows by time, so it’s easy for the data warehouse to determine in which partition new data should go (the current month). Also, older data is eventually archived by month more easily.    
**Traffic camera locations**. No partitions.    
**Vehicle/owner data**. No partitions.    
**Stolen vehicles**. Partition by year. Most records for unrecovered vehicles will be for the current/previous year. Older data is more likely to indicate a recovered date.    

#### Task 2: Use SQL Server Management Server to create data warehouse tables and indexes
1.  In Microsoft SQL Server Management Studio, in Object Explorer, click the **Connect Object Explorer** button.
2.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
3.  In the **Authentication** list, click **SQL Server Authentication**.
4.  In the **Login** box, type **student**.
5.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6.  In Object Explorer, right-click **trafficwarehouse**, and then click **New Query**.
7.  In the SQL Editor, type the following statement (you copy the SQL statements in this exercise from **E:\\Labfiles\\Lab07\\Exercise2\\Exercise2.sql**), and then click **Execute**:
	```
	CREATE TABLE VehicleSpeed
	(
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate DATETIME NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH
	(
	HEAP,
	DISTRIBUTION = HASH(CameraID),
	PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
	)
	GO

    CREATE TABLE CameraLocation
	(
	CameraID VARCHAR(10) NOT NULL,
	GPSLocationX FLOAT NOT NULL,
	GPSLocationY FLOAT NOT NULL
	)
	WITH
	(
	HEAP,
	DISTRIBUTION = REPLICATE
	)
	GO

    CREATE TABLE VehicleOwner
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	WITH
	(
	CLUSTERED COLUMNSTORE INDEX,
	DISTRIBUTION = ROUND_ROBIN
	)
	GO

    CREATE TABLE StolenVehicle
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	DateStolen DATETIME NOT NULL,
	DateRecovered DATETIME NULL,
	YearStolen INT NOT NULL
	)
	WITH
	(
	CLUSTERED COLUMNSTORE INDEX,
	DISTRIBUTION = HASH(VehicleRegistration),
	PARTITION (YearStolen RANGE FOR VALUES(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017))
	)
	GO
	```
8.  In Object Explorer, expand **trafficwarehouse**, expand **Tables**, and verify that the **dbo.VehicleSpeed**, **dbo.CameraLocation**, **dbo.VehicleOwner**, and **dbo.StolenVehicle** tables have been created.
9.  Close the SQL Editor window, without saving any changes.

>**Result**: At the end of this exercise, you will have designed tables and indexes for a data warehouse application, and used SQL Server Management Server to create the required data warehouse tables and indexes.

## Exercise 3: Import static data into SQL Data Warehouse

#### Task 1: Stage data in Data Lake Store prior to SQL Data Warehouse import
**Note**: This task is only necessary if you have not performed exercise 3 of module 6 which uses the same data. If you have already staged the stolen vehicle data in Data Lake Store, then skip to the next task to stage data in an on-premises SQL Server database.
1.  In the Azure portal, click **+ New**, click **Storage**, and then click **Storage account - blob, file, table, queue**.
2.  On the **Create Storage account** blade, in the **Name** box, type **vehicledata*&lt;your name&gt;&lt;date&gt;***.
3.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
4.  In the **Location** list, select the same location as you used for the data warehouse in Exercise 1.
5.  Leave all other details at their defaults, and click **Create**.
6.  Wait until the storage account has been successfully created before continuing with the exercise.
7.  Click **All resources**, and then click **vehicledata*&lt;your name&gt;&lt;date&gt;***.
8.  On the **vehicledata*&lt;your name&gt;&lt;date&gt;*** blade, under **BLOB SERVICE**, click **Containers**, and then click **+ Container**.
9.  In the **New container** dialog box, in the **Name** box, type **stolen**, and then click **OK**.
10. On the **vehicledata*&lt;your name&gt;&lt;date&gt;* -** **Containers** blade, under **SETTINGS**, click **Access keys**.
11. Next to **key1**, click the **Click to copy** button, to copy the key to the clipboard.
12. Right-click the Start button, and then click **Command Prompt (Admin)**.
13. In the **User Account Control** dialog box, click **Yes**.
14. At the command prompt, type the following command, and then press Enter:
	```
    dir E:\Labfiles\Lab07\Exercise3\StolenVehicles /s
	```
15.  Note that the **StolenVehicles** folder contains eight years of stolen vehicle data, organized in subfolders by year/month/day; there are 2,914 separate CSV files.
16.  At the command prompt, type the following command (replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the key you copied to the clipboard), and then press Enter:
	```
	azcopy /Source:"E:\Labfiles\Lab07\Exercise3\StolenVehicles" /Dest:https://<storage account name>.blob.core.windows.net/stolen /DestKey:<storage key> /S
	```
You copy this command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd1.txt**.
17.  The copy process might take several minutes to complete; wait until all files have been copied before continuing with the exercise.
18.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
19.  In Visual Studio, on the **View** menu, click **Cloud Explorer**.
20.  In Cloud Explorer, if you do not have your Azure Learning Pass subscription folder, complete the following steps:
    1.  Click the **Azure** **Account settings** icon, and then click **Add an account**.
    2.  In the **Sign in to your account** dialog box, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
    3.  In Cloud Explorer, select your Azure Learning Pass subscription, and then click **Apply**..
21.  Under your Azure Learning Pass subscription folder, expand **Storage Accounts**, expand **vehicledata*&lt;your name&gt;&lt;date&gt;***, expand **Blob Containers**, and then double-click **stolen**.
22.  Verify that the files and subfolders have been uploaded.
23.  Switch to the Azure portal.
24.  Click **All resources**, click **adls*&lt;your name&gt;&lt;date&gt;***, and then click **Data Explorer**.
25.  In Data Explorer, click **New Folder**.
26. In the **Create new folder** box, type **Stolen**, and then click **OK**.
27. Switch to the command prompt.
28. At the command prompt, type the following command (replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, replacing **&lt;Data Lake Store name&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the blob store key you copied to the clipboard), and then press Enter:
	```
	adlcopy /source https://<storage account name>.blob.core.windows.net/stolen/ /dest adl://<Data Lake Store name>.azuredatalakestore.net/Stolen/ /sourcekey <storage key>
	```
You copy the preceding command from **E:\\Labfiles\\Lab07\\Exercise3\\AdlCopyCmd.txt**.
29.  If a **Sign in to your account** dialog box appears, sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
30.  Note: Depending on the region and the location of the Blob storage account (ideally they should be in the same region, but might not be), AdlCopy will take from two to 20 minutes to copy the data. Ignore the stats that indicate the percentage of files copied—it sits at 0.00% until complete then jumps to 100%, and may copy the data in three phases (files 1 to 1000, then files 1001 to 2000, and then the remainder).
31.  Switch to the Azure portal.
32.  In Data Explorer, click the **Stolen** folder, and verify that all the files and folders have been copied across.

#### Task 2: Stage data in an on-premises SQL Server database prior to SQL Data Warehouse import
1.  Switch to Microsoft SQL Server Management Studio.
2.  In Object Explorer, click the **Connect Object Explorer** button.
3.  In the **Connect to Server** dialog box, in the **Server name** box, type **LON-SQL**.
4.  In the **Authentication** list, click **Windows Authentication**, and then click **Connect**.
5.  In Object Explorer, expand **LON-SQL**, right-click **Databases**, and then click **New Database**.
6.  In the **New Database** dialog box, in the **Database name** box, type **VehicleInfo**, and then click **OK**.
7.  In Object Explorer, expand **Databases**, right-click **VehicleInfo**, and click **New Query**.
8.  In the SQL Editor, type the following command (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd1.txt**), and then click **Execute**:
	```
	USE VehicleInfo
	GO
	-- Create VehicleOwner table
	CREATE TABLE VehicleOwner
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	GO
	```
9.  Switch to the command prompt.
10.  At the command prompt, type the following command, and then press Enter:
	```
	E:
	```
11.  At the command prompt, type the following command, and then press Enter:
	```
	cd E:\Labfiles\Lab07\Exercise3
	```
12.  At the command prompt, type the following command (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\BcpCmd1.txt**), and then press Enter:
	```
	bcp VehicleInfo.dbo.VehicleOwner in ownerdata.csv /T /SLON-SQL /c /t,
	```
Ensure that you include the comma at the end of the line.
13.  The bcp command should upload more than 7.7 million rows to the database (it’s fairly quick, and should take no more than five minutes).
14.  Switch to Microsoft SQL Server Management Studio.
15.  In the SQL Editor, replace the existing code with the following statement (you copy this from **E:\\Labfiles\\Lab07\\Exercise3\\SqlCmd2.txt**), and then click **Execute**:
	```
	SELECT TOP(1000) *
	FROM VehicleOwner
	GO
	```
16.  This command should display the first 1,000 rows of data; the names and addresses contain random strings, but the vehicle registrations tie in with those of the stolen vehicle data (and the data generated by the speed cameras that you will use in Exercise 4).
17.  Close the SQL Editor window, without saving any changes.
18.  In Object Explorer, right-click **LON-SQL**, and then click **Disconnect**.

#### Task 3: Import data from a local CSV file into SQL Data Warehouse
1.  Switch to the Azure portal.
2.  Click **All resources**, and then click **vehicledata*&lt;your name&gt;&lt;date&gt;***.
3.  On the **vehicledata*&lt;your name&gt;&lt;date&gt;*** blade, under **BLOB SERVICE**, click **Containers**, and then click **+ Container**.
4.  In the **New container** dialog box, in the **Name** box, type **locationdata**, and then click **OK**.
5.  On the **vehicledata*&lt;your name&gt;&lt;date&gt;*** blade, under **SETTINGS**, click **Access keys**.
6.  Next to **key1**, click the **Click to copy** button, to copy the key to the clipboard.
7.  Switch to the command prompt.
8.  At the command prompt, type the following command (replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the key you copied to the clipboard), and then press Enter:
	```
	azcopy /Source:"E:\Labfiles\Lab07\Exercise3\" /Pattern:CameraData.csv /Dest:https://<storage account name>.blob.core.windows.net/locationdata /DestKey:<storage key>
	```
You copy this command from **E:\\Labfiles\\Lab07\\Exercise3\\AzCopyCmd2.txt**.
9.  Switch to Microsoft SQL Server Management Studio.
10.  In Object Explorer, click **Connect Object Explorer** to re-establish the database connection with the correct credentials.
11.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
12.  In the **Authentication** list, click **SQL Server Authentication**.
13.  In the **Login** box, type **student**.
14.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
15.  On the **File** menu, point to **Open**, and then click **File**.
16.  In the **Open File** dialog box, go to **E:\\Labfiles\\Lab07\\Exercise3**, click **SpeedCameraLocationScript.sql**, and then click **Open**.
17.  In Object Explorer, ensure that **trafficserver*&lt;your name&gt;&lt;date&gt;*** is selected.
18. In the SQL Editor, locate the following command:
	```
	CREATE MASTER KEY ENCRYPTION BY PASSWORD='<password>'
	GO
	```
19.  Edit the command, replacing **&lt;password&gt;** with your own password (for example, Pa55w.rd).
20.  In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
21.  In the SQL Editor, locate the following command:
	```
	CREATE DATABASE SCOPED CREDENTIAL CredentialsToBlobStorage
	WITH IDENTITY = '<storage account name>',
	SECRET = '<storage key>';
	GO
	```
22.  Edit the command, replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage key&gt;** with the key you copied to the clipboard).
23.  In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
24.  In the SQL Editor, locate the following command:
	```
	CREATE EXTERNAL DATA SOURCE LocationDataSource
	WITH (
	TYPE = HADOOP,
	LOCATION = 'wasbs://locationdata@<storage account name>.blob.core.windows.net',
	CREDENTIAL = CredentialsToBlobStorage
	)
	GO
	```
25.  Edit the command, replacing **&lt;storage account name&gt;** with **vehicledata*&lt;your name&gt;&lt;date&gt;***.
26.  In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
27.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE EXTERNAL FILE FORMAT CommaSeparatedFileFormat
	WITH (
	FORMAT_TYPE = DelimitedText,
	FORMAT_OPTIONS (FIELD_TERMINATOR =',')
	)
	GO
	```
28.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE EXTERNAL TABLE ExternalLocationData (
	CameraID VARCHAR(10) NOT NULL,
	GPSLocationX FLOAT NOT NULL,
	GPSLocationY FLOAT NOT NULL)
	WITH (
	LOCATION='CameraData.csv',
	DATA_SOURCE = LocationDataSource,
	FILE_FORMAT = CommaSeparatedFileFormat,
	REJECT_TYPE = VALUE,
	REJECT_VALUE = 0
	)
	GO
	```
29.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT *
	FROM ExternalLocationData
	GO
	```
30.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	DROP TABLE CameraLocation
	GO
	```
31.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE TABLE CameraLocation
	WITH
	(
	HEAP,
	DISTRIBUTION = REPLICATE
	)
	AS SELECT CameraID, GPSLocationX, GPSLocationY
	FROM ExternalLocationData
	GO
	```
32.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT *
	FROM CameraLocation
	GO
	```
33.  Close the SQL Editor window, without saving any changes.

#### Task 4: Import data from Data Lake Store into SQL Data Warehouse
1.  Switch to the Azure portal.
2.  Click **Azure Active Directory**, under **MANAGE**, click **App registrations**, and then click **New application registration**.
3.  On the **Create** blade, enter the following details, and then click **Create**:
 -   **Name**: ADLSToPolyBase
 -   **Application type**: Web app / API
 -   **Sign-on URL**: https://ADLSToPolyBase/Dummy
    Note that the actual URL entered on this blade is immaterial because you don’t actually build or deploy an app at this location; it’s merely acting as an identifier in this example.
4.  Click the **ADLSToPolyBase** application, next to **Application ID**, and then click the **Click to copy** button.
5.  On the Start menu, type **Notepad**, and then press Enter.
6.  In Notepad, type **&lt;application ID&gt;**, and then press Enter.
7.  On the **Edit** menu, click **Paste**, to store the copied Application ID.
8.  On the **File** menu, click **Save**, and save the file as **App\_details.txt** in your **Documents** folder.
9.  Switch to the Azure portal.
10.  On the **Settings** blade for **ADLSToPolyBase**, under **API ACCESS**, click **Keys**.
11.  On the **Keys** blade, enter the following information, and then click **Save**:
 -   **Key description**: Key1
 -   **Duration**: In 1 year
12.  Select the **VALUE** for **Key1**, then right-click, and click **Copy**.
13.  Switch to Notepad.
14.  Press CTRL+END, press Enter twice, type **Key1**, and then press Enter.
15.  On the **Edit** menu, click **Paste**, to store the API key.
16.  Switch to the Azure portal.
17.  Close the **Keys**, **Settings**, and **ADLSToPolyBase** blades.
18.  On the **Azure Active Directory** blade, under **MANAGE**, click **Properties**.
19.  On the **Properties** blade, next to **Directory ID**, click the **Click to copy** button.
20.  Switch to Notepad.
21. Press CTRL+END, press Enter twice, type **Directory ID**, and then press Enter.
22. On the **Edit** menu, click **Paste**, to store the Directory ID.
23. In Notepad, on the **File** menu, click **Save**.
24. Switch to the Azure portal.
25. Click **All resources**, click **adls*&lt;your name&gt;&lt;date&gt;***, and then click **Data Explorer**.
26. In Azure Data Lake Explorer, ensure the root folder (**adls*&lt;your name&gt;&lt;date&gt;***) is open, and then click **Access**.
27. On the **Access** blade, click **Add**.
28. On the **Select User or Group** blade, click **ADLSToPolyBase**, and then click **Select**.
29. On the **Select Permissions** blade, select the **Read**, **Write**, and **Execute** check boxes, click **This folder and all children**, and then click **OK**.
30. **IMPORTANT**. Wait while permissions are assigned to **ADLSPolyBase**; notice that the assignments to each file and folder is displayed in the **Assigning permissions to ...** box, in the **Access** blade. Do not continue with the exercise until you see the notification that the permissions assignment has successfully completed (this might take several minutes).
31. Close the **Access** blade.
32. Switch to Microsoft SQL Server Management Studio.
33. On the **File** menu, point to **Open**, and then click **File**.
34. In the **Open File** dialog box, go to **E:\\Labfiles\\Lab07\\Exercise3**, click **StolenVehicleDataScript.sql**, and then click **Open**.
35. In Object Explorer, ensure that **trafficserver*&lt;your name&gt;&lt;date&gt;*** is selected.
36. In the SQL Editor, locate the following command:
	```
	CREATE DATABASE SCOPED CREDENTIAL ADLCredential
	WITH
	IDENTITY = '<application ID>@https://login.windows.net/<directory ID>/oauth2/token',
	SECRET = '<key>'
	GO
	```
37.  Edit the command, replacing **&lt;application ID&gt;**, **&lt;directory ID&gt;**, and **&lt;key&gt;** with the values you saved to **App\_details.txt** in Notepad.
38.  In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
39.  In the SQL Editor, locate the following command:
	```
	CREATE EXTERNAL DATA SOURCE StolenVehicleDataSource
	WITH (
	TYPE = HADOOP,
	LOCATION = 'adl://<Data Lake Store name>.azuredatalakestore.net',
	CREDENTIAL = ADLCredential
	)
	GO
	```
40.  Edit the command, replacing **&lt;Data Lake Store name&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***.
41.  In the SQL Editor, highlight your edited version of the preceding command, and on the toolbar, click **Execute**.
42.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE EXTERNAL FILE FORMAT DelimitedCsvTextFileFormat
	WITH (
	FORMAT_TYPE = DelimitedText,
	FORMAT_OPTIONS (
	FIELD_TERMINATOR = ',',
	STRING_DELIMITER = '"'
	));
	GO
	```
43.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE EXTERNAL TABLE ExternalStolenVehicleData (
	VehicleRegistration VARCHAR(7) NOT NULL,
	DateStolen DATETIME NOT NULL,
	DateRecovered DATETIME NULL )
	WITH (
	LOCATION='/Stolen/',
	DATA_SOURCE = StolenVehicleDataSource,
	FILE_FORMAT = DelimitedCsvTextFileFormat,
	REJECT_TYPE = PERCENTAGE,
	REJECT_VALUE = 5,
	REJECT_SAMPLE_VALUE = 1000
	);
	GO
	```
44.  It might take a couple of minutes to create this table; while you are waiting, note the following configuration details:
 -   DateRecovered must allow NULL values.
 -   The date fields are defined as VARCHAR rather than DATETIME in the external table. When you transfer the data into the data warehouse, you will convert this data, and also extract the year from the DateStolen field to use as the partition key.
 -   Not all of the rows in each file in ADLS contain valid data (there are some header rows, and there could also be some other forms of corruption, given the volume of the data). Specify that you will allow up to 5 percent of the rows retrieved by a query to be discarded if they do not appear to contain valid data. Sample over every 1,000 rows.
45.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT TOP(1000) *
	FROM ExternalStolenVehicleData
	GO
	```
46.  This query is likely to be slow (three or four minutes).
47.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	INSERT INTO StolenVehicle(VehicleRegistration, DateStolen, DateRecovered, YearStolen)
	SELECT VehicleRegistration, CONVERT(DATETIME, DateStolen), CONVERT(DATETIME, DateRecovered), DATEPART(yyyy, CONVERT(DATETIME, DateStolen))
	FROM ExternalStolenVehicleData
	GO
	```
48.  This operation will take some time to perform (three or four minutes). Additionally, you should see that 2,914 rows are rejected (these are the rows containing headers rather than data)—this is normal.
49.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT TOP(1000) *
	FROM StolenVehicle
	GO
	```
50.  Close the SQL Server Management Studio window, without saving any changes.

#### Task 5: Import data from an on-premises SQL Server database into SQL Data Warehouse
1.  In Visual Studio, on the **Tools** menu, click **Extension and Updates**.
2.  In the **Extension and Updates** dialog box, expand **Updates**, and then click **Product Updates**.
3.  In the middle pane, click **Update**.
4.  In the **User Account Control** dialog box, click **Yes**.
5.  On the **Please update Visual Studio Installer before proceeding** page, click **Update**.
6.  On the **Before we get started** page, click **Continue**.
7.  On the **Installed** page, click **Launch**.
8.  Close all existing instances of Visual Studio.
9.  In Internet Explorer, go to **https://go.microsoft.com/fwlink/?linkid=853836**.
10. In the Internet Explorer message box, click **Run**.
11. In the **Microsoft SQL Server Data Tools** dialog box, on the **Welcome** page, click **Next**.
12. In the **Install tools to this Visual Studio 2017 instance** list, click Visual Studio Enterprise 2017.
13. Under **Install tools for these SQL Server features**, select all the check boxes, and then click **Install**.
14. In the **User Account Control** dialog box, click **Yes**.
15. On the **Setup Completed** page, click **Close**.
16. Reboot the virtual machine.
17. Log on as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
18. In Internet Explorer, go to **https://www.microsoft.com/en-us/download/details.aspx?id=54798**, and then click **Download**.
19. On the **Choose the download you want** page, select the **SsisAzureFeaturePack\_2017\_x64.msi** check box, and then click **Next**.
20. In the Internet Explorer message box, click **Run**.
21. In the **Microsoft SQL Server Feature Pack Setup** dialog box, on the **Welcome to the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x64) Setup Wizard** page, click **Next**.
22. On the **Please read the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x64) License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Install**.
23. In the **User Account Control** dialog box, click **Yes**.
24. On the **Completed the Microsoft SQL Server 2017 Integration Services Feature Pack for Azure (x64) Setup Wizard** page, click **Finish**.
25. Close Internet Explorer.
26. On the Start menu, type **Visual Studio 2017**, and then press Enter.
27. In Visual Studio, on the **File** menu, point to **New**, and then click **Project**.
28. In the **New Project** dialog box, expand **Business Intelligence**, click **Integration Services**, and then click **Integration Service Project**.
29. In the **Name** box, type **ImportVehicleOwnerData**, and then click **OK**.
30. In the SSIS Toolbox, double-click **Data Flow Task**, to add this object to the Design pane.
31. At the top of the Design pane, click the **Data Flow** tab.
32. In the SSIS Toolbox, expand **Other Sources**, double-click **ADO NET Source**, to add this object to the Data Flow Task pane.
33. In the **Design** pane, double-click **ADO NET Source**.
34. In the **ADO.NET Source Editor** dialog box, click **New**.
35. In the **Configure ADO.NET Connection Manager** dialog box, click **New**.
36. In the **Configure ADO.NET Connection Manager** dialog box, in the **Server name** box, type **LON-SQL**.
37. In the **Connect to a database** section, click **Select or enter a database name**, in the box, type **VehicleInfo**, and then click **Test Connection**.
38. In the **Connection Manager** dialog box, click **OK**.
39. In the **Connection Manager** dialog box, click **OK**.
40. In the **ADO.NET Source Editor** dialog box, in the **Name of the table or the view** list, click **"dbo"."VehicleOwner"**, and then click **Columns**.
41. Verify that all of the columns in the **External Column** table are mapped to the corresponding columns in the **Output Column** table, and then click **OK**.
42. In the SSIS Toolbox, expand **Other Destinations**, double-click **ADO NET Destination**, and then drag this object to the Data Flow Task pane under the **ADO NET Source object**.
43. In the **Data Flow Task** pane, click **ADO NET Source**, and then click and drag the blue connector to **ADO NET Destination**.
44. In the **Data Flow Task** pane, double-click **ADO NET Destination**.
45. In the **ADO.NET Destination Editor** dialog box, next to the **Connection manager** list, click **New**.
46. In the **Configure ADO.NET Connection Manager** dialog box, click **New**.
47. In the **Connection Manager** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
48. In the **Authentication** list, click **SQL Server Authentication**.
49. In the **User name** box, type **student**, in the **Password** box, type **Pa55w.rd**, and then select the **Save my password** check box.
50. In the **Select or enter a database name** box, type **trafficwarehouse**, and then click **OK**.
51. In the **Configure ADO.NET Connection Manager** dialog box, click **OK**.
52. In the **ADO.NET Destination Editor** dialog box, in the **Use a table or view** list, click **"[dbo]"."[VehicleOwner]"**, and then click **Mappings**.
53. Verify that all of the columns in the source table are mapped to the corresponding columns in the destination table, and then click **OK**.
54. On the **File** menu, click **Save All**.
55. On the **Debug** menu, click **Start Debugging** to run the package and perform the upload then wait until the upload completes.
56. On the **Debug** menu, click **Stop Debugging**.
57. Close Visual Studio.

>**Result**: At the end of this exercise, you will have:
-   Staged data in Data Lake Store prior to SQL Data Warehouse import.
-   Staged data in an on-premises SQL Server database prior to SQL Data Warehouse import.
-   Imported data from a local CSV file directly into SQL Data Warehouse.
-   Imported data from Data Lake Store into SQL Data Warehouse.
-   Imported data from an on-premises SQL Server database into SQL Data Warehouse.

## Exercise 4: Stream dynamic data to SQL Data Warehouse

#### Task 1: Configure an Azure Stream Analytics job to output to SQL Data Warehouse
1.  In the Azure portal, click **All resources**, and then click the **CaptureTrafficData** Azure Stream Analytics job; remember that this job captures data from the speed cameras and writes it to Data Lake storage.
2.  On the **CaptureTrafficData** blade, under **JOB TOPOLOGY**, click **Outputs**, and then click **+ Add**.
3.  On the **New output** blade, enter the following details:
 -   **Output alias**: DataWarehouse
 -   **Sink**: SQL database
 -   **Import option**: Use SQL database from current subscription
 -   **Database**: trafficwarehouse
 -   **Username**: student
 -   **Password**: Pa55w.rd
 -   **Table**: VehicleSpeed
4.  Leave all other settings at their defaults, and click **Create**.
5.  Wait until the output has been successfully created before continuing with the lab.
6.  On the **CaptureTrafficData - Outputs** blade, under **JOB TOPOLOGY**, click **Query**, and then add the following to the end of the existing query:
	```
	SELECT
	CameraID, SpeedLimit, Speed, VehicleRegistration, Time AS WhenDate, DATEPART(month, Time) AS WhenMonth
	INTO
	DataWarehouse
	FROM
	CameraDataFeed4
	```
You copy the preceding commands from **E:\\Labfiles\\Lab07\\Exercise4\\ASAquery.txt**.
7 .  Click **Save**, and then click **Yes**.
8 .  Note that the names of the columns or aliases in the SELECT statement must match the names of the columns in the table in the SQL Data Warehouse; remember that the VehicleSpeed table uses the WhenMonth column to partition the data.
9 .  On the **CaptureTrafficData - Query** blade, click **Overview**, and then click **Start**.
10.  On the **Start job** blade, click **Now**, and then click **Start**.
11.  Wait until the job has been successfully started before continuing with the lab.

#### Task 2: Configure a Visual Studio app to use the Stream Analytics job
1.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
2.  In Visual Studio, click **Open Project / Solution**.
3.  In the **Open Project** dialog box, go to the **E:\\Labfiles\\Lab07\\Exercise4\\SpeedCameraDevice** folder, click **SpeedCameraDevice.sln**, and then click **Open**.
4.  Note that this version of the application captures data from 500 speed cameras.
5.  In Solution Explorer, double-click **App.config**.
6.  In App.config, in the **appSettings** section, in the **ServiceBusConnectionString** key, replace **YourNamespace** with ***camerafeeds&lt;your name&gt;&lt;date&gt;***, and replace **YourPrimaryKey** with the event hub primary key you copied to **Config\_details.txt** in Lab 2, Exercise 3.
7.  In Solution Explorer, right-click **SpeedCameraDriver**, and then click **Set as StartUp Project**.
8.  On the **Build** menu, click **Build Solution**.
9.  Verify that the app compiles successfully, and then click **Start**.
10. Verify that the app opens a console window displaying generated speed camera data that is being sent to the event hub.

#### Task 3: View Stream Analytics job data in SQL Data Warehouse
1.  On the Start menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
2.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
3.  In the **Authentication** list, click **SQL Server Authentication**.
4.  In the **Login** box, type **student**.
5.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
6.  In Object Explorer, expand **Databases**, expand **trafficwarehouse**, expand **Tables**, right-click **dbo.VehicleSpeed**, and then click **Select Top 1000 Rows**.
7.  Verify that the first 1000 rows of data appear.

#### Task 4: Lab cleanup
1.  Switch to the Visual Studio app window for **SpeedCameraDevice**, and press Enter to stop the app.
2.  Close Visual Studio and SQL Server Management Studio.
3.  Switch to the Azure portal.
4.  On the **CaptureTrafficData** blade, click **Stop**, and then click **Yes**.
5.  In the Azure portal, click **All resources**, and then click **trafficwarehouse**.
6.  On the **trafficwarehouse** blade, click **Pause**, and then click **Yes**.
7.  Remember that resources are billed for all the time that the data warehouse is running, even if it is not actively performing a task. If you are not going to use the data warehouse for a while, you should click **Pause** to stop the warehouse and release resources. You will not be billed for the time the data warehouse is paused. If you don't do this, you will quickly run out of Azure credits!

>**Result**: At the end of this exercise, you will have configured a Stream Analytics job to output to SQL Data Warehouse, configured a Visual Studio app to use the Stream Analytics job, and viewed Stream Analytics job data in SQL Data Warehouse.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
