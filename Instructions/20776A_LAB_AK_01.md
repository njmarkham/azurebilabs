# Module 1: Architectures for Big Data Engineering with Azure

# Lab: Designing a big data architecture

## Exercise 1: Capturing vehicle data and alerting patrol cars

#### Task 1: Handling incoming speed camera data
Answers to the questions might vary, but the following suggestions are used to implement the system in subsequent labs:
1.  There could be many hundreds or even thousands of speed cameras. The system must be scalable to support a large number of devices.
2.  You could use Azure Event Hubs or IoT Hubs to send data to the PCCC. The solution uses a single event hub.
3.  The PCCC can use Azure Stream Analytics (ASA) to process the data read from the event hub. ASA is designed to support this type of solution.
4.  ASA can send data as a stream to Power BI. You can use Power BI to generate real-time reports based on this stream.
5.  The PCCC can detect possible traffic incidents by calculating the average speed passing the camera during a given time window and comparing it to the overall average speed for that camera. For example, if traffic typically passes a camera at between 30 and 40 mph, but the speed drops by 1 or 2 mph for several minutes, this could indicate some sort of blockage caused by an accident. You use an Azure Machine Learning model to detect speed anomalies for a camera.
6.  The PCCC can detect whether a vehicle might be fitted with false registration plates by determining whether the same vehicle registration has been spotted by two cameras when the distance between the cameras is too great for the vehicle to have reasonably travelled between them during the interval between observations. ASA enables you to include complex processing over data for a specified time window by defining user-defined functions in C\#. These functions are compiled and can execute very quickly.
7.  You should save the streamed data to fast, long-term storage. There could be a vast amount of data gathered over time. Consider saving the data initially to Azure Data Lake Storage (for speed), but then running a separate batch process to restructure this data to an analytical data store, such as Azure SQL Data Warehouse.

#### Task 2: Communicating between the PCCC and the patrol cars
1.  As with the speed camera data, the PCCC can use ASA to capture and process patrol car information. Note that you should create a separate ASA job for handling patrol car information; don’t try to process patrol car and speed camera information as part of the same job. Additionally, you can log patrol car data to Data Lake Storage, and then to SQL Data Warehouse for further batch analysis and reporting.
2.  You use ASA to post messages to Service Bus queues and topics. This approach provides a reliable mechanism for sending data back to patrol cars. The software running in the patrol cars can connect to a topic, read messages, and signal alerts to the officers in the car.

#### Task 3: Incorporating supplementary data
1.  The PCCC needs an up-to-date list of records of stolen vehicles. This is likely to come from another system.
2.  The stolen vehicle data is likely to change at irregular intervals. Given the volume of vehicles on the road, there could be several hundreds of changes each day, but the records for a single vehicle are unlikely to be amended more than once every few months (or possibly years).
3.  The data from the external systems sourcing the stolen vehicle records can be written to Data Lake Storage by a separate batch process. An ASA job can incorporate data held in Data Lake Storage files into the stream processing for that job.

>**Result**: At the end of this exercise, you will have selected the technologies that support the stream processing requirements listed in the scenario.

## Exercise 2: Performing batch processing

#### Task 1: Generating fines and summonses
Answers to the questions might vary, but the following suggestions are used to implement the system in subsequent labs:
1.  The PCCC needs an up-to-date list of vehicle owner records, containing the name and address of the owner for every vehicle. This data is likely to come from another system.
2.  Azure Data Lake Analytics is arguably the best suited technology to use for this processing. It’s optimized to perform batch analytics jobs.
3.  You could save the data in tabular form in the catalog in the Data Lake Store. Create a database, and add a table for holding the speed data. You can index and cluster the data for fast access.

#### Task 2: Reporting and analyzing data in batch
1.  Data Lake Analytics enables you to partition data over a specified number of observations or a time interval. You can perform aggregations, such as AVERAGE, COUNT, MAX, and MIN over the data in a partition, and group it by an attribute of the data, such as the Camera ID. Additionally, to handle complex data structures, you can implement custom processors, appliers, combiners, reducers, aggregators, and extractors.
2.  You have several options for presenting the reports generated by using Data Lake Analytics:
    -   For text-based reports, you write the report data to a file in Data Lake Store.
    -   For graphical reports, you send the data to Power BI.
3.  Data Lake Analytics enables you to include existing routines written in R and Python into the processing for a Data Lake Analytics job. You can also integrate Azure Cognitive Services (for example, to perform operations such as facial recognition over image data).

>**Result**: At the end of this exercise, you will have selected the technologies that support the batch processing requirements listed in the scenario.

## Exercise 3: Storing long-term data and performing analytical processing

#### Task 1: Determining long-term data storage and processing requirements
Answers to the questions might vary, but the following suggestions are used to implement the system in subsequent labs:
1.  The data might come from flat files held in Data Lake Storage or Blob storage, the Data Lake Storage catalog, external databases (including on-premises SQL Server databases), or even be streamed directly from ASA.
2.  Effectively, there could be an almost unlimited volume of data, especially if data is being streamed into long-term storage. The storage must be scalable, but enable you to implement an archiving strategy for old data.
3.  The type of analytical processing could vary from simple queries to detailed examination by statistical packages. The data might be used by machine learning to make predictions; for example, it could be used to help model the effects on traffic flow if road junctions are modified.
4.  Given that there could be vast quantities of data, the processing requirements could be equally vast. Any solution must be scalable but elastic to keep costs at a reasonable level.

#### Task 2: Migrating data to long-term analytical storage
1.  Arguably the most suitable repository is Azure SQL Data Warehouse. This is a highly scalable relational solution that is optimized to support analytical processing. You can access data held in Azure SQL Data Warehouse, in much the same way as any SQL Server database, by using tools such as SQL Server Management Studio. You can also write your own custom code to query data, and you can connect to a SQL Data Warehouse from Azure Machine Learning.
2.  SQL Data Warehouse supports a large number of migration options. For example, you can:
    -   Import data from SQL Server by using bcp, AZCopy, SQL Server Integration Services, or the Import/Export service.
    -   Import data from flat files held in Data Lake Storage and Blob storage by using PolyBase.
    -   Write data directly to SQL Data Warehouse from an ASA job.
    You can also use Azure Data Factory to transform and transfer data into SQL Data Warehouse as a series of scheduled batch jobs. This approach is useful if you need to update and import fresh data on a regular basis.

>**Result**: At the end of this exercise, you will have selected the technologies that support the data storage and detailed analytical processing requirements of the system.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.