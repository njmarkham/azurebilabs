# Module 2: Processing Event Streams using Azure Stream Analytics

# Lab: Process event streams with Stream Analytics

### Scenario
You work for Adatum as a data engineer, and have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data that holds vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For the first phase of the project, you will use Stream Analytics, together with Event Hubs, IoT Hubs, Service Bus, and custom applications to:
- Provide insights into average speeds at various locations.
- Determine the locations of police patrol cars.
- Present vehicle locations on a map.
- Check vehicles recorded by speed cameras against a list of stolen vehicles.
- Determine the nearest patrol car to a speeding vehicle or stolen vehicle, send a dispatch alert to the nearest patrol car, and show the dispatched patrol car locations on a map.
- Use Stream Analytics monitoring and alerting tools to help identify issues during the system's deployment, and use the Azure portal and PowerShell to scale up and scale down the system to cope with particular demands.

### Objectives
After completing this lab, you will be able to:
- Create a Stream Analytics job to process event hub data.
- Create a Stream Analytics job to process IoT hub data.
- Reconfigure a Stream Analytics job to send output through a Service Bus queue.
- Reconfigure a Stream Analytics job to process both event hub and static file data.
- Use multiple Stream Analytics jobs to process event hub, IoT hub and static file data, and output results using a Service Bus and custom application.
- Use the Azure portal and PowerShell to manage and scale Stream Analytics jobs.

### Lab Setup
Estimated time: 120 minutes
Virtual machine: **20776A-LON-DEV**  
User name: **ADATUM\AdatumAdmin**  
Password: **Pa55w.rd**

## Exercise 1: Create a Stream Analytics job to process event hub data

### Scenario
For the first phase of the project, you will start to build the traffic surveillance system to provide insights into average speeds at various locations. In this exercise, you will create a Stream Analytics job that captures speed camera data sent to an event hub from a Visual Studio application (SpeedCameraDevice). You will configure the Stream Analytics job to send output data to a Power BI dashboard and to Azure Data Lake Storage, using filters to remove unnecessary fields before storage.

The main tasks for this exercise are as follows:
1. Create a Data Lake Store
2. Create an event hubs namespace and hub
3. Create a Stream Analytics job
4. Configure Stream Analytics job inputs
5. Configure Stream Analytics job outputs
6. Configure a Stream Analytics job query
7. Start a Stream Analytics job
8. Generate event hub data for processing with Stream Analytics
9. Visualize Stream Analytics output using Power BI
10. View Stream Analytics output in Data Lake Store

#### Task 1: Create a Data Lake Store
1.   Using the Microsoft account that is associated with your Azure Learning Pass subscription, use the Azure portal to create a Data Lake Store, with the following details:
     -   **Name**: adls*&lt;your name&gt;&lt;date&gt;*
     -   **Resource group (Create new)**: CamerasRG
     -   **Location**: Select your nearest location from the currently available Data Lake Store regions
    -    Leave all other settings at their defaults
2.   Wait until the storage has deployed before continuing with the lab.

#### Task 2: Create an event hubs namespace and hub
1.  Create an event hub namespace blade, with the following details:
    -   **Name**: camerafeeds&lt;your name&gt;&lt;date&gt;
    -   **Resource group (Use existing)**: CamerasRG
    -   **Location**: Select the same location as you used for the Data Lake Store
    -   Leave all other settings at their defaults
3.  Wait until the namespace has deployed before continuing with the lab.
4.  Add an event hub to your event hub namespace, with the following details:
    -   **Name**: traffic
    -   **Partition count**: 16
    -   Leave all other settings at their defaults
5.  Wait until the event hub has deployed before continuing with the lab.
6.  Add a consumer group to the traffic event hub, with the following details:
    -   **Name**: cameradatafeed
7.  Add a second consumer group to the traffic event hub , with the following details:
    -   **Name**: cameradatafeed2
8.  Copy the **Primary Key** for your event hubs namespace **RootManageSharedAccessKey** policy, and save it as **Config\_details.txt** in the **E:\\Labfiles\\Lab02** folder.

#### Task 3: Create a Stream Analytics job
1.  Create a Stream Analytics job, with the following details:
    -   **Job name**: TrafficAnalytics
    -   **Resource group (Use existing)**: CamerasRG
    -   **Location**: Select the same location as you used for the Data Lake Store
2.  Wait until the Stream Analytics job has deployed before continuing with the lab.

#### Task 4:Configure Stream Analytics job inputs
1.  Add an input to the TrafficAnalytics Stream Analytics job, with the following details:
    -   **Input alias**: CameraDataFeed
    -   **Source Type**: Data stream
    -   **Source**: Event hub
    -   **Import option**: Provide event hub settings manually
    -   **Service bus namespace**: camerafeeds*&lt;your name&gt;&lt;date&gt;* as you created earlier
    -   **Event hub name**: traffic
    -   **Event hub policy name**: RootManageSharedAccessKey
    -   **Event hub policy key**: Paste the key you copied into Config\_details.txt
    -   **Event hub consumer group**: cameradatafeed
2.  Leave all other settings at their defaults.
3.  Wait until the input has been successfully created before continuing with the lab.
4.  Add a second input to the TrafficAnalytics Stream Analytics job, with the following details:
    -   **Input alias**: CameraDataFeed2
    -   **Source Type**: Data stream
    -   **Source**: Event hub
    -   **Import option**: Provide event hub settings manually
    -   **Service bus namespace**: camerafeeds*&lt;your name&gt;&lt;date&gt;* as you created earlier
    -   **Event hub name**: traffic
    -   **Event hub policy name**: RootManageSharedAccessKey
    -   **Event hub policy key**: Paste the key you copied to Config\_details.txt
    -   **Event hub consumer group**: cameradatafeed2
5.  Leave all other settings at their defaults.
6.  Wait until the input has been successfully created before continuing with the lab.

#### Task 5: Configure Stream Analytics job outputs
1.  Add an output to the TrafficAnalytics Stream Analytics job, with the following details:
    -   **Output alias**: VisualData
    -   **Sink**: Power BI
    -   Click **Authorize**, and sign in using your Power BI credentials
    -   **Dataset Name**: TrafficData
    -   **Table Name**: TrafficData
2.  Wait until the output has been successfully created before continuing with the lab.
3.  Add a second output to the TrafficAnalytics Stream Analytics job, with the following details:
    -   **Output alias**: StoredData
    -   **Sink**: Data Lake Store
    -   Click **Authorize**, and sign in using your Power BI credentials (you might not be prompted this time).
    -   **Path prefix pattern**: SpeedData/{date}/{time}
    -   Leave all other settings at their defaults
4.  Wait until the output has been successfully created before continuing with the lab.

#### Task 6: Configure a Stream Analytics job query
-   Add the following query to the TrafficAnalytics Stream Analytics job:

```
SELECT
CameraID,VehicleRegistration,Speed,SpeedLimit,LocationLatitude,LocationLongitude,Time
INTO
StoredData
FROM
CameraDataFeed

SELECT
CameraID, AVG(Speed) AS AvgSpeed
INTO
VisualData
FROM
CameraDataFeed2
TIMESTAMP BY
Time
GROUP BY
CameraID, TumblingWindow(second, 30)
```
You copy the preceding commands from **E:\\Labfiles\\Lab02\\ASAquery1.txt**.

#### Task 7: Start a Stream Analytics job
1.  Start the TrafficAnalytics Stream Analytics job.
2.  Wait until the job has successfully started before continuing with the lab.

#### Task 8: Generate event hub data for processing with Stream Analytics
1.  Start Visual Studio, and open the **E:\\Labfiles\\Lab02\\SpeedCameraDevice\\SpeedCameraDevice.sln** project.
2.  Edit the **appSettings** section of **App.config** to replace the Endpoint value **YourNamespace** with **camerafeeds*&lt;your name&gt;&lt;date&gt;***.
3.  In App.config, in the **appSettings** section, replace the SharedAccessKey value **YourPrimaryKey** with the primary key you copied to **Config\_details.txt**.
4.  Set **SpeedCameraDriver** as the startup project.
5.  Build the solution, and verify that the app compiles successfully.
6.  Start the app, and verify that the app opens a console window displaying generated speed camera data that is being sent to the event hub.

#### Task 9: Visualize Stream Analytics output using Power BI
1.  Go to **https://powerbi.microsoft.com**, and, if prompted, sign in using your Power BI account credentials.
2.  Verify that a streaming dataset named **TrafficData** is available (if this does not appear, wait a few minutes and then refresh the page).
3.  Create a new dashboard called **Traffic**.
4.  Add a custom streaming data tile, with the following details:
    -   **Source**: TrafficData
    -   **Visualization Type**: Clustered column chart
    -   **Legend**: cameraid
    -   **Value**: avgspeed
    -   **Time window to display**: 30 seconds
    -   **Title**: Camera speeds
5.  Leave the tile to display for a few minutes; note that it summarizes information about the average speeds recorded by each camera during the last 30-second interval.

#### Task 10: View Stream Analytics output in Data Lake Store
1.  Use the Data Explorer for your Data Lake Store to view the subfolders under **SpeedData**.
2.  Open the log file in File Preview; verify that the data includes the fields you specified in your Azure Stream Analytics query.
3.  Stop the TrafficAnalytics Stream Analytics job.
4.  In the Visual Studio app window (where the data is being generated), press Enter to stop the app.

>**Result**: At the end of this exercise, you will have created an Azure Data Lake Store, an event hubs namespace, and a Stream Analytics job. You will then use Stream Analytics to process event hubs data, and view the results in a Power BI dashboard and in Data Lake Store.

## Exercise 2: Create a Stream Analytics job to process IoT hub data

### Scenario
You will now add the locations of police patrol cars to the traffic surveillance system. In this exercise, you will create a second Stream Analytics job that captures patrol car location data from an IoT hub (using a Visual Studio application, PatrolCarDevice, to generate the raw data). You will configure the Stream Analytics job to send data to a Power BI report and to Data Lake Storage.

The main tasks for this exercise are as follows:
1. Create an IoT hub
2. Create a new Stream Analytics job
3. Configure Stream Analytics job inputs
4. Configure Stream Analytics job outputs
5. Configure the Stream Analytics job query
6. Start the Stream Analytics job
7. Generate IoT hub data for processing with Stream Analytics
8. Visualize Stream Analytics output using Power BI
9. View Stream Analytics output in Azure Data Lake Store

#### Task 1: Create an IoT hub
1.  Create an IoT hub, with the following details:
 -   **Name**: patrolcars*&lt;your name&gt;&lt;date&gt;*
 -   **Pricing and scale tier**: F1 Free
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: Select the same location as you used for the Data Lake Store in Exercise 1
2.  Wait until the IoT hub has deployed before continuing with the lab.
3.  Add two consumer groups to the IoT hub, called **patrolcars** and **patrolcars2**.
4.  Copy the Hostname and the Connection string primary key for the iothubowner Shared access policy, to Config\_details.txt.

#### Task 2: Create a new Stream Analytics job
1.  Create a Stream Analytics job, with the following details:
 -   **Job name**: PatrolCarAnalytics
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: Select the same location as you used for the Data Lake Store
2.  Wait until the Stream Analytics job has deployed before continuing with the lab.

#### Task 3: Configure Stream Analytics job inputs
1.	Add an input to the PatrolCarAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: PatrolCarDataFeed
 -   **Source Type**: Data stream
 -   **Source**: IoT hub
 -   **Import option**: Use IoT hub from current subscription
 -   **Consumer group**: patrolcars
 -   Leave all other settings at their defaults
2.	Wait until the input has been successfully created before continuing with the lab.
3.	Add a second input to the PatrolCarAnalytics Stream Analytics job, with the following details:   
 -   **Input alias**: PatrolCarDataFeed2
 -   **Source Type**: Data stream
 -   **Source**: IoT hub
 -   **Import option**: Use IoT hub from current subscription
 -   **Consumer group**: patrolcars2
 -   Leave all other settings at their defaults
4.	Wait until the input has been successfully created before continuing with the lab.

#### Task 4: Configure Stream Analytics job outputs
1.  Add an output to the PatrolCarAnalytics Stream Analytics job, with the following details:
 -   **Output alias**: PatrolCarVisualData
 -   **Sink**: Power BI
 -   Click **Authorize**, and (if prompted) enter your Power BI credentials
 -   **Dataset Name**: PatrolCarData
 -   **Table Name**: PatrolCarData
2.  Wait until the output has been successfully created before continuing with the lab.
3.  Add a second output to the PatrolCarAnalytics Stream Analytics job, with the following details:
 -   **Output alias**: PatrolCarStoredData
 -   **Sink**: Data Lake Store
 -   Click **Authorize**, and (if prompted) enter your Power BI credentials
 -   **Path prefix pattern**: PatrolCarData/{date}/{time}
 -   Leave all other settings at their defaults
4.  Wait until the output has been successfully created before continuing with the lab.

#### Task 5: Configure the Stream Analytics job query
 -   Add the following query to the PatrolCarAnalytics Stream Analytics job:
```
SELECT
CarID,LocationLatitude,LocationLongitude,System.TimeStamp AS Time
INTO
PatrolCarVisualData
FROM
PatrolCarDataFeed
SELECT
\*
INTO
PatrolCarStoredData
FROM
PatrolCarDataFeed2
```
You copy the preceding commands from **E:\\Labfiles\\Lab02\\ASAquery2.txt**.

#### Task 6: Start the Stream Analytics job
1.  Start the PatrolCarAnalytics Stream Analytics job.
2.  Wait until the job has successfully started before continuing with the lab.

#### Task 7: Generate IoT hub data for processing with Stream Analytics
1.  Start Visual Studio, and open the **E:\\Labfiles\\Lab02\\PatrolCarDevice\\PatrolCarDevice.sln** project.
2.  In the **appSettings** section of **App.config**, in the **IoTHubConnectionString** and **IotHubUri** values, replace **YourIoTHub** with **patrolcars*&lt;your name&gt;&lt;date&gt;***.
3.  In the **appSettings** section of **App.config**, in the **IoTHubConnectionString** value, replace the SharedAccessKey value **YourPrimaryKey** with the primary key you copied to **Config\_details.txt**.
4.  Build the solution, and verify that the app compiles successfully.
5.  Start the app, and verify that the app opens a console window displaying the generated positions of patrol cars that are sent to the IoT hub.

#### Task 8: Visualize Stream Analytics output using Power BI
1.  In Power BI, verify that a streaming dataset named **PatrolCarData** is available (if this does not appear, wait a few minutes then refresh the page).
2.  Create a report, with the following details:
 -   **Source**: PatrolCarData
 -   **Fields pane**: Select all the data fields
 -   **Visualizations pane**: Select ArcGIS Maps for Power BI
3.  You might need to resize the map control to make it larger.
4.  The report shows a history of the movements of each patrol car.
5.  Note that, as the report is generated, the data displayed is cumulative; you need to click **Refresh** to update the report.

#### Task 9: View Stream Analytics output in Azure Data Lake Store
1.  Use the Data Explorer for your Data Lake Store to view the subfolders under PatrolCarData.
2.  Open one of the JSON files in File Preview; verify that the data includes the fields you specified in your Stream Analytics query.
3.  Stop the PatrolCarAnalytics Stream Analytics job.
4.  In the Visual Studio app window (where the data is being generated), press Enter to stop the app.

>**Result**: At the end of this exercise, you will have created a Data Lake Store, an IoT hub, and a new Stream Analytics job. You will then use Stream Analytics to process IoT hub data, and view the results in a Power BI report and in Data Lake Store.

## Exercise 3: Reconfigure a Stream Analytics job to send output through a Service Bus queue

### Scenario
After you have added the locations of police patrol cars to the system, it becomes clear that a better, more visual, approach is needed by presenting vehicle locations on a map. In this exercise, you will add an input to the PatrolCarAnalytics Azure Stream Analytics job that captures patrol car locations—and displays the results on a map—by using a simple custom Visual Studio application that listens to a Service Bus queue. This exercise demonstrates how to overcome the shortcomings of trying to show this type of data in a Power BI report, and illustrates how to send data from a Stream Analytics job to another application instead of directly to an output such as Power BI.

The main tasks for this exercise are as follows:
1. Create a Service Bus and queue
2. Reconfigure the IoT hub
3. Reconfigure the PatrolCarAnalytics Stream Analytics job
4. Start the Stream Analytics job
5. Prepare an application to receive Stream Analytics data using a Service Bus
6. Generate IoT hub data for processing with Stream Analytics

#### Task 1: Create a Service Bus and queue
1.  Create a Service Bus, with the following details:
 -   **Name**: locationalerts*&lt;your name&gt;&lt;date&gt;*
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: Select the same location as you used for the Data Lake Store
2.  Wait until the Service Bus has deployed before continuing with the lab.
3.  Copy the **Primary Key** for the **RootManageSharedAccessKey** Shared access policy to **Config\_details.txt**.
4.  Create a queue called **LocationAlerts**.
5.  Wait until the queue has been successfully created before continuing with the lab.

#### Task 2: Reconfigure the IoT hub
-   Add a consumer group to the IoT hub called **patrolcars3**.

#### Task 3: Reconfigure the PatrolCarAnalytics Stream Analytics job
1.  Add an input to the PatrolCarAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: PatrolCarDataFeed3
 -   **Source Type**: Data stream
 -   **Source**: IoT hub
 -   **Import option**: Use IoT hub from current subscription
 -   **Consumer group**: patrolcars3
 -   Leave all other settings at their defaults
2.  Add an output to the PatrolCarAnalytics Stream Analytics job with the following details:
 -   **Output alias**: PatrolCarLocationAlerts
 -   **Sink**: Service bus Queue
 -   **Import option**: Use queue from current subscription
 -   Leave all other settings at their defaults
3.  Wait until the input and output have been successfully created before continuing with the lab.
4.  Add the following to the end of the existing job query:
```
SELECT
CarID,CarNum,LocationLatitude,LocationLongitude,Speed
INTO
PatrolCarLocationAlerts
FROM
PatrolCarDataFeed3
```
You copy the preceding commands from **E:\\Labfiles\\Lab02\\ASAquery3.txt**.

#### Task 4: Start the Stream Analytics job
1.  Start the PatrolCarAnalytics Stream Analytics job.
2.  Wait until the job has successfully started before continuing with the lab.

#### Task 5: Prepare an application to receive Stream Analytics data using a Service Bus
1.  Start a new instance of Visual Studio, and open the **E:\\Labfiles\\Lab02\\LocationAlerts\\LocationAlerts.sln** project; this project displays the movements of patrol cars that it receives from the queue.
2.  Edit **ConfigSettings.txt**, and replace **YourServiceBusName** with **locationalerts*&lt;your name&gt;&lt;date&gt;***.
3.  Edit **ConfigSettings.txt**, and replace **YourPrimaryKey** with the primary key from the service bus connection string you copied to **Config\_details.txt**.
4.  Build the solution, and verify that the app compiles successfully.
5.  Start the app (on the Local Machine), and verify that the app displays a map (of London), but not the positions of any patrol cars yet.

#### Task 6: Generate IoT hub data for processing with Stream Analytics
1.  Switch to your first instance of Visual Studio, restart the **PatrolCarDevice** app, and start generating patrol car movements.
2.  Switch to the map.
3.  Verify that, after a few seconds, the locations of patrol cars start appearing on the map—and that these positions slowly change as patrol cars are driven around.
4.  Allow the system to run for a while then stop the PatrolCarAnalytics job, PatrolCarDevice app, and LocationAlerts app.
5.  Close both instances of Visual Studio.

>**Result**: At the end of this exercise, you will have created an Azure Service Bus, reconfigured an existing IoT hub, and an existing Stream Analytics job. You will then use Stream Analytics to process IoT hub data and to send results to the Service Bus. Finally, you will use a custom Visual Studio application to view the output of the Service Bus.

## Exercise 4: Reconfigure a Stream Analytics job to process both event hub and static file data

### Scenario
The next requirement for the traffic surveillance system is to add a facility for checking the vehicles recorded by speed cameras against a list of stolen vehicles. In this exercise, you will edit the Stream Analytics job from Exercise 1 (TrafficAnalytics) to detect whether a vehicle observed in a speed camera is stolen. You will create an Azure Storage block blob and upload a file containing static vehicle theft records, and use this file as input reference data for the Stream Analytics job (in addition to the speed camera data). To achieve this, you will update the event hub that you used in Exercise 1, and add two more consumer groups (this is a best practice, because you will use the event hub to provide two additional inputs to Stream Analytics). You will configure Stream Analytics to send the results to a Power BI dashboard, and to a JSON format file in Data Lake Store, organized by date and time.

The main tasks for this exercise are as follows:
1. Create a Blob storage account for holding stolen vehicle data
2. Examine the StolenVehiclesReport.csv file
3. Use Azure Storage Explorer to upload a StolenVehiclesReport.csv to the Blob storage container
4. Update the event hub and add two more consumer groups
5. Reconfigure the TrafficAnalytics Stream Analytics job inputs
6. Reconfigure the TrafficAnalytics Azure Stream Analytics job outputs
7. Reconfigure the TrafficAnalytics Stream Analytics job query
8. Start the TrafficAnalytics Stream Analytics job
9. Generate event hub data for processing with Stream Analytics
10. Visualize Stream Analytics output using Power BI
11. View Stream Analytics output in Data Lake Store

#### Task 1: Create a Blob storage account for holding stolen vehicle data
1.  Create a storage account, with the following details:
 -   **Name**: datastore*&lt;your name&gt;&lt;date&gt;*
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: Select the same location as you used for the Data Lake Store
 -   Leave all other details at their defaults
2.  Wait until the storage account has been successfully created before continuing with the lab.
3.  Add a container to the storage account called **stolenvehicledata**.

#### Task 2: Examine the StolenVehiclesReport.csv file
1.  Open **E:\\Labfiles\\Lab02\\StolenVehiclesReport.csv**.
2.  Verify that this file contains the registration number, date stolen, and date recovered for stolen vehicles; if a vehicle is still missing, the date recovered is empty.
3.  Close the file.

#### Task 3: Use Azure Storage Explorer to upload a StolenVehiclesReport.csv to the Blob storage container
1.  Open Microsoft Azure Storage Explorer, and sign in using the credentials for the Microsoft account that is associated with your Azure Learning Pass subscription.
2.  Upload **E:\\Labfiles\\Lab02\\StolenVehiclesReport.csv** to the **stolenvehicledata** container.
3.  When the upload is complete, close Storage Explorer.

#### Task 4: Update the event hub and add two more consumer groups
1.  Add the following consumer groups to the event hub:
 -  stolenvehiclefeed1
 -  stolenvehiclefeed2

#### Task 5: Reconfigure the TrafficAnalytics Stream Analytics job inputs
1.  Add an input to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: StolenVehicleData
 -   **Source Type**: Reference data
 -   **Import option**: Use Blob storage from current subscription
 -   **Path pattern**: StolenVehiclesReport.csv
 -   **Event serialization format**: CSV
 -   Leave all other settings at their defaults
2.  Add another input to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: StolenVehicleFeed1
 -   **Source Type**: Data stream
 -   **Source**: Event hub
 -   **Import option**: Use event hub from current subscription
 -   **Event hub consumer group**: stolenvehiclefeed1
 -   Leave all other settings at their defaults
3.  Add another input to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: StolenVehicleFeed2
 -   **Source Type**: Data stream
 -   **Source**: Event hub
 -   **Import option**: Use event hub from current subscription
 -   **Event hub consumer group**: stolenvehiclefeed2
 -   Leave all other settings at their defaults
4.  Wait until the inputs have been successfully created before continuing with the lab.

#### Task 6: Reconfigure the TrafficAnalytics Azure Stream Analytics job outputs
1.  Add an output to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Output alias**: StolenCarAlerts
 -   **Sink**: Power BI
 -   Click **Authorize**, and (if prompted) enter your Power BI credentials
 -   **Dataset name**: StolenCarAlerts
 -   **Table name**: StolenCarAlerts
2.  Add another output to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Output alias**: StolenCarObservations
 -   **Sink**: Data Lake Store
 -   Click **Authorize**, and (if prompted) enter your Azure credentials
 -   **Path prefix pattern**: StolenCarObservations/{date}/{time}
 -   Leave all other settings at their defaults
3.  Wait until the outputs have been successfully created before continuing with the lab.
#### Task 7: Reconfigure the TrafficAnalytics Stream Analytics job query
 -   Add the following to the end of the existing TrafficAnalytics Stream Analytics job query:
```
SELECT
C.VehicleRegistration,C.LocationLatitude,C.LocationLongitude,C.Time
INTO
StolenCarAlerts
FROM
StolenVehicleFeed1 C
JOIN
StolenVehicleData V
ON
C.VehicleRegistration = V.Vehicle
WHERE
V.Recovered = ""
SELECT
C.VehicleRegistration,C.LocationLatitude,C.LocationLongitude,C.Time
INTO
StolenCarObservations
FROM
StolenVehicleFeed2 C
JOIN
StolenVehicleData V
ON
C.VehicleRegistration = V.Vehicle
WHERE
V.Recovered = ""
```
You copy the preceding commands from **E:\\Labfiles\\Lab02\\ASAquery4.txt**.

#### Task 8: Start the TrafficAnalytics Stream Analytics job
1.  Increase the processing capacity of the job to 12 Streaming Units.
2.  Start the TrafficAnalytics Stream Analytics job.
3.  Wait until the job has successfully started before continuing with the lab.

#### Task 9: Generate event hub data for processing with Stream Analytics
1.  Start Visual Studio, and open the **E:\\Labfiles\\Lab02\\SpeedCameraDevice\\SpeedCameraDevice.sln** project.
2.  Start the app, and verify that it opens a console window displaying generated speed camera data that is being sent to the event hub.

#### Task 10: Visualize Stream Analytics output using Power BI
1.  In Power BI, verify that a streaming dataset named **StolenCarAlerts** is available (if this does not appear, wait a few minutes, and then refresh the page).
2.  Create a new dashboard called **Stolen Vehicle Alerts**.
3.  Add a custom streaming data tile, with the following details:
 -   **Source**: StolenCarAlerts
 -   **Visualization Type**: Clustered column chart
 -   **Axis**: vehicleregistration
 -   **Legend**: time
 -   **Value**: locationlatitude
 -   **Tooltips**: locationlongitude
 -   **Time window to display**: 1 seconds
 -   **Title**: Vehicles reported stolen and detected by cameras
4.  Leave the tile to display for a few minutes; note that it is updated every 1 seconds and displays the registration number of a vehicle, together with the date, time, and location, if it is marked as stolen.
5.  Hover the mouse over the chart, and verify that it shows vehicle registration, date/time, and location.

#### Task 11: View Stream Analytics output in Data Lake Store
1.  Use the Data Explorer for your Data Lake Store to view the subfolders under **StolenCarObservations**.
2.  Open the log file in File Preview; verify that the data includes the fields you specified in your Azure Stream Analytics query.
3.  Stop the TrafficAnalytics Stream Analytics job.
4.  In the Visual Studio app window (where the data is being generated), press Enter to stop the app.

>**Result**: At the end of this exercise, you will have uploaded data to a new Blob storage container, updated your event hub with new consumer groups, and reconfigured your TrafficAnalytics Azure Stream Analytics job to use these new inputs. You will then use Stream Analytics to process the event hubs data, and view the results in a Power BI dashboard, and in Data Lake Store.

## Exercise 5: Use multiple Stream Analytics jobs to process event hub, IoT hub and static file data, and output results using a Service Bus and custom application

### Scenario
For the final part of this initial phase in the development of the traffic surveillance system, you have been asked to add the ability to determine the nearest patrol car to a speeding vehicle or stolen vehicle, send a dispatch alert to the nearest patrol car, and then show the dispatched patrol car locations on a map. Specifically, the system must be able to identify the nearest patrol car to a speeding or stolen vehicle, and then send a message (using Service Bus) to that patrol car. The message would contain details about the vehicle’s registration number, location, and speed. Any patrol car situated within five kilometers of the stolen or speeding vehicle’s most recently reported location could then be dispatched to that location. The message should contain the ID of the patrol car, the registration number of the stolen vehicle, and the coordinates of the location where the vehicle was observed. In this exercise, you will create a new Service Bus topic, and add a subscription to the topic. You will use this topic to send alert messages to patrol cars about stolen vehicles. Patrol car devices will subscribe to the subscription in this topic.

The main tasks for this exercise are as follows:
1. Create a new Service Bus topic and add a subscription
2. Reconfigure the IoT hub and add a new consumer group
3. Reconfigure the event hub and add a new consumer group
4. Reconfigure the TrafficAnalytics Azure Stream Analytics job inputs
5. Reconfigure the TrafficAnalytics Stream Analytics job outputs
6. Reconfigure the TrafficAnalytics Stream Analytics job query
7. Start the TrafficAnalytics Azure and PatrolCarAnalytics Stream Analytics jobs
8. Generate event hub and IoT hub data for processing with Stream Analytics
9. Start an application to receive Stream Analytics data using a Service Bus

#### Task 1: Create a new Service Bus topic and add a subscription
1.  Add a topic called **stolencaralerts** to the Service Bus.
2.  Add a subscription called **stolen** to the new topic.

#### Task 2: Reconfigure the IoT hub and add a new consumer group
 -   Add a consumer group to the IoT hub called **patrolcars4**.

#### Task 3: Reconfigure the event hub and add a new consumer group
 -   Add a consumer group called **stolenvehiclefeed3** to the event hub.

#### Task 4: Reconfigure the TrafficAnalytics Azure Stream Analytics job inputs
1.  Add an input to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: PatrolCarLocation
 -   **Source Type**: Data stream
 -   **Source**: IoT hub
 -   **Import option**: Use IoT hub from current subscription
 -   **Consumer group**: patrolcars4
 -   Leave all other settings at their defaults
2.  Add another input to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Input alias**: StolenVehicleFeed3
 -   **Source Type**: Data stream
 -   **Source**: Event hub
 -   **Import option**: Use event hub from current subscription
 -   **Event hub consumer group**: stolenvehiclefeed3
 -   Leave all other settings at their defaults
3.  Wait until the inputs have been successfully created before continuing with the lab.

#### Task 5: Reconfigure the TrafficAnalytics Stream Analytics job outputs
1.  Add an output to the TrafficAnalytics Stream Analytics job, with the following details:
 -   **Output alias**: StolenVehicleAlerts
 -   **Sink**: Service bus Topic
 -   Leave all other settings at their defaults
2.  Wait until the output has been successfully created before continuing with the lab.

#### Task 6: Reconfigure the TrafficAnalytics Stream Analytics job query
 -   Add the following to the end of the existing TrafficAnalytics job query:
```
SELECT
P.CarID,V.Vehicle,C.LocationLatitude,C.LocationLongitude
INTO
StolenVehicleAlerts
FROM
StolenVehicleFeed3 C
JOIN
StolenVehicleData V
ON
C.VehicleRegistration = V.Vehicle
JOIN
PatrolCarLocation P
ON
ST\_DISTANCE(CreatePoint(C.LocationLatitude,C.LocationLongitude),CreatePoint(P.LocationLatitude,P.LocationLongitude)) \< 8000
AND
DATEDIFF(second,P,C) BETWEEN 0 AND 10
WHERE
V.Recovered = ""
```
You copy the preceding commands from **E:\\Labfiles\\Lab02\\ASAquery5.txt**.

#### Task 7: Start the TrafficAnalytics Azure and PatrolCarAnalytics Stream Analytics jobs
1.  Start the **TrafficAnalytics** and **PatrolCarAnalytics** jobs.
2.  Wait until the jobs have successfully started before continuing with the lab.

#### Task 8: Generate event hub and IoT hub data for processing with Stream Analytics
1.  Switch to the Visual Studio instance that has the **SpeedCameraDevice** project open, and start the app.
2.  Verify that the app opens a console window displaying generated speed camera data that is being sent to the event hub.
3.  Start a new instance of Visual Studio, and open **E:\\Labfiles\\Lab02\\PatrolCarDevice2\\PatrolCarDevice.sln**; note that this is a modified version of the app you worked with in Exercise 2 and 3.
4.  Edit **App.config**, in the **appSettings** section, in the **ServiceBusConnectionString** key, replace **YourServiceBusName** with **locationalerts*&lt;your name&gt;&lt;date&gt;***, and replace **YourServiceBusPrimaryKey** with the primary key you copied to **Config\_details.txt**.
5.  Edit **App.config**, in the **appSettings** section, in the **IotHubConnectionString** and **IotHubUri** keys, to replace **YourNamespace** with **patrolcars*&lt;your name&gt;&lt;date&gt;***.
6.  Edit **App.config**, in the **appSettings** section, in the **IotHubConnectionString** key, to replace the SharedAccessKey value **YourIoTHubPrimaryKey** with the SharedAccessKey from the IoT hub connection string you copied to **Config\_details.txt**.
7.  Build the solution, and verify that the app compiles successfully.
8.  Start the app, and verify that the app opens a console window displaying the generated positions of patrol cars that are being sent to the IoT hub.

#### Task 9: Start an application to receive Stream Analytics data using a Service Bus
1.  Start a new instance of Visual Studio, and open **E:\\Labfiles\\Lab02\\LocationAlerts\\LocationAlerts.sln**.
2.  Start the app (on the Local Machine).
3.  Verify that the app displays a map (of London), and starts to show the positions of patrol cars.
4.  Let the system run for a few minutes, to give it time to detect some stolen vehicles, and then observe the fun!
5.  Note that the **PatrolCarDevice** project reports messages when a patrol car is dispatched to chase a stolen vehicle. You should also see the location of the patrol car change to move to the location reported for the stolen vehicle in the **LocationAlerts** app. Arrange your desktop so that you see the Patrol Car output and map side by side. If you do not see all the patrol cars, zoom out of the map.
6.  At the end of the exercise, keep the apps and the Stream Analytics jobs running, ready for the final exercise in this lab.
7.  Close Notepad, and save any changes.

>**Result**: At the end of this exercise, you will have:
 -   Created a new Service Bus topic, and added a subscription to this topic.
 -   Reconfigured the IoT and event hubs, and added a new consumer group to each hub.
 -   Reconfigured the TrafficAnalytics Azure Stream Analytics job to use these new inputs, and to use the new Service Bus topic as a job output.
 -   Updated the job query to send data to the Service Bus topic, by using a Visual Studio application.

## Exercise 6: Use the Azure portal and Azure PowerShell to manage and scale Stream Analytics jobs

### Scenario
You have been asked how the traffic surveillance system could cope with any large-scale incident or event that requires additional police resources being brought on stream. You have also been asked how the system might be monitored and managed, and to demonstrate any potential for automation. In this exercise, you will monitor a Stream Analytics job, and create an alert when the job uses more than a threshold number of streaming units. You will then use the Azure portal to scale up the job, and review the streaming unit utilization. You will use the Azure PowerShell cmdlets for Stream Analytics to stop the Stream Analytics job, to scale the job back down, and then to restart the job. Finally, you will use job diagrams to visualize the configurations of your two Stream Analytics jobs.

The main tasks for this exercise are as follows:
1. Add a monitoring alert to a Stream Analytics job
2. Use the Azure portal to scale up a Stream Analytics job
3. Use Azure PowerShell to stop a Stream Analytics job
4. Use Azure PowerShell to scale down and restart a Stream Analytics job
5. Use job diagrams to visualize Stream Analytics job configurations
6. Lab closedown

#### Task 1: Add a monitoring alert to a Stream Analytics job
1.  Open the PatrolCarAnalytics Stream Analytics job, using the Azure portal, and view the Monitoring graph.
2.  Note that, for this job, the number of input and output events are typically the same.
3.  Click the graph, and add an alert called **Streaming unit utilization**, using the SU % Utilization metric.
4.  Note the current range of utilization.
5.  Set the alert condition list to be greater than n, where "n" is less than the maximum utilization you noted earlier; for example, if you noted a maximum of 60%, set the threshold to 50%.
6.  Leave all the other settings at their defaults.
7.  Within five minutes, your alert should be triggered.

#### Task 2: Use the Azure portal to scale up a Stream Analytics job
1.  Stop the PatrolCarAnalytics Stream Analytics job.
2.  Wait until the job has successfully stopped before continuing with the lab.
3.  Scale up the PatrolCarAnalytics Stream Analytics job to use 18 streaming units.
4.  Start the PatrolCarAnalytics Stream Analytics job.
5.  Wait until the job has successfully started before continuing with the lab.
6.  View the Monitoring graph, and then view the Streaming unit utilization alert.
7.  Note that the SU % Utilization is lower than before, because more streaming units have been deployed.

#### Task 3: Use Azure PowerShell to stop a Stream Analytics job
1.  Note that all the following PowerShell commands can be copied from **E:\\Labfiles\\Lab02\\ASAPowerShell.txt**.
2.  Start PowerShell ISE as Administrator, and in the script area, type the following commands, and then click **Run**:
```
Login-AzureRmAccount
Get-AzureRMStreamAnalyticsJob -NoExpand
```
3.  When prompted, enter the details of the Microsoft account that is associated with your Azure Learning Pass subscription.
4.  The results display information about the two Stream Analytics jobs.
5.  In the script area, type the following command, highlight it, and then click **Run Selection**:
```
Stop-AzureRMStreamAnalyticsJob -ResourceGroupName CamerasRG –Name PatrolCarAnalytics
```
6.  In the script area, type the following command, highlight it, and then click **Run Selection**:
```
(Get-AzureRmStreamAnalyticsJob -ResourceGroupName CamerasRG -Name PatrolCarAnalytics).JobState
```
7.  Verify that the job has stopped.
8.  Save the script as **E:\\Labfiles\\Lab02\\ShutdownASAjob.ps1**.

#### Task 4: Use Azure PowerShell to scale down and restart a Stream Analytics job
1.  Use File Explorer to go to **E:\\Labfiles\\Lab02**, and then open the **patrolcaranalytics.json** file in Visual Studio.
2.  This file contains the JSON format code for the Stream Analytics job query, and the scaling information; note the line near the end of the file that specifies the number of StreamingUnits to use (1).
3.  Close patrolcaranalytics.json.
4.  In the PowerShell ISE, create a new file.
5.  In the script area, type the following commands:
```
New-AzureRmStreamAnalyticsTransformation -File E:\\Labfiles\\Lab02\\patrolcaranalytics.json -JobName PatrolCarAnalytics -ResourceGroupName CamerasRG -Name Transformation -Force
Start-AzureRMStreamAnalyticsJob -ResourceGroupName CamerasRG –Name PatrolCarAnalytics
(Get-AzureRmStreamAnalyticsJob -ResourceGroupName CamerasRG -Name PatrolCarAnalytics).JobState
```
6.  Save the script as **E:\\Labfiles\\Lab02\\ScaleUpAndStartASAjob.ps1**.
7.  Run the script—this might take several minutes to complete.
8.  In the Azure portal, open the **PatrolCarAnalytics Stream Analytics job** blade, and verify that the number of streaming units has been reset to 1.

#### Task 5: Use job diagrams to visualize Stream Analytics job configurations
1.  In the Azure portal, open the job diagram for the **PatrolCarAnalytics Stream Analytics job** blade.
2.  Note the inputs from the IoT hub, the queries, and the outputs to the Service Bus queue, Data Lake Store, and Power BI.
3.  Click any of the boxes to get more information.
4.  Close the job diagram.
5.  Repeat the preceding steps for the TrafficAnalytics Stream Analytics job; note that, for this job, the diagram is more complex because there are more inputs, outputs, and queries, including data merging.
6.  Close the job diagram.

#### Task 6:  Lab closedown
1.  Stop the TrafficAnalytics and PatrolCarAnalytics jobs.
2.  Close the LocationAlerts, PatrolCarDevice, and SpeedCameraDevice apps.
3.  Close the PowerShell ISE.
4.  Do not remove the Azure resources (resource group, Stream Analytics jobs, event hub, IoT hub, and storage); these resources will be used in Lab 3.

>**Result**: At the end of this exercise, you will have:
 -   Added a monitoring alert to a Stream Analytics job.
 -   Used the Azure portal to scale up a Stream Analytics job.
 -   Used Azure PowerShell to stop a Stream Analytics job.
 -   Used Azure PowerShell to scale down and restart a Stream Analytics job.
 -   Used job diagrams to visualize Stream Analytics job configurations.

**Question**: What data types would you process using Stream Analytics within your organization?

**Question**: How might you use multiple stream analytics jobs within your organization?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
