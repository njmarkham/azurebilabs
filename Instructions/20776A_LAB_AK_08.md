# Module 8: Performing Analytics with Azure SQL Data Warehouse

# Lab: Performing analytics with SQL Data Warehouse

## Exercise 1: Visualize data stored in SQL Data Warehouse

>**Note:** In this exercise, you will upload the speed data from a CSV file rather than streaming it from the cameras; the CSV file contains data for a longer period of time than is feasible by using streaming.

>**Note:** If you have completed Lab 4, you do not need to complete Exercise 1, Task 1; however, you will still need to check that all the virtual machines are running (as instructed in Step 1).

#### Task 1: Install AzCopy
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  On the Start menu, type **Internet Explorer**, and then press Enter.
3.  In Internet Explorer, go to **https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy**.
4.  On the **Transfer data with the AzCopy on Windows** page, click the **Download and install AzCopy on Windows** link, and then click the latest version of **AzCopy on Windows** link.
5.  In the Internet Explorer message box, click **Run**.
6.  In the **Microsoft Azure Storage Tools - v6.2.0 Setup** dialog box, on the **Welcome to the Microsoft Azure Storage Tools - v6.2.0 Setup Wizard** page, click **Next**.
7.  On the **End-User License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Next**.
8.  On the **Destination Folder** page, click **Next**.
9.  On the **Ready to install Microsoft Azure Storage Tools - v6.2.0** page, click **Install**.
10. In the **User Account Control** dialog box, click **Yes**.
11. On the **Completed the Microsoft Azure Storage Tools - v6.2.0 Setup Wizard** page, click **Finish**.
12. Right-click the Start button, click **System**, and then click **Advanced system settings**.
13. In the **System Properties** dialog box, click **Environment Variables**.
14. In the **Environment Variables** dialog box, under **User variable for AdatumAdmin**, click **Path**, and then click **Edit**.
15. In the **Edit User Variable** dialog box, in the **Variable value** box, at the end of the existing text, type **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy;**, and then click **OK**.
16. In the **Environment Variables** dialog box, click **OK**.
17. In the **System Properties** dialog box, click **OK**.
18. Close the System window.

#### Task 2: Prepare the environment
1.  In the Azure portal, click **All resources**, and then click **trafficwarehouse**.
2.  On the **trafficwarehouse** blade, click **Start**, and then click **Yes**.
3.  Wait until the database has started before continuing with the exercise.
4.  On the Start menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
5.  In the **Connect to Server** dialog box, in the **Server name** box, type **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**.
6.  In the **Authentication** list, click **SQL Server Authentication**.
7.  In the **Login** box, type **student**.
8.  In the **Password** box, type **Pa55w.rd**, and then click **Connect**.
9.  In Object Explorer, expand **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**, expand **Databases**, and then click **trafficwarehouse**.
10. On the **File** menu, point to **Open**, and then click **File**.
11. In the **Open File** dialog box, go to **E:\\Labfiles\\Lab08\\Exercise1**, click **Exercise1.sql**, and then click **Open**.
12. In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	DELETE FROM dbo.VehicleSpeed
	GO
```
13.  Switch to the Azure portal.
14.  Click **+ New**, click **Storage**, and then click **Storage account - blob, file, table, queue**.
15.  On the **Create storage account** blade, in the **Name** box, type **speeddata*&lt;your name&gt;&lt;date&gt;***.
16.  Under **Resource group**, click **Use existing**, and then click **CamerasRG**.
17.  In the **Location** list, select the same location as you have used previously for your Data Lake Store.
18.  Leave all other details at their defaults, and click **Create**.
19.  Wait until the storage account has been successfully created before continuing with the exercise.
20.  Click **All resources**, and then click **speeddata*&lt;your name&gt;&lt;date&gt;***.
21.  On the **speeddata*&lt;your name&gt;&lt;date&gt;*** blade, under **BLOB SERVICE**, click **Containers**, and then click **+ Container**.
22. In the **New container** dialog box, in the **Name** box, type **capturedspeeds**, and then click **OK**.

#### Task 3: Upload data to Blob storage
1.  On the **speeddata*&lt;your name&gt;&lt;date&gt;*** **- Containers** blade, under **SETTINGS**, click **Access keys**.
2.  Next to **key1**, click the **Click to copy** button, to copy the key to the clipboard.
3.  Right-click the Start button, and then click **Command Prompt (Admin)**.
4.  In the **User Account Control** dialog box, click **Yes**.
5.  At the command prompt, type the following command (replacing **&lt;storage account name&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;storage account key&gt;** with the key you copied to the clipboard), and then press Enter:
	```
	azcopy /Source:"E:\Labfiles\Lab08\Exercise1" /Pattern:"SpeedData.csv" /Dest:https://<storage account name>.blob.core.windows.net/capturedspeeds /DestKey:<storage account key>
	```
The preceding command can be copied from **E:\\Labfiles\\Lab08\\Exercise1\\AZCopyCmd.txt**.

#### Task 4: Use PolyBase to transfer blob data to SQL Data Warehouse
1.  Switch to SQL Server Management Studio.
2.  In the SQL Editor, locate the following command, and replace **&lt;storage account name&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***, and replace **&lt;storage account key&gt;** with the key you copied to the clipboard:
	```
	CREATE DATABASE SCOPED CREDENTIAL SpeedDataCredentials
	WITH IDENTITY = '<storage account name>',
	SECRET = '<storage account key>';
	GO
	```
3.  Highlight the edited command, on the toolbar, and click **Execute**; this command will create a database scoped credential for accessing the **speeddata*&lt;your name&gt;&lt;date&gt;*** storage account.
4.  In the SQL Editor, locate the following command, and replace **&lt;storage account name&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***:
	```
	CREATE EXTERNAL DATA SOURCE SpeedDataSource
	WITH (
	TYPE = HADOOP,
	LOCATION = 'wasbs://capturedspeeds@<storage account name>.blob.core.windows.net',
	CREDENTIAL = SpeedDataCredentials
	)
	GO
	```
5.  Highlight the edited command, and on the toolbar, click **Execute**; this command will create an external data source named **SpeedDataSource** that connects to the **capturedspeeds** container in the **speeddata*&lt;your name&gt;&lt;date&gt; ***Blob storage account using this credential.
6.  In the SQL Editor, highlight the following command:
	```
	CREATE EXTERNAL TABLE ExternalSpeedData (
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate VARCHAR(20) NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH (
	LOCATION='SpeedData.csv',
	DATA_SOURCE = SpeedDataSource,
	FILE_FORMAT = CommaSeparatedFileFormat,
	REJECT_TYPE = percentage,
	REJECT_VALUE = 2,
	REJECT_SAMPLE_VALUE = 1000
	)
	GO
	```
7.  On the toolbar, click **Execute**; this command will create an external table named **ExternalSpeedData** that uses the data source and the **CommaSeparatedFileFormat** (from Lab 7) to read the location data from Blob storage. Note that the **REJECT\_VALUE** and **REJECT\_SAMPLE\_VALUE** settings allow for some minor corruption or malformed records in the CSV file.
8.  In the SQL Editor, highlight the following command:
	```
	INSERT INTO dbo.VehicleSpeed(CameraID, SpeedLimit, Speed, VehicleRegistration, WhenDate, WhenMonth)
	SELECT CameraID, SpeedLimit, Speed, VehicleRegistration, CONVERT(DATETIME, WhenDate, 103) AS WhenDate, WhenMonth
	FROM ExternalSpeedData
	GO
	```
9.  On the toolbar, click **Execute**; this command will copy the data from the **ExternalSpeedData** table to the **dbo.VehicleSpeed** table. Note that the value in the **WhenDate** column is a string, which is converted into a **DateTime** by specifying the appropriate format style. When you execute this command, you might get one or two messages about rejected rows; you can ignore these errors.
10.  In the SQL Editor, close **Exercise1.sql**, without saving any changes.

#### Task 5: Use Power BI to visualize the data
1.  Switch to the Azure portal.
2.  Click **All resources**, and then click **trafficwarehouse**.
3.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Open in PowerBI**.
4.  In Power BI, if prompted, sign in using the Power BI account credentials that you've used in previous labs.
5.  In the **Connect to Azure SQL Data Warehouse** dialog box, accept the default **Server** and **Database** name, and then click **Next**.
6.  In the **Connect to Azure SQL Data Warehouse** dialog box, in the **Password** box, type **Pa55w.rd**, and then click **Sign in**.
7.  In Power BI, click **DataSets**.
8.  In the **ACTIONS** column for the **trafficwarehouse** source, click **Create report**.
9.  In the **VISUALIZATIONS** pane, click **Line chart**.
10. In the **FIELDS** pane, expand **VehicleSpeed**, drag **WhenDate** to **Axis**, drag **CameraID** to **Legend**, and then drag **Speed** to **Values**.
11. In the **VISUALIZATIONS** pane, under **FILTERS**, click **CameraID(All)**.
12. In the **Filter Type** list, click **Basic filtering**, and then select the **Camera 0**, **Camera 1**, **Camera 10**, and **Camera 100** check boxes; note that, if you do not use the **Filters** pane at this point to select just one or two speed cameras, you might exceed the maximum number of data points allowed on a Power BI graph.
13. Click **Reading view**, and then click **Focus mode**.
14. On the **File** menu, click **Save as**.
15. In the **Save your report** dialog box, type **Vehicle Speeds over Time by CameraID**, and then click **Save**.
16. In the **FILTERS** pane, select a single camera.
17. Examine the graph, and notice how speeds vary throughout the day.
18. In the **FILTERS** pane, select another camera, and note the pattern; it should be similar to the first camera.
19. Note that, if the system was using a Stream Analytics job to send data as it was captured to the SQL Data Warehouse, you could click **Refresh** periodically to see the most recent data. As it stands, the data warehouse just contains an historical snapshot of speed camera data.
20. Close the Power BI tab.

>**Result**: At the end of this exercise, you will have uploaded data to Blob storage, and then used PolyBase to transfer this blob data to SQL Data Warehouse. You will then use Power BI to visualize this data, and look for patterns in the data.

## Exercise 2: Use Machine Learning with SQL Data Warehouse

#### Task 1: Create experiment
1.  In the Azure portal, click **All Resources**, and then click **Traffic**.
2.  On the **Traffic** blade, under **Additional Links**, click **Launch Machine Learning Studio**.
3.  On the **Microsoft Azure Machine Learning Studio** page, click **Sign in here**.
4.  On the **My Experiments** page, click **+ NEW**, and then click **Blank Experiment**.
5.  In the left pane, expand **Data Input and Output**, and then drag **Import Data** to the workspace canvas.
6.  Click **Import Data**.
7.  In the **Properties** pane, set the following values:
 -   **Data source**: Azure SQL Database
 -   **Database server name**: trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net
 -   **Database name**: trafficwarehouse
 -   **User name**: student
 -   **Password**: Pa55w.rd
 -   **Database query**:
	```
	SELECT CameraID, Speed, DATEPART(hour, WhenDate) AS Hour, DATEPART(weekday, WhenDate) AS Day FROM dbo.VehicleSpeed
	```
The query returns the Hour (0-23) and Day (1-7) rather than the date and time down to the nearest fraction of a millisecond, which would be much too low a level of granularity for generating a predictive model. We are more interested in knowing whether traffic will be slow between 5 and 6 PM on a Friday, rather than trying to guess what the speed will be at 17:37:42.6586 on a specific date.
8.  In the left pane, expand **Data Transformation**, expand **Sample and Split**, and then drag **Split Data** to the workspace canvas, below **Import Data**.
9.  Connect the output of the **Import Data** module to the input of the **Split Data** module.
10.  Click **Split Data**.
11.  In the **Properties** pane, set **Fraction of rows in the first output dataset** to **0.9**.
12.  In the left pane, expand **Machine Learning**, expand **Initialize Model**, expand **Regression**, and then drag **Decision Forest Regression** to the workspace canvas, to the left of the **Split Data** module.
13.  In the left pane, under **Machine Learning**, expand **Train**, and drag **Train Model** to the workspace canvas, below the **Split Data** and **Decision Network Regression** modules.
14.  Connect the output from the **Decision Forest Regression** module to the left input of the **Train Model** module.
15.  Connect the left output of the **Split Data** module to the right input of the **Train Model** module.
16.  Click **Train Model**.
17. In the **Properties** pane, click **Launch column selector**.
18. In the **Select a single** **column** dialog box, in the second list box, click **column names**, and in the third box, type **Speed**.
19. Click the check mark (tick) to close the **column selector** dialog box.
20. In the left pane, under **Machine Learning**, expand **Score**, and drag **Score Model** to the workspace canvas, below and slightly to the right of the **Train Model** module.
21. Connect the output from the **Train Model** module to the left input of the **Score Model** module.
22. Connect the right output of the **Split Data** module to the right input of the **Score Model** module.
23. In the toolbar, click **SAVE**, and then click **RUN**.
24. When the experiment has finished running, all modules will show a green check mark to indicate that they have successfully finished; note that the experiment might take 10-15 minutes to complete.
25. When the experiment has completed, right-click the output of the **Score Model** module, and then click **Visualize**.
26. Note that the **Scored Labels Mean** column contains the predicted speeds made by the model, and the **Speed** column contains the actual speeds.
27. Close the visualization window.

#### Task 2: Create trained model
1.  Right-click the **Train Model** module, point to **Trained model**, and then click **Save as Trained Model**.
2.  In the **Save trained model** dialog box, in the **Name** box, type **Decision Forest Trained Regression Model for Vehicle Speeds**, and click **Ok** (tick).
3.  Right-click the **Decision Forest Regression** module, and then click **Delete**.
4.  Right-click the **Train Model** module, and then click **Delete**.
5.  In the left pane, expand **Trained Models**, and then drag **Decision Forest Trained Regression Model for Vehicle Speeds** to the workspace canvas, to the left of the **Split Data** module.
6.  Connect the output of the **Decision Forest Trained Regression Model for Vehicle Speeds** module to the left input of the **Score Model** module.
7.  In the toolbar, click **SET UP WEB SERVICE**.
8.  If the **STEP 1** dialog box appears, close it.
9.  Right-click the connection from the **Web service input** module to the **Split Data** module, and then click **Delete**.
10. Connect the output of the **Web service input** module to the right input of the **Score Model** module.
11. In the toolbar, click **SAVE**, and then click **RUN** (to validate the changes).
12. When the experiment has finished running, all modules will show a green check mark to indicate that they have successfully finished.

#### Task 3: Deploy the Machine Learning web service
1.  In the toolbar, click **DEPLOY WEB SERVICE**, and then click **Deploy Web Service [Classic]**.
2.  Next to the **API key**, click the **Copy** button.
3.  Switch to Notepad, and then open the **Config\_details.txt** file.
4.  In Notepad, click at the end of the file, press Enter to create a new line, type **Web service API key**, press Enter to create a new line, and then paste the API key.
5.  Switch to Microsoft Azure Machine Learning Studio.
6.  On the **experiment created on *&lt;date&gt;*** page, click **New Web Services Experience**.
7.  On the **default** page, under **BASICS**, click **Configure endpoint**.
8.  In the **Description** box, type **Vehicle speed prediction web service**, and then click **Save**.
9.  In the toolbar, click **Quickstart**, and then click **Use endpoint**.
10. Next to **Request-Response**, click the **Copy** button.
11. Switch to Notepad, and the open **Config\_details.txt** file.
12. In Notepad, click at the end of the file, press Enter to create a new line, type **Request-Response URL**, press Enter to create a new line, and then paste the Request-Response URL.

#### Task 4: Generate predictions by using the Machine Learning web service in an application
1.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
2.  In Visual Studio, click **Open Project / Solution**.
3.  In the **Open Project** dialog box, go to the **E:\\Labfiles\\Lab08\\Exercise2\\VehicleSpeedPredictor** folder, click **VehicleSpeedPredictor.sln**, and then click **Open**.
4.  In Solution Explorer, double-click **App.config**.
5.  In App.config, in the **appSettings** section, replace the APIKey value **&lt;YourApiKey&gt;** with the API key of the web service that you copied to **Config\_details.txt**.
6.  In App.config, in the **appSettings** section, replace the URL value **&lt;YourEndpointUrl&gt;** with the **Request-Response URL** that you copied to Config\_details.txt.
    **IMPORTANT**: Replace the **&** character near the end of the Request-Response URL (before the text **format=swagger**) with the sequence **&\#x26;** this is necessary because the **&** character is interpreted as an escape character in XML rather than a literal. The sequence **&\#x26;** generates a literal **&**.
7.  On the **Build** menu, click **Build VehicleSpeedPredictor**.
8.  Verify that the app compiles successfully.
9.  On the **Debug** menu, click **Start Without Debugging**.
10. At the **Enter a camera ID** prompt, type any number between 0 and 499, and then press Enter.
11. At the **Enter an hour** prompt, type any number between 0 and 23, and then press Enter.
12. At the **Enter a day** prompt, type any number between 1 and 7 (where day 1 is Sunday, and day 7 is Saturday), and then press Enter.
13. After a few moments, the predicted speed at that time for that camera will be displayed.
14. Repeat steps 9-13 several times for other cameras, dates, and times.
15. Close Visual Studio.
16. In Internet Explorer, close the Power BI and web services tabs.

>**Result**: At the end of this exercise, you will have created a trained model, deployed this model as a web service, and then used this service in an application to generate traffic speed predictions for particular camera locations, at particular times of day, and for particular days of the week.

## Exercise 3: Assess SQL Data Warehouse query performance and optimize database configuration

#### Task 1: Assess performance of a baseline query
1.  In SQL Server Management Studio, ensure that you are still connected to **trafficserver&lt;your name&gt;&lt;date&gt;**.
2.  On the **File** menu, point to **Open**, and then click **File**.
3.  In the **Open File** dialog box, go to **E:\\Labfiles\\Lab08\\Exercise3**, click **Exercise3.sql**, and then click **Open**.
4.  In Object Explorer, ensure that **trafficwarehouse** is selected.
5.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
6.  Wait for the query to finish; the query should fetch 7,187,718 rows.
7.  Repeat Steps 5-6 twice more to execute the query another couple of times.
8.  Switch to the Azure portal.
9.  In the Azure portal, click **All Resources**, and then click **trafficwarehouse**.
10.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Monitoring**.
11.  On the **Monitoring** blade, click the **Query Activity** graph.
12.  On the **Queries** blade, click the **Start Time** column twice to order the queries by time, with the most recent at the top.
13.  Click the most recent query that was run by the **student** login.
14.  On the **Query Details** blade, click **Show Query Text**; note that the text matches the query you performed in Step 5.
15. Close the **Query Text** blade.
16. On the **Query Details** blade, click **Show Query Plan**; note that the Query Plan lists five steps (numbered 0 to 4) that are the steps performed by SQL Data Warehouse to run the query.
17. On the **Query Plan** blade, click the first **OnOperation** operation.
18. In the **Query Step Details** blade, click **Show Query Step Command**; note that the Query Step Text shows that SQL Data Warehouse creates a temporary table for storing vehicle registration numbers, and this table is created on every node in the data warehouse.
19. Close the **Query Step Text** blade.
20. On the **Query Plan** blade, click the **BroadcastMoveOperation** operation.
21. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** shows that **VehicleOwner** data for vehicles that have been caught speeding is copied from every node in the data warehouse to the temporary table. This is a potentially expensive operation, as it could involve moving lots of data between nodes.
22. Close the **Query Step Text** blade.
23. On the **Query Plan** blade, click the **ReturnOperation** operation.
24. On the **Query Step Details** blade, click **Show Query Step Command**; note that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding. The details from each node are aggregated and returned as the overall results.
25. Close the **Query Step Text** blade.
26. Click the second **OnOperation** operation.
27. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** deletes the temporary table from each node when the query has finished.
The temporary table and data broadcast operations are required because the vehicle registration information and the vehicle speed information are both distributed throughout the nodes in the data warehouse. While this distribution enables SQL Data Warehouse to manage the load more evenly and reduce the chances of contention, it causes extra work and data movement for some queries. If you perform these queries often, you should consider modifying the distribution policies of the tables used.
28. Close the **Query Step Text**, **Query Step Details**, **Query Plan**, **Query Details**, **Queries**, and **Monitoring** blades.

#### Task 2: Assess query performance when using replicated tables
1.  Switch to SQL Server Management Studio.
2.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT T.name, P.distribution_policy_desc
	FROM sys.pdw_table_distribution_properties P
	JOIN sys.tables T
	ON P.object_id = T.object_id
	GO
	```
This query displays the name of each table in the data warehouse and the distribution policy it uses. Note that the **VehicleOwner** table implements the **ROUND\_ROBIN** policy. The **VehicleSpeed** table is **HASH** (you should recall that the **CameraID** is used as the hash column).
3.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	DBCC PDW_SHOWSPACEUSED('VehicleOwner')
	GO
	```
This command shows how much space the **VehicleOwner** table occupies. The statistics in the various "space" columns are all measured in KB. The **VehicleOwner** table currently consumes between 12 and 12.5 MB in each node. In general, if a relatively static fact table is smaller than 2 GB, consider implementing it as a replicated table. This might reduce the chances of data movement, because the same data is automatically available on every node.
4.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE TABLE VehicleOwner2
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	WITH
	(
	DISTRIBUTION = REPLICATE
	)
	GO
	INSERT INTO VehicleOwner2
	SELECT *
	FROM VehicleOwner
	GO
	```
These commands create a new version of the **VehicleOwner** table (VehicleOwner2) that is replicated. The **INSERT** statement copies the data into this new table.
5.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
Verify that this query still returns 7,187,718 rows. However, it might not be any faster than the previous query (it might actually be a little slower, at least for the first time).
6.  Repeat Step 5 twice more to execute the query another couple of times.
7.  Switch to the Azure portal.
8.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Monitoring**.
9.  On the **Monitoring** blade, click the **Query Activity** graph.
10.  On the **Queries** blade, click the **Start Time** column twice to order the queries by time, with the most recent at the top.
11.  Click the most recent query that was run by the **student** login.
12.  On the **Query Details** blade, click **Show Query Text**, and verify that the text matches the query you performed in Step 5.
13.  Close the **Query Text** blade.
14.  On the **Query Details** blade, click **Show Query Plan**, and note that the Query Plan still comprises five steps (numbered 0 to 4).
15. On the **Query Plan** blade, click the first **OnOperation** operation.
16. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** shows that SQL Data Warehouse is still creating a temporary table for storing vehicle registration numbers on every node in the data warehouse.
17. Close the **Query Step Text** blade.
18. On the **Query Plan** blade, click the **BroadcastMoveOperation** operation.
19. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Text step** shows that **VehicleOwner** data for vehicles that have been caught speeding is still being copied from every node in the data warehouse to the temporary table.
20. Close the **Query Step Text** blade.
21. On the **Query Plan** blade, click the **ReturnOperation** operation.
22. On the **Query Step Details** blade, click **Show Query Step Command**; note that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding. Note that one reason why the performance has not improved—and possibly why it has likely got worse—is because the **ReturnOperation** in Step 3 has to compare data in the temporary table against more data in each node. This is because the **VehicleRegistration** table in each node now contains the complete set of vehicle registrations rather than a smaller subset.
23. Close the **Query Step Text** blade.
24. On the **Query Plan** blade, click the second **OnOperation** operation.
25. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** deletes the temporary table from each node when the query has finished.
26. Close the **Query Step Text**, **Query Step Details**, **Query Plan**, **Query Details**, **Queries**, and **Monitoring** blades.
27. The problem is that, although the **VehicleOwner** data is replicated to each node, the **VehicleSpeed** data is distributed by speed camera ID. One possible idea is that you need to reorganize the speed data by vehicle registration number to ensure that the records for a single vehicle are all held together in the same node.

#### Task 3: Assess query performance when distributing data by vehicle number
1.  Switch to SQL Server Management Studio.
2.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE TABLE VehicleSpeed2
	(
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate DATETIME NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH
	(
	HEAP,
	DISTRIBUTION = HASH(VehicleRegistration),
	PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
	)
	GO
	INSERT INTO VehicleSpeed2
	SELECT *
	FROM VehicleSpeed
	GO
	```
These statements create a copy of the **VehicleSpeed** table that is distributed by **VehicleRegistration** number rather than camera ID. The table is created as a **HEAP** (as was the original table).
3.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed2 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
This is the same query as before, except that it now references the reorganized version of the **VehicleSpeed** table.
4.  Repeat Step 3 twice more, to execute the query another couple of times.
5.  Switch to the Azure portal.
6.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Monitoring**.
7.  On the **Monitoring** blade, click the **Query Activity** graph.
8.  On the **Queries** blade, click the **Start Time** column twice to order the queries by time, with the most recent at the top.
9.  Click the most recent query that was run by the **student** login.
10.  On the **Query Details** blade, click **Show Query Text**, and verify that the text matches the query you performed in Step 3.
11.  Close the **Query Text** blade.
12.  On the **Query Details** blade, click **Show Query Plan**, and note that the Query Plan still comprises five steps (numbered 0 to 4).
13. On the **Query Plan** blade, click the first **OnOperation** operation.
14. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** shows that SQL Data Warehouse is still creating a temporary table for storing vehicle registration numbers on every node in the data warehouse.
15. Close the **Query Step Text** blade.
16. On the **Query Plan** blade, click the **BroadcastMoveOperation** operation.
17. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Text** step shows that **VehicleOwner** data for vehicles that have been caught speeding is still being copied from every node in the data warehouse to the temporary table.
18. Close the **Query Step Text** blade.
19. On the **Query Plan** blade, click the **ReturnOperation** operation.
20. On the **Query Step Details** blade, click **Show Query Step Command**; note again that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding.
21. Close the **Query Step Text** blade.
22. On the **Query Plan** blade, click the second **OnOperation** operation.
23. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** deletes the temporary table from each node when the query has finished.
24. Close the **Query Step Text**, **Query Step Details**, **Query Plan**, **Query Details**, **Queries**, and **Monitoring** blades.
25. You suspect that implementing the **VehicleSpeed** table as a heap might be the problem. Perhaps it’s better to index this data as a **ColumnStore** structure.

#### Task 4: Assess query performance when using columnstore
1.  Switch to SQL Server Management Studio.
2.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE TABLE VehicleSpeed3
	(
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate DATETIME NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH
	(
	DISTRIBUTION = HASH(VehicleRegistration),
	PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
	)
	GO
	INSERT INTO VehicleSpeed3
	SELECT *
	FROM VehicleSpeed
	GO
	```
3.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed3 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
Note that this time the speed might be much slower than in previous examples.
4.  Repeat Step 3 twice more to execute the query another couple of times.
5.  Switch to the Azure portal.
6.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Monitoring**.
7.  On the **Monitoring** blade, click the **Query Activity** graph.
8.  On the **Queries** blade, click the **Start Time** column twice to order the queries by time, with the most recent at the top.
9.  Click the most recent query that was run by the **student** login.
10.  On the **Query Details** blade, click **Show Query Text**, and verify that the text matches the query you performed in Step 3.
11.  Close the **Query Text** blade.
12.  On the **Query Details** blade, click **Show Query Plan**, and note that the Query Plan still comprises five steps (numbered 0 to 4).
13. On the **Query Plan** blade, click the first **OnOperation** operation.
14. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** shows that SQL Data Warehouse is still creating a temporary table.
15. Close the **Query Step Text** blade.
16. On the **Query Plan** blade, click the **BroadcastMoveOperation** operation.
17. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Text** step shows that data is still being copied from every node in the data warehouse to the temporary table.
18. Close the **Query Step Text** blade.
19. On the **Query Plan** blade, click the **ReturnOperation** operation.
20. On the **Query Step Details** blade, click **Show Query Step Command**; note again that this step uses the data in the temporary table in each node.
21. Close the **Query Step Text** blade.
22. On the **Query Plan** blade, click the second **OnOperation** operation.
23. On the **Query Step Details** blade, click **Show Query Step Command**; note that the **Query Step Text** deletes the temporary table from each node when the query has finished.
24. Close the **Query Step Text**, **Query Step Details**, **Query Plan**, **Query Details**, **Queries**, and **Monitoring** blades.
25. You realize that the issue might arise because of the way in which the query is performed. The query optimizer has to drive the query from the replicated table to find all **VehicleOwner** records that have no matching speed records. The query fetches the speed data from all nodes to compare against a single instance of the replicated vehicle registration table. If the query could drive from the **VehicleSpeed** table instead, it could quickly look up the necessary data locally in each node and avoid the data movement. The idea, therefore, is to ensure that the data in the **VehicleOwner** and **VehicleSpeed** tables are distributed in the same way.

#### Task 5: Assess query performance when distributing linked data to the same node
1.  Switch to SQL Server Management Studio.
2.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	CREATE TABLE VehicleOwner3
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	WITH
	(
	DISTRIBUTION = HASH(VehicleRegistration)
	)
	GO
	INSERT INTO VehicleOwner3
	SELECT *
	FROM VehicleOwner
	GO
	```
These commands create another version of the **VehicleOwner** table, hashed by registration number. This matches the distribution policy of the **VehicleSpeed** table, so vehicle ownership and speed records for any given vehicle should now be held in the same node in the data warehouse.
3.  In the SQL Editor, highlight the following command, and then click **Execute**:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner3 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed3 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
4.  Repeat Step 3 twice more to execute the query another couple of times.
5.  Switch to the Azure portal.
6.  On the **trafficwarehouse** blade, in the **Common Tasks** section, click **Monitoring**.
7.  On the **Monitoring** blade, click the **Query Activity** graph.
8.  On the **Queries** blade, click the **Start Time** column twice to order the queries by time, with the most recent at the top.
9.  Click the most recent query that was run by the **student** login.
10.  On the **Query Details** blade, click **Show Query Text**, and verify that the text matches the query you performed in Step 3.
11.  Close the **Query Text** blade.
12.  On the **Query Details** blade, click **Show Query Plan**, and note that this time the Query Plan contains a single **ReturnOperation** step.
13. On the **Query Plan** blade, click the **ReturnOperation** operation.
14. On the **Query Step Details** blade, click **Show Query Step Command**; note that this step shows that the data is retrieved from each node without creating a temporary table or moving data between nodes.
15. Close the **Query Step Text**, **Query Step Details**, **Query Plan**, **Query Details**, **Queries**, and **Monitoring** blades.
16. Close SQL Server Management Studio.

>**Result**: At the end of this exercise, you will have run a series of queries against SQL Data Warehouse, assessed how each query is processed, and reconfigured the data structure several times to see the impact on the query process.

## Exercise 4: Configure SQL Data Warehouse auditing and analyze threats

#### Task 1: Enable auditing and threat detection
1.  In the Azure portal, on the **trafficwarehouse** blade, under **SETTINGS**, click **Auditing & Threat Detection**.
2.  On the **trafficwarehouse -** **Auditing & Threat Detection** blade, under **Auditing**, click **ON**, and then click **Storage details**.
3.  On the **Audit logs storage** blade, click **Storage account**.
4.  On the **Create Storage account** blade, in the **Name** box, type **auditdata*&lt;your name&gt;&lt;date&gt;***, and then click **OK**.
5.  Wait until the storage account has been successfully created before continuing with the exercise.
6.  On the **Audit logs storage** blade, click **OK**.
7.  On the **trafficwarehouse -** **Auditing & Threat Detection** blade, under **Auditing**, click **Audited events**.
8.  Verify that all events are selected, and then click **OK**.
9.  On the **trafficwarehouse -** **Auditing & Threat Detection** blade, under **Threat Detection**, click **ON**, and then click **Threat Detection types**.
10. Verify that all types are selected, and then click **OK**.
11. In the **Send alerts to** box, type your own email address, or the email address of your Azure Learning Pass account.
12. Ensure that **Email service** **and** **co-administrators** is selected, click **Save**, and then click **OK**.

#### Task 2: Generate audit and threat test data
1.  On the Start menu, type **Visual Studio 2017**, and then press Enter.
2.  In Visual Studio, click **Open Project / Solution**.
3.  In the **Open Project** dialog box, go to the **E:\\Labfiles\\Lab08\\Exercise4\\Threaten** folder, click **Threaten.sln**, and then click **Open**.
4.  In Solution Explorer, double-click **App.config**.
5.  In App.config, in the **appSettings** section, replace the Server value **&lt;YourServerName&gt;** with **trafficserver*&lt;your name&gt;&lt;date&gt;***.
6.  In Solution Explorer, double-click **Program.cs**.
7.  View the code in the **RunQuery** method; this is a classic example of a SQL injection vulnerability. The code runs a SELECT statement to retrieve data using criteria specified by the user. The code makes no attempt to validate the user input, and the user can provide a string that contains all sorts of “nasty stuff”.
8.  On the **Build** menu, click **Build Threaten**.
9.  Verify that the app compiles successfully.
10. On the **Debug** menu, click **Start Without Debugging**.
11. At the application prompt, type **AAA 111**, and then press Enter; the application will attempt to find how many stolen vehicle records there are for this registration (0).
12. Press any key to close the app console window.
13. On the **Debug** menu, click **Start Without Debugging**.
14. At the application prompt, type **HSZ 883**, and then press Enter; the application will attempt to find how many stolen vehicle records there are for this registration (this vehicle has been reported stolen once).
15. Press any key to close the app console window.
16. On the **Debug** menu, click **Start Without Debugging**.
17. At the application prompt, type **H%**, and then press Enter; this time, the application will display how many times vehicles with a registration number that begins with the letter H have been stolen (59372).
18. Press any key to close the app console window.
19. On the **Debug** menu, click **Start Without Debugging**.
20. At the application prompt, type the following text on a single line, and then press Enter:
	```
	H%'; DROP TABLE Test; --
	```
21.  This query returns the same number of rows, but also causes a DROP TABLE command to be run; the DROP TABLE command is effectively appended to the end of the SELECT statement. The first quote in the data finishes off the string expected by the SELECT statement. The semicolon starts a new statement, and the "--" starts a comment, which causes the final quote character in the string added by the application to be ignored, so that it doesn't cause a syntax error that would make the command fail.
22.  This is extremely dangerous. The application has effectively provided a back door into your database and a malicious user could do access your data.
23.  Fortunately, enabling threat detection causes SQL Data Warehouse to recognize this situation, and the injected command (DROP TABLE Test) will not be run.
24.  Press any key to close the app console window.

#### Task 3: View audit logs and alerts
1.  Open the inbox for the email address you used in the previous task; steps will vary, depending on the email provider.
2.  You should have a message from SQL Data Warehouse warning you that a potential SQL injection attack has occurred.
3.  Note that this email includes information on:
 -   The target Azure subscription
 -   The target Azure SQL Server and the database
 -   The IP address of the source of the attack
 -   The date and time of the attack
 -   Potential causes and recommendations
4.  Switch to the Azure portal.
5.  On the **trafficwarehouse - Auditing & Threat Detection** blade, click **View audit logs**; note the list of all recent actions and queries that have been performed.
6.  Near the top of the list, you should see an entry with event type **DataAccess**, and another of type **SchemaChanges**; the **ACTION STATUS** of both records should be **Failure**. This indicates that SQL Data Warehouse detected a problem and did not run these statements.
7.  Click the **SchemaChanges** record.
8.  On the **Audit record** blade, examine the statement, which should look like this:
	```
	SELECT COUNT(*) FROM dbo.StolenVehicle WHERE VehicleRegistration LIKE 'H%'; DROP TABLE Test; -- '
	```
9.  You see how the user input has been appended to the query to perform the DROP TABLE operation.
10.  Examine the older statements from previous runs of the application; these statements should all have an action status of success, and indicate operations that were performed successfully.

#### Task 4: Monitor login failures
1.  Switch to Visual Studio.
2.  In app.config, edit the **Password** value, and set it to a random string.
3.  On the **Build** menu, click **Build Threaten**.
4.  Verify that the app compiles successfully.
5.  On the **Debug** menu, click **Start Without Debugging**.
6.  At the application prompt, type **H%**, and then press Enter; this time the application should stop with a “Login failed” exception.
7.  Press any key to close the app console window.
8.  Switch to the Azure portal.
9.  On the **Audit records** blade, click **Refresh** to see the most recent records. This might take a few minutes to update.
10. Note that there is now a **Login** record with an **ACTION STATUS** of **Failure**.
11. Click the **Login Failure** record; note that the record details include the IP address of the client application. Such information is useful so that, if necessary, you could block this IP address in the firewall of the database server.

#### Task 5: Lab cleanup
1.  Close the **Audit** **record**, and **Audit records** blades.
2.  On the **trafficwarehouse – Auditing & Threat Detection** blade, click **Overview**, click **Pause**, and then click **Yes**.
3.  Close Visual Studio.

>**Result**: At the end of this exercise, you will have enabled auditing, and used a sample application that includes a SQL injection vulnerability to attempt to attack the data warehouse. You will also have examined the audit logs, including identifying login failures.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
