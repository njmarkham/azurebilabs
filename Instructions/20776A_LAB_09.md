# Module 9: Automating Data Flow with Azure Data Factory

# Lab: Automating the Data Flow with Azure Data Factory

### Scenario
You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then cross-check the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed road-side cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that can read vehicle registration plates.

For this final phase of the project, you are going to use Azure Data Factory to automate the management of data associated with the traffic surveillance system. You will use a Data Factory pipeline to automatically backup stolen vehicle data from one Data Lake Store to another. You will also use Data Factory pipelines to perform batch transformations—using Azure Data Analytics to summarize speed camera data as the data is uploaded from one Data Lake Store to another, and using Azure ML to perform predictive analytics on speed data as it is uploaded from an Azure Storage blob. In order to ensure the reliability of your Data Factory pipelines, you are also going to test the monitoring and management capabilities provided with Azure Data Factory.

### Objectives
After completing this lab, you will be able to:
-   Use Data Factory to back up data from an Azure Data Lake Store to a second ADLS store.
-   Transform uploaded data by running a U-SQL script in an ADLA linked service.
-   Transform uploaded data by running an ML model in a Machine Learning linked service.
-   Use the monitoring and management app to track progress of a pipeline.

### Lab Setup
Estimated time: 75 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

>**Note:** This lab uses the following resources from previous labs:
-   **Resource group**: CamerasRG
-   **Azure SQL Data Warehouse**: trafficwarehouse
-   **Azure SQL Server**: trafficserver*&lt;your name&gt;&lt;date&gt;*
-   **Azure Blob storage**: speeddata*&lt;your name&gt;&lt;date&gt;*, vehicledata*&lt;your name&gt;&lt;date&gt;*
-   **Machine Learning namespace**: Traffic
-   **Azure Data Lake Store**: adls*&lt;your name&gt;&lt;date&gt;*
-   **Azure Data Lake Analytics account**: speedsdla*&lt;your name&gt;&lt;date&gt;*

## Exercise 1: Use Data Factory to back up data from an Azure Data Lake Store to a second ADLS store

### Scenario
You’re using Azure Data Factory to automate the management of data associated with the traffic surveillance system, and will use a Data Factory pipeline to automatically back up stolen vehicle data from one Data Lake Store to another.

In this exercise, you will check the data in the original ADLS account, and then create a second ADLS account which will host the backup location. After creating a new Data Factory, you will create and configure a service principal so that Data Lake Store authorization can occur during the execution of a Data Factory pipeline. You will then assign permissions to the Service Principal to enable data copying, and use the Data Factory Copy Wizard to back up the data from one ADLS account to another.

The main tasks for this exercise are as follows:
1. Verify the data in the original ADLS account
2. Create a second backup ADLS account
3. Create new Data Factory
4. Create a Service Principal
5. Assign copy data permissions to the Service Principal
6. Use Copy Wizard to back up data from ADLS1 to ADLS2

#### Task 1: Verify the data in the original ADLS account
1.  Use the Azure Portal Data Explorer to view the contents of the **Stolen** folder in **adls*&lt;your name&gt;&lt;date&gt;***.
2.  Verify that all the files and folders you imported in Lab07 are present.

#### Task 2: Create a second backup ADLS account
-   Use the Azure portal to create a new Data Lake Store called **adlsbackup*&lt;your name&gt;&lt;date&gt;***, with the following details:
 -   **Resource group (Use existing)**: **CamerasRG**
 -   **Location**: select the same location as you used for your original Data Lake Store
 -   Leave all other settings at their defaults

#### Task 3: Create new Data Factory
-   Use the Azure portal to create a new Data Factory called **TrafficDF*&lt;your name&gt;&lt;date&gt;***, with the following details:
 -   **Resource group (Use existing)**: CamerasRG
 -   **Version**: V1
 -   **Location**: from the list of currently available Data Factory regions, select the same location as you used for your Data Lake Stores, or if this is not available, select your nearest location.

#### Task 4: Create a Service Principal
1.  Use the Azure portal to create a new application registration in **Azure Active Directory**, with the following details:
 -   **Name**: ADLStoDataFactory
 -   **Application type**: Web app/ API
 -   **Sign-on URL**: https://ADLStoDataFactory/Dummy
> Note that the actual URL entered on this blade is immaterial as you do not actually build or deploy an app at this location; it is merely acting as an identifier.
2.  Copy the **Application ID**, paste it to Notepad, and save the file as **Auth\_details.txt** in your Documents folder.
3.  In the Azure portal, create a new key for **ADLStoDataFactory**, with the following information:
 -   **Key description**: Key1
 -   **Duration**: select In 1 year
4.  Copy the **VALUE** for **Key1**, and save it to **Auth\_details.txt**.
5.  In the Azure portal, copy the **Directory ID** for your Azure Active Directory, and save it to **Auth\_details.txt**.

#### Task 5: Assign copy data permissions to the Service Principal
1.  In the Azure portal, open the Data Explorer for **adls*&lt;your name&gt;&lt;date&gt;***, and then for the root folder, assign the following to **ADLStoDataFactory**:
 -   Read
 -   Write
 -   Execute
 -   This folder and all children
 -   An access permission entry and a default permission entry
> **IMPORTANT**. Wait while permissions are assigned to **ADLStoDataFactory**; notice the assignments to each file and folder is displayed in the **Assigning permissions to** box. Do not continue with the exercise until you see the notification that the permissions assignment has successfully completed (this might take several minutes), and the blue assigning graphic becomes a green check mark.
2.  Open the Data Explorer for **adlsbackup*&lt;your name&gt;&lt;date&gt;***, and then for the root folder, assign the following to **ADLStoDataFactory**:
 -   Read
 -   Write
 -   Execute
 -   This folder and all children
 -   An access permission entry and a default permission entry
> Note that, because there is no data in this Lake Store, the permissions assignment is very quick.
3.  In the new Data Lake Store, create a new folder called **Stolen**.

#### Task 6: Use Copy Wizard to back up data from ADLS1 to ADLS2
1.  In the Azure portal, run the **Copy data (PREVIEW)** wizard for your Data Factory, using the following configuration:
 -   **Task name**: Traffic DF Copy Pipeline
 -   **Start date time (UTC)**: set the date to be five days before today
 -   **End date time (UTC)**: set the date to be today.
 -   **Source**: Azure Data Lake Store
 -   **Connection name**: Input Data Lake Store
 -   **Azure subscription**: select your Azure Pass subscription
 -   **Data Lake store account name**: adls*&lt;your name&gt;&lt;date&gt;*
 -   **Authentication type**: Service Principal
 -   **Tenant**: ensure that the value matches the **Directory ID** that you saved in **Auth\_details.txt**
 -   **Service principal id**: paste the **Application ID** value that you saved in **Auth\_details.txt**
 -   **Service principal key**: paste the **API key** (Key1) value that you saved in **Auth\_details.txt**
 -   **Choose the input file or folder**: Stolen
 -   Select the **Copy files recursively**, and **Binary copy** check boxes
 -   **Destination**: Azure Data Lake Store
 -   **Connection name**: Output Data Lake Store
 -   **Azure subscription**: select your Azure Pass subscription
 -   **Data Lake store account name**: adlsbackup*&lt;your name&gt;&lt;date&gt;*
 -   **Authentication type**: Service Principal
 -   **Tenant**: ensure that the value matches the **Directory ID** that you saved in **Auth\_details.txt**
 -   **Service principal id**: paste the **Application ID** value that you saved in **Auth\_details.txt**
 -   **Service principal key**: paste the **API key** (Key1) value that you saved in **Auth\_details.txt**
 -   **Choose the output file or folder**: Stolen
> Note the verification steps, and start the deployment.
2.  When the deployment has completed, click **Click here to monitor copy pipeline**.
> Note the datasets and pipeline in the diagram; there should be a Copy animation showing in the pipeline as the pipeline is active.
3.  In the **ACTIVITY WINDOWS** list, click the most recent activity (this may still be showing as **In progress**), to show more details in the right-hand Activity Window Explorer pane; if you do not see this, click **Refresh**.
4.  Scroll down through the details, and if the job has completed note the data read and written, row count, copy duration, and billed duration.
5.  In the **ACTIVITY WINDOWS** list, note that there is an activity for each day in the period between your specified start and end times.
> Note that you will check the results of this job in a later exercise, after it has finished processing.

>**Result**: At the end of this exercise, you will have:
1.  Verified the data in the original ADLS account.
2.  Created a second ADLS account to act as the backup location.
3.  Created a new Data Factory.
4.  Created a service principal to enable Data Lake Store authorization in a Data Factory pipeline.
5.  Assigned permissions to the Service Principal to enable data copying.
6.  Used the Data Factory Copy Wizard to backup data from one ADLS account to another.

## Exercise 2: Transform uploaded data by running a U-SQL script in an ADLA linked service

### Scenario

You are using Azure Data Factory to automate the management of data associated with the traffic surveillance system, and will use Azure Data Factory pipelines to perform batch transformations. By using an Azure Data Analytics linked service, you will summarize speed camera data as the data is uploaded from one Data Lake Store to another.

In this exercise, you will upload a CSV file containing camera speed data to ADLS ready for processing in ADLA by using a U-SQL script; To authorize ADLA to process this data in a Data Factory pipeline, you will add the Service Principal from Exercise 1 as a Contributor to the ADLA account. You will then create Data Factory linked services for Azure Data Lake Analytics, for Data Lake Store (for the input and output datasets), and for Azure Storage (as the scripts location for the U-SQL script). You will then create Data Lake Store input and output datasets, create a script for the Data Lake Analytics U-SQL Activity that extracts summary data for a specific speed camera. Finally, you will create and deploy a new pipeline to run this activity, and check the results to verify the U-SQL data transformation.

The main tasks for this exercise are as follows:
1. Prepare the environment
2. Add the Service Principal as Contributor to the ADLA account
3. Create an Azure Data Lake Analytics linked service
4. Create a Data Lake Store linked service for input and output datasets
5. Create an Azure Storage Blob linked service for the U-SQL script
6. Create Data Lake Store input and output datasets
7. Create a Data Lake Analytics U-SQL activity
8. Create and deploy a new pipeline
9. Verify the U-SQL data transformation

#### Task 1: Prepare the environment
1.  Use the Azure portal to create a new container called **cameratestdata** in your **speeddata*&lt;your name&gt;&lt;date&gt;*** storage account.
2.  In the storage account, locate the value for **key1**, and save this key value to **Auth\_details.txt**.
3.  At an Admin Command Prompt (Admin), execute the following commands, replacing **&lt;storage account name&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;access key&gt;** with the **Storage access key** value that you saved in **Auth\_details.txt**:
    ```
    azcopy /Source:"E:\Labfiles\Lab09\\xercise2" /Pattern:"CameraData.csv" /Dest:https://<storage account name>.blob.core.windows.net/cameratestdata /DestKey:<access key> /S

    azcopy /Source:"E:\Labfiles\Lab09\Exercise2" /Pattern:"CameraDataProcessing.txt" /Dest:https://<storage account name>.blob.core.windows.net/scripts /DestKey:<access key> /S
    ```
> You copy the preceding commands from: **E:\\LabFiles\\Lab09\\Exercise2\\AZCopyCmd.txt**.

4.  When the copies have completed, execute the following command, replacing **&lt;storage account name&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***, replacing **&lt;data lake store&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;access key&gt;** with the **Storage access key** value that you saved in **Auth\_details.txt**:
    ```
    adlcopy /source https://<storage account name>.blob.core.windows.net/cameratestdata/ /dest adl://<data lake store>.azuredatalakestore.net/CameraTestData/ /sourcekey <access key>
    ```
> The above command can be copied from **E:\\LabFiles\\Lab09\\Exercise2\\AdlCopyCmd.txt**.

#### Task 2: Add the Service Principal as Contributor to the ADLA account
-   Use the Azure portal to add **ADLStoDataFactory** to **speedsdla*&lt;your name&gt;&lt;date&gt;***, with **Contributor** permissions.

#### Task 3: Create an Azure Data Lake Analytics linked service
1.  Use the Azure portal to create a new Azure Data Lake Analytics linked service.
2.  In the JSON editor, edit **accountName**, replacing the existing value with **speedsdla*&lt;your name&gt;&lt;date&gt;***.
3.  Delete the following lines:
	```
	"authorization": "<Authorization code is automatically retrieved after clicking 'Authorize' and completing the OAuth login>",

    "subscriptionId": "<[Optional] Data Lake Analytics account subscription ID (if different from Data Factory account)>",

    "resourceGroupName": "<[Optional] Data Lake Analytics account resource group name (if different from Data Factory account)>",

    "sessionId": "<OAuth session id from OAuth authorization session. Each session id is unique and may only be used once>"
	```
4.  Add the following lines, replace **&lt;service principal id&gt;** with the **Application ID** value, replace **&lt;service principal key&gt;** with the **API key** (Key1) value, and replace **&lt;tenant id&gt;** with the **Directory ID** that you saved in **Auth\_details.txt** (you copy these from **E:\\Labfiles\\Lab09\\Exercise2\\JsonCmd1.txt**):
	```
	"dataLakeAnalyticsUri": "azuredatalakeanalytics.net",

    "servicePrincipalId": "<service principal id>",

    "servicePrincipalKey": "<service principal key>",

    "tenant": "<tenant id>"
	```
5.  Deploy the linked service then open **AzureDataLakeAnalyticsLinkedService**, and note that the JSON-formatted information is the configuration detail for the connection to your Azure Data Lake Analytics, and that the key has now been obfuscated with *\*\*\*\*\*\*\*\*.

#### Task 4: Create a Data Lake Store linked service for input and output datasets
1.  Use the Azure portal to create a new Azure Data Lake Store data store.
2.  In the JSON editor, edit the following properties:
 -   **dataLakeStoreUri**: replace the existing value with **https://adls*&lt;your name&gt;&lt;date&gt;*.azuredatalakestore.net/webhdfs/v1** (you copy the URI from **E:\\Labfiles\\Lab09\\Exercise2\\JsonCmd2.txt**)
 -   **accountName**: replace the existing value with **adls*&lt;your name&gt;&lt;date&gt;***
 -   **servicePrincipalId**: replace the existing value with the Application ID value that you saved in Auth\_details.txt
 -   **servicePrincipalKey**: replace the existing value with the **API key** (Key1) value that you saved in **Auth\_details.txt**
3.  Delete the following lines:
	```
	"resourceGroupName": "<[Optional] Data Lake Store account resource group name (if different from Data Factory account)>",

    "subscriptionId": "<[Optional] Data Lake Store account subscription ID (if different from Data Factory account)>"
	```
4.  Delete the comma (,) at the end of the **tenant** line, and deploy the linked service.

#### Task 5: Create an Azure Storage Blob linked service for the U-SQL script
1.  Use the Azure portal to create a new Azure Storage data store.
2.  In the JSON editor, edit the **connectionString**, replacing **&lt;accountname&gt;** with **speeddata*&lt;your name&gt;&lt;date&gt;***, and replacing **&lt;accountkey&gt;** with the **Storage access key** value that you saved in **Auth\_details.txt**.
3.  Deploy the linked service.

#### Task 6: Create Data Lake Store input and output datasets
1.  Use the Azure portal to create a new Azure Data Lake Store dataset, replacing the existing JSON text with the following (you copy this from **E:\\Labfiles\\Lab09\\Exercise2\\JsonCmd3.txt**):
	```
	{
	"name": "Input CameraTest DataLakeStore",
	"properties": {
	"type": "AzureDataLakeStore",
	"linkedServiceName": "AzureDataLakeStoreLinkedService",
	"typeProperties": {
	"folderPath": "CameraTestData/",
	"fileName": "CameraData.csv",
	"format": {
	"type": "TextFormat",
	"rowDelimiter": "\r\n",
	"columnDelimiter": ",",
	"firstRowAsHeader": true
	}
	},
	"external": true,
	"availability": {
	"frequency": "Minute",
	"interval": 30
	},
	"policy": {
	"externalData": {
	"retryInterval": "00:01:00",
	"retryTimeout": "00:10:00",
	"maximumRetry": 3
	}
	}
	}
	}
	```
2.  Deploy the dataset.
3.  Create another Azure Data Lake Store dataset, and replace the existing JSON text with the following (you copy this from **E:\\Labfiles\\Lab09\\Exercise2\\JsonCmd4.txt**):
	```
	{
	"name": "Output CameraTest DataLakeStore",
	"properties": {
	"type": "AzureDataLakeStore",
	"linkedServiceName": "AzureDataLakeStoreLinkedService",
	"typeProperties": {
	"folderPath": "CameraTestData/output/"
	},
	"external": false,
	"availability": {
	"frequency": "Minute",
	"interval": 30
	},
	"policy": {}
	}
	}
	```
4.  Deploy the dataset.

#### Task 7: Create a Data Lake Analytics U-SQL activity
1.  Open **E:\\Labfiles\\Lab09\\Exercise2\\CameraDataProcessing.txt**, to view the following script:
	```
	// Find the statistics for a specific camera
	@datasource =
	EXTRACT CameraID string,
	SpeedLimit int,
	RecordedSpeed int,
	VehicleRegistration string,
	DetectionDateTime DateTime
	FROM @in
	USING Extractors.Csv(skipFirstNRows:1,nullEscape:"#NULL#");

    @speedSummary =
	SELECT CameraID,
	MAX(SpeedLimit) AS SpeedLimit,
	COUNT(*) AS NumberOfObservations,
	MIN(RecordedSpeed) AS Lowest,
	MAX(RecordedSpeed) AS Highest,
	AVG(RecordedSpeed) AS Average
	FROM @datasource
	WHERE CameraID == @camera
	GROUP BY CameraID;

    // Save the results
	OUTPUT @speedSummary
	TO @out
	USING Outputters.Csv(outputHeader: true, quoting: false);
	```
> Note that this U-SQL script uses the CSV extractor to read a datafile that is stored at @in, extract some data, and assign this extract to @datasource. The script then performs some analysis on the data from a particular camera (as specified by @camera), and assigns the results to @speedSummary. The results in @speedSummary are then written out using the CSV outputter, and saved to the @out location. The @in, @out, and @camera parameters are assigned in the pipeline, not in this script.

2.  Close **CameraDataProcessing.txt**.

#### Task 8: Create and deploy a new pipeline
1.  Use the Azure portal to create a new pipeline, replacing the existing JSON text with the following (you copy this from **E:\\Labfiles\\Lab09\\Exercise2\\JsonCmd5.txt**):
	```
	{
	"name": "Speed data extract pipeline",
	"properties": {
	"description": "Pipeline to extract speed data for a specific CameraID.",
	"activities":
	[
	{
	"type": "DataLakeAnalyticsU-SQL",
	"typeProperties": {
	"scriptPath": "scripts\\CameraDataProcessing.txt",
	"scriptLinkedService": "AzureStorageLinkedService",
	"degreeOfParallelism": 3,
	"priority": 100,
	"parameters": {
	"in": "/CameraTestData/CameraData.csv",
	"out": "/CameraTestData/output/SpeedSummary.csv",
	"camera": "Camera 121"
	}
	},
	"inputs": [
	{
	"name": "Input CameraTest DataLakeStore"
	}
	],
	"outputs":
	[
	{
	"name": "Output CameraTest DataLakeStore"
	}
	],
	"policy": {
	"timeout": "06:00:00",
	"concurrency": 1,
	"executionPriorityOrder": "NewestFirst",
	"retry": 1
	},
	"scheduler": {
	"frequency": "Minute",
	"interval": 30
	},
	"name": "ASA U-SQL process",
	"linkedServiceName": "AzureDataLakeAnalyticsLinkedService"
	}
	],
	"start": "2017-10-28T01:00:00Z",
	"end": "2017-10-30T01:00:00Z",
	"isPaused": false
	}
	}
	```
> **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.

2.  Deploy the pipeline.

#### Task 9: Verify the U-SQL data transformation
1.  Open the **Data Factory Diagram** and, if necessary, use the mouse to rearrange the pipelines so that they do not overlap.
2.  Open **Input CameraTest DataLakeStore** and, under **Monitoring**, note the slices with a **Ready** or **In Progress** status (you might need to click **See more** to see these, depending on your current time in relation to UTC).
3.  Open **Output CameraTest DataLakeStore**, and under **Monitoring**, note the slices with a **Ready** or **In Progress** status (you might need to click **See more** to see these, depending on your current time in relation to UTC).
4.  Using Data Explorer for your original Data Lake Store, open **CameraTestData/output/SpeedSummary.csv**; this file should contain a header row and one row of data, showing the following:
 -   The camera ID.
 -   The maximum speed limit at that camera (that is, the actual speed limit, which is 30).
 -   The number of recorded observations at that camera.
 -   The lowest speed recorded at that camera (0).
 -   The highest speed recorded at that camera (21).
 -   The average speed recorded at that camera (7).

>**Result**: At the end of this exercise, you will have:
1.  Prepared your environment and uploaded test data to your Data Lake Store.
2.  Added the Service Principal as a Contributor to the ADLA account.
3.  Created an Azure Data Lake Analytics linked service.
4.  Created a Data Lake Store linked service for input and output datasets.
5.  Created an Azure Storage Blob linked service for the U-SQL script.
6.  Created Data Lake Store input and output datasets.
7.  Created a Data Lake Analytics U-SQL Activity.
8.  Created and deployed a new pipeline.
9.  Verified the U-SQL data transformation.

## Exercise 3: Transform uploaded data by running an ML model in a Machine Learning linked service

### Scenario

You are using Azure Data Factory to automate the management of data associated with the traffic surveillance system, and will use Azure Data Factory pipelines to perform batch transformations. By using an Azure ML linked service, you will perform predictive analytics on speed data as it’s uploaded from an Azure Storage blob.

In this exercise, you will start your Data Warehouse, because this was the original data source for the deployed ML model you are going to use—you will then obtain the API key and batch execution URL for this ML model. You will then create an ML linked service, using these parameters, and then create Azure Storage input and output datasets that link to live test input data, and an output results location respectively. Finally, you will create and deploy a new pipeline, and check the results in order to verify the ML data transformation.

The main tasks for this exercise are as follows:
1. Prepare the environment
2. Obtain the API key and batch execution URL for a deployed ML model
3. Create an ML linked service
4. Create Azure Storage input and output datasets
5. Create and deploy a new pipeline
6. Verify the ML data transformation
>**Note**: This exercise uses the deployed Decision Forest Trained Regression Model for Vehicle Speeds from Lab08, Exercise 2, and the web service details that were saved to Config\_details.txt. If you did not complete Lab08, Exercise 2, you will need to go back to Lab08, Exercise 2 and work through Tasks 1 – 3 in order to be able to complete the following exercise in Lab09.

#### Task 1: Prepare the environment
1.  At an Admin Command Prompt (Admin), execute the following command, replacing **&lt;storage account name&gt;** with **speeddata&lt;your name&gt;&lt;date&gt;**, and replacing **&lt;access key&gt;** with the **Storage access key** value that you saved in **Auth\_details.txt**:
	```
	azcopy /Source:"E:\Labfiles\Lab09\Exercise3" /Pattern:"LiveSpeedData.csv" /Dest:https://<storage account name>.blob.core.windows.net/capturedspeeds /DestKey:<access key> /S
	```
> You copy the preceding commands from **E:\\LabFiles\\Lab09\\Exercise3\\AZCopyCmd.txt**.

2. Use the Azure portal to start the **trafficwarehouse** Data Warehouse.

#### Task 2: Obtain the API key and batch execution URL for a deployed ML model
1.  In Machine Learning Studio for the Traffic Machine Learning Studio workspace, open the web service for the experiment that you created in Lab08, Exercise 2.
2.  Copy the **API key** to Notepad.
3.  On the **Batch Execution** page, copy the **Request URI** to Notepad and save the file in your Documents folder, as **Web\_config.txt**.

#### Task 3: Create an ML linked service
1.  Use the Azure portal to create a new Azure ML linked service.
2.  In the JSON editor, edit the following properties:
 -   **mlEndpoint**: replace the existing value with the **Request URI** you saved to **Config\_details.txt** (remember to use double quotes around the string).
 -   **apiKey**: replace the existing value with the **Web service API key** you saved to **Config\_details.txt** (remember to use double quotes around the key).
3.  Delete the three lines marked as **(Optional)**.
4.  Deploy the linked service, open **AzureMLLinkedService**, and note that the JSON-formatted information is the configuration detail for the connection to the Machine Learning web service, and that the API key has now been obfuscated with \*\*\*\*\*\*\*\*\*.

#### Task 4: Create Azure Storage input and output datasets
1.  Use the Azure portal to create a new Azure Blob storage dataset, replacing the existing JSON text with the following (you copy this from **E:\\Labfiles\\Lab09\\Exercise3\\JsonCmd1.txt**):
	```
	{
	"name": "Input from Azure Storage",
	"properties": {
	"type": "AzureBlob",
	"linkedServiceName": "AzureStorageLinkedService",
	"typeProperties": {
	"folderPath": "capturedspeeds/",
	"fileName": "LiveSpeedData.csv",
	"format": {
	"type": "TextFormat",
	"columnDelimiter": ","
	}
	},
	"external": true,
	"availability": {
	"frequency": "Minute",
	"interval": 15
	},
	"policy": {
	"externalData": {
	"retryInterval": "00:01:00",
	"retryTimeout": "00:10:00",
	"maximumRetry": 3
	}
	}
	}
	}
	```
2.  Deploy the dataset.
3.  Create another Azure Blob storage dataset, replacing the existing JSON text with the following (this can be copied from **E:\\Labfiles\\Lab09\\Exercise3\\JsonCmd2.txt**):
	```
	{
	"name": "Output to Azure Storage",
	"properties": {
	"type": "AzureBlob",
	"linkedServiceName": "AzureStorageLinkedService",
	"typeProperties": {
	"folderPath": "cameralivedata/scored/{folderpart}/",
	"fileName": "{filepart}result.csv",
	"partitionedBy": [
	{
	"name": "folderpart",
	"value": {
	"type": "DateTime",
	"date": "SliceStart",
	"format": "yyyyMMdd"
	}
	},
	{
	"name": "filepart",
	"value": {
	"type": "DateTime",
	"date": "SliceStart",
	"format": "HHmmss"
	}
	}
	],
	"format": {
	"type": "TextFormat",
	"columnDelimiter": ","
	}
	},
	"availability": {
	"frequency": "Minute",
	"interval": 15
	}
	}
	}
	```
4.  Deploy the dataset.

#### Task 5: Create and deploy a new pipeline
1.  Use the Azure portal to create a new pipeline, replacing the existing JSON text with the following (this can be copied from **E:\\Labfiles\\Lab09\\Exercise3\\JsonCmd3.txt**):
	```
	{
	"name": "ML Predictive Pipeline",
	"properties": {
	"activities": [
	{
	"name": "ML Activity",
	"type": "AzureMLBatchExecution",
	"inputs": [
	{
	"name": "Input from Azure Storage"
	}
	],
	"outputs": [
	{
	"name": "Output to Azure Storage"
	}
	],
	"linkedServiceName": "AzureMLLinkedService",
	"typeProperties":
	{
	"webServiceInput": "Input from Azure Storage",
	"webServiceOutputs": {
	"output1": "Output to Azure Storage"
	}
	},
	"policy": {
	"concurrency": 3,
	"executionPriorityOrder": "NewestFirst",
	"retry": 1,
	"timeout": "02:00:00"
	}
	}
	],
	"start": "2017-10-23T00:00:00Z",
	"end": "2017-10-24T00:00:00Z"
	}
	}
	```
> **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.
2.  Deploy the pipeline.

#### Task 6: Verify the ML data transformation
1.  Open the **Data Factory Diagram** and, if necessary, use the mouse to rearrange the pipelines so that they do not overlap.
2.  Open **Input from Azure Storage** and, under **Monitoring**, note the slices with a **Ready** or **In Progress** status (you might need to click **See more** to see these, depending on your current time in relation to UTC).
3.  Open **Output to Azure Storage** and, under **Monitoring**, note the slices with a **Ready** or **In Progress** status (you might need to click **See more** to see these, depending on your current time in relation to UTC).
4.  In Microsoft Azure Storage Explorer, open **speeddata*&lt;your name&gt;&lt;date&gt;***, and then view the contents of **cameralivedata/scored**, and note the dated folder (there might be more than one, depending on your time of day in relation to UTC).
5.  Open one of the folders, and note that there are **results.csv** files for each timed slice; note also that as the output slices are being generated every 15 minutes, you might need to click Refresh to see additional files.
6.  Open one of the **results.csv** files in Microsoft Excel, and note that the Scored Label Mean represents the predicted speed for that date and time, and at that camera location; in the real world, you would probably run the job every day, rather than every 15 minutes.
7.  Pause the **trafficwarehouse** Data Warehouse.

>**Result**: At the end of this exercise, you will have:
1.  Started your Data Warehouse.
2.  Obtained the API key and batch execution URL for the deployed ML model.
3.  Created an ML linked service in Data Factory.
4.  Created Azure Storage input and output datasets.
5.  Created and deployed a new pipeline.
6.  Verified the ML data transformation.

## Exercise 4: Use the Monitoring and Management app to track progress of a pipeline

### Scenario

You are using Azure Data Factory to automate the management of data associated with the traffic surveillance system. To ensure the reliability of your Data Factory pipelines, you’re going to test the monitoring and management capabilities provided with Azure Data Factory.

In this exercise, you will use the Diagram View, in the monitoring and management app, to see the status of the Traffic DF Copy Pipeline that you created in Exercise 1. You will then use filters and views to find specific status information on the inputs and outputs, and on the copy activity in the Traffic DF Copy Pipeline. Finally, you will set up and test the use of alerts with Data Factory pipelines.

The main tasks for this exercise are as follows:
1. Use the Diagram View to see overall job statuses
2. Use filters and views to find specific status information
3. Use alerts with Data Factory pipelines
4. Lab clean up

#### Task 1: Use the Diagram View to see overall job statuses
1.  Open **Monitor & Manage** for your Data Factory, and in the Diagram View at the top of the middle pane, use your mouse wheel or the + and - controls on the lower toolbar to zoom in and out; note the three pipelines from the previous exercises in this lab.
2.  In the **RESOURCE EXPLORER** tree view in the left pane, note the pipelines, datasets, linked services, and gateways from the previous exercises.
3.  In the **ACTIVITY WINDOWS** list at the bottom of the middle pane, note that by default all activities are listed in reverse chronological order, with the most recent activity at the top of the list.
4.  Open the top activity to show details in the Activity Window Explorer in the right pane; note the calendar view, and how the 15 minute data slices are represented; if you had used daily or weekly slices, for example, the calendar display would reflect this.
5.  Scroll down and look at the summary information for this activity, including start and end time, activity name, the associated pipeline and datasets, and status information.
6.  View the properties of this activity, including more detailed execution data.
7.  In the Diagram view, select **Input from Azure Storage**; note the properties shown for this dataset in the right-hand pane.
8.  Use the Script view, to see the JSON for this dataset.
9.  In the Diagram view, select **Output to Azure Storage**; note the Properties shown for this dataset in the right pane.
10. Use the Script view, to see the JSON for this dataset.
11. In the Diagram view, select **Traffic DF Copy Pipeline**, and on the Properties pane, note the start and end times for the pipeline.

#### Task 2: Use filters and views to find specific status information
1.  Click the filter for the **Pipeline** column.
2.  In the dialog box, select the **Traffic DF Copy Pipeline** check box, and click **OK**; note that using filters helps you see which activities are used by particular pipelines.
3.  In the **ACTIVITY WINDOWS** list, click the **Copy** icon.
4.  In Notepad, press Ctrl+V; verify that you can copy detailed activity log information for use in other applications, or for reporting.
5.  Click the filter for the **Status** column, and in the dialog box, select all the check boxes except for **Ready** and **None**, and then click **OK**; depending on how well your previous exercises performed, you might not actually see any activities at this point.
6.  Click the **Clear all filters from the activity windows list** icon.
7.  In the left-hand pane, click the **Monitoring views** icon (glasses).
8.  Expand **System Views**, and click **Failed activity windows**; note that using this view is an alternative to filtering the Activities list; again, you may not actually see any activities at this point.
9.  Under **System Views**, and click **In-progress activity windows**; again, depending on the state of your pipelines, you might not actually see any activities at this point.

#### Task 3: Use alerts with Data Factory pipelines
1.  Add an Alert with the following details:
 -   **Name**: Completed activities
 -   **Event**: select Activity Run Finished
 -   **Status**: select Succeeded
 -   **Email subscription admins**: selected
2.  Add a second Alert by right-clicking the **Input from Azure Storage** activity, and use the following details:
 -   **Name**: Completed backup input
 -   **Event**: Activity Run Started
 -   **Email subscription admins**: selected
3.  Open the inbox for the email address associated with your Azure subscription; steps will vary depending on the email provider.
4.  You should have several alert messages; note that these may take a few minutes to arrive; note the differences between the general alert (Completed activities), and the alert for a specific activity (Completed backup input).

#### Task 4: Lab clean up
1.  Delete the **CamerasRG** resource group.
2.  Close all open windows on the virtual machine, without saving if prompted.

>**Result**: At the end of this exercise, you will have:
1.  Used the Diagram View to see overall job statuses.
2.  Used filters and views to find specific status information.
3.  Used alerts with Data Factory pipelines.

**Question**: Why might you pause a deployed Data Factory pipeline?

**Question**: Why might you choose to use Service Principal Authentication with a Data Factory pipeline?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
