# Module 8: Performing Analytics with Azure SQL Data Warehouse

# Lab: Performing analytics with SQL Data Warehouse

### Scenario

You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that reads vehicle registration plates.

For this phase of the project, you are going to use data output from Azure Stream Analytics into SQL Data Warehouse to produce visually-rich reports from the stored data, as a first step in the identification of any patterns and trends in vehicle speeds at each camera location. You are then going to use Machine Learning against the data in SQL Data Warehouse to try to predict speeds at a given camera location at a given time; this information could be used to deploy patrol cars to potential hotspots ahead of time. Next, you want to be able to query the data in SQL Data Warehouse to find information such as the registration numbers of all cars that have never been caught speeding. Because you will be performing such queries at regular intervals, and the datasets are very large, it’s important that the data is properly structured to optimize the query processes. Finally, because a lot of the traffic data stored in SQL Data Warehouse includes personal and confidential details, it’s essential that the databases are protected from both accidental and malicious threats. You will, therefore, configure SQL Data Warehouse auditing and look at how to protect against various threats.

### Objectives
After completing this lab, you will be able to:
-   Visualize data stored in SQL Data Warehouse.
-   Use Machine Learning with SQL Data Warehouse.
-   Assess SQL Data Warehouse query performance and optimize database configuration.
-   Configure SQL Data Warehouse auditing and analyze threats.

### Lab Setup
Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

>**Note:** This lab uses the following resources from Lab 7 and earlier:
-   **Resource group**: CamerasRG
-   **Azure SQL Server**: trafficserver*&lt;your name&gt;&lt;date&gt;*
-   **Azure SQL Data Warehouse**: trafficwarehouse
-   **Machine Learning workspace**: Traffic

## Exercise 1: Visualize data stored in SQL Data Warehouse

### Scenario

You want to be able to identify any patterns and trends in vehicle speeds at each camera location. The SQL Data Warehouse holds data captured from speed cameras by using Stream Analytics, and you are going to use this data to produce visualizations in Power BI, as a first step in this analysis.

The main tasks for this exercise are as follows:
1. Install AzCopy
2. Prepare the environment
3. Upload data to Blob storage
4. Use PolyBase to transfer blob data to SQL Data Warehouse
5. Use Power BI to visualize the data

>**Note:** In this exercise, you will upload the speed data from a CSV file rather than streaming it from the cameras; the CSV file contains data for a longer period of time than is feasible by using streaming.

>**Note:** If you have completed Lab 4, you do not need to complete Exercise 1, Task 1; however, you will still need to check that all the virtual machines are running (as instructed in Step 1).

#### Task 1: Install AzCopy
1.  Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to **20776A-LON-DEV** as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2.  Install **AzCopy** from **https://docs.microsoft.com/en-us/azure/storage/storage-use-azcopy**.
3.  Add: **C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy** to the system path.

#### Task 2: Prepare the environment
1.  Start the **trafficwarehouse** Data Warehouse.
2.  Start SQL Server Management Studio and connect to **trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net**, that you created in Lab 7.
3.  Ensure that the database is set to **trafficwarehouse**, and then run the following statement to remove all the current data in the dbo.VehicleSpeed table:
	```
	DELETE FROM dbo.VehicleSpeed
	GO
	```
4.  Using the Azure portal, create a new Blob storage account named **speeddata*&lt;your name&gt;&lt;date&gt;***, using the **CamerasRG** Resource group, and the same location as you have used previously for your Data Lake Store.
5.  Add a blob container named **capturedspeeds** to the storage account.

#### Task 3: Upload data to Blob storage
1.  Make a note of the storage access key for the storage account.
2.  Use **AzCopy** to upload **E:\\Labfiles\\Exercise1\\Lab08\\SpeedData.csv** file to the **capturedspeeds** blob.
    You copy an example command from **E:\\Labfiles\\Lab08\\Exercise1\\AZCopyCmd.txt**.

#### Task 4: Use PolyBase to transfer blob data to SQL Data Warehouse
1.  In SQL Server Management Studio, open **E:\\Labfiles\\Lab08\\Exercise1\\Exercise1.sql**, and then use the following command to create a database scoped credential for accessing the **speeddata*&lt;your name&gt;&lt;date&gt;*** storage account (replace **&lt;storage account name&gt;** with the name of the account, and replace **&lt;storage account key&gt;** with the key you noted earlier):
	```
	CREATE DATABASE SCOPED CREDENTIAL SpeedDataCredentials
	WITH IDENTITY = '<storage account name>',
	SECRET = '<storage account key>';
	GO
	```
2.  Use the following command to create an external data source named **SpeedDataSource** that connects to the **capturedspeeds** container in the **speeddata*&lt;your name&gt;&lt;date&gt;*** Blob storage account using this credential (replace **&lt;storage account name&gt;** with the name of the account):
	```
	CREATE EXTERNAL DATA SOURCE SpeedDataSource
	WITH (
	TYPE = HADOOP,
	LOCATION = 'wasbs://capturedspeeds@<storage account name>.blob.core.windows.net',
	CREDENTIAL = SpeedDataCredentials
	)
	GO
	```
3.  Use the following command to create an external table named **ExternalSpeedData** that uses the data source and the **CommaSeparatedFileFormat** (from Lab 7) to read the location data from Blob storage:
	```
	CREATE EXTERNAL TABLE ExternalSpeedData (
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate VARCHAR(20) NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH (
	LOCATION='SpeedData.csv',
	DATA_SOURCE = SpeedDataSource,
	FILE_FORMAT = CommaSeparatedFileFormat,
	REJECT_TYPE = percentage,
	REJECT_VALUE = 2,
	REJECT_SAMPLE_VALUE = 1000
	)
	GO
	```
Note that the **REJECT\_VALUE** and **REJECT\_SAMPLE\_VALUE** settings allow for some minor corruption or malformed records in the CSV file.
4.  Use the following command to copy the data from the **ExternalSpeedData** table to the **dbo.VehicleSpeed** table:
	```
	INSERT INTO dbo.VehicleSpeed(CameraID, SpeedLimit, Speed, VehicleRegistration, WhenDate, WhenMonth)
	SELECT CameraID, SpeedLimit, Speed, VehicleRegistration, CONVERT(DATETIME, WhenDate, 103) AS WhenDate, WhenMonth
	FROM ExternalSpeedData
	GO
	```
Note that the value in the **WhenDate** column is a string, which is converted into a **DateTime** by specifying the appropriate format style. When you execute this command you might get one or two messages about rejected rows; you can ignore these errors.

#### Task 5: Use Power BI to visualize the data
1.  Using the Azure portal, open the blade for the **trafficwarehouse** database, and then use the option to open the database in PowerBI.
2.  In Power BI, if prompted, sign in using the Power BI account credentials that you've used in previous labs.
3.  When connecting to Azure SQL Data Warehouse, accept the default server and database name, and use the Username **student**, with Password **Pa55w.rd**, as the sign-in credentials.
4.  In the Power BI datasets list, verify that a dataset named after your SQL Data Warehouse (**trafficwarehouse**) has been created, then in the actions list for the **trafficwarehouse** source, select the option to create a report.
5.  Create a Line Chart visualization, using **WhenDate** as the axis for the graph, **CameraID** as the legend, and **Speed** as the values.
6.  Use filters to select three or four speed cameras, otherwise you will exceed the maximum number of data points allowed on a Power BI graph.
7.  Expand the graph to fill the entire pane, and save the report as **Vehicle Speeds over Time by CameraID**.
8.  Use the filters option to select a single camera.
9.  Examine the graph, and notice how speeds vary throughout the day.
10. Examine the data for other cameras. You should see a similar pattern.
11. Note that, if the system was using a Stream Analytics job to send data as it was captured to the SQL Data Warehouse, you could click Refresh periodically to see the most recent data. As it stands, the data warehouse just contains an historical snapshot of speed camera data.
12. Close Power BI.

>**Result**: At the end of this exercise, you will have uploaded data to Blob storage, and then used PolyBase to transfer this blob data to SQL Data Warehouse. You will then use Power BI to visualize this data, and look for patterns in the data.

## Exercise 2: Use Machine Learning with SQL Data Warehouse

### Scenario

You want to preposition patrol cars at potential traffic hotspots. You will use the speed camera data held in SQL Data Warehouse, together with Machine Learning, to create a model that you use to predict traffic speeds for each camera for a given time of day. The model uses Decision Forest Regression. This form of regression runs relatively quickly over the volume of data used by this lab. Neural Network Regression might generate a slightly more accurate set of predictions, but might take a considerable amount of time to run. Additionally, remember that the model generates predictions, not cast-iron guarantees of behavior.

The main tasks for this exercise are as follows:
1. Create experiment
2. Create trained model
3. Deploy the Machine Learning web service
4. Generate predictions by using the Machine Learning web service in an application

#### Task 1: Create experiment
1.  Using the Azure portal, go to the **Traffic ML** workspace, and launch **Machine Learning Studio**; sign in if prompted.
2.  Create a new, blank experiment.
3.  From the **Data Input and Output** group in the toolbox, add an **Import Data** module to the experiment.
4.  Set the properties of the Import Data Module as follows:
 -   **Data Source**: Azure SQL Database
 -   **Database server name**: trafficserver*&lt;your name&gt;&lt;date&gt;*.database.windows.net
 -   **Database name**: trafficwarehouse
 -   **User name**: student
 -   **Password**: Pa55w.rd
 -   **Database query**:
	```
	SELECT CameraID, Speed, DATEPART(hour, WhenDate) AS Hour, DATEPART(weekday, WhenDate) AS Day FROM dbo.VehicleSpeed
	```
The query returns the Hour (0-23) and Day (1-7) rather than the date and time down to the nearest fraction of a millisecond, which would be much too low a level of granularity for generating a predictive model. We are more interested in knowing whether traffic will be slow between 5 and 6 PM on a Friday, rather than trying to guess what the speed will be at 17:37:42.6586 on a specific date.
5.  In the toolbox, in the **Data Transformation** group, expand **Sample and Split**, and add a **Split Data** module to the experiment.
6.  Connect the output of the **Import Data** module to the input of the **Split Data** module.
7.  In the properties for the **Split Data** module, set **Fraction of rows in the first output dataset** to **0.9**.
8.  In the **Machine Learning** group, in the toolbox, expand **Initialize Model**, expand **Regression**, and add a **Decision Forest Regression** module to the experiment.
9.  In the **Machine Learning** group, in the toolbox, expand **Train**, and add a **Train Model** module to the experiment.
10.  Connect the output from the **Decision Forest Regression** module to the left input of the **Train Model** module.
11.  Connect the left output of the **Split Data** module to the right input of the **Train Model** module.
12.  In the properties of the **Train Model** module, click **Launch column selector**, select **column names**, and enter **Speed**.
13.  Add a **Score Model** module to the experiment.
14. Connect the output of the **Train Model** module to the first input, and the second output of the **Split Data** **transformation** to the second input.
15. Save and run the experiment; note that the experiment might take 10-15 minutes to complete.
16. When the experiment has completed, visualize the output of the **Score Model** module.
17. Compare the values in the **Scored Labels Mean** column with the data in the **Speed** column. The **Scored Labels Mean** column contains predicted mean speeds made by the model for the specified camera, hour, and day, compared to the actual speeds recorded.

#### Task 2: Create trained model
1.  Save the trained model as **Decision Forest Trained Regression Model for Vehicle Speeds**.
2.  Remove the **Decision Forest Regression** module from the experiment.
3.  Remove the **Train Model** module from the experiment.
4.  In the toolbox, from the **Trained Models** group, add the **Decision Forest Trained Regression Model for Vehicle Speeds** module.
5.  Connect the output of the **Decision Forest Trained Regression Model for Vehicle Speeds** module to the first input of the **Score Model** module.
6.  Set up the **web service** for the model.
7.  Delete the connection from the **Web Service Input** module to the **Split Data Transformation**, and then connect the **Web Service Input** module to the second input of the **Score Model** module.
8.  Save and run the experiment again (to validate the changes).

#### Task 3: Deploy the Machine Learning web service
1.  Deploy the model as a web service, using the Classic option, and make a note of the API key generated for the web service.
2.  Click the **New Web Services Experience** link, and then configure the endpoint to have a meaningful description (**Vehicle speed prediction web service**).
3.  Use the **QuickStart** option in the toolbar, go to **Use endpoint**, and then make a note of the **Request-Response URL**.

#### Task 4: Generate predictions by using the Machine Learning web service in an application
1.  Using Visual Studio, open the **VehicleSpeedPredictor** solution in the **E:\\Labfiles\\Lab08\\Exercise2** folder.
2.  Edit the **app.config** file for the project, to replace the APIKey value **&lt;YourApiKey&gt;** with the API key of the web service that you noted previously.
3.  In app.config, also replace the URL value **&lt;YourEndpointUrl&gt;** with the **Request-Response URL** that you noted previously.
4.  Replace the **&** character near the end of the URL (before the text **format=swagger**) with the sequence **&\#x26;**. This is necessary because the **&** character is interpreted as an escape character in XML rather than a literal. The sequence **&\#x26;** generates a literal **&**.
5.  Build the application, and then run it using **Start Without Debugging**.
6.  At the **Camera ID** prompt, type any number between 0 and 499, and then press Enter.
7.  At the **Hour** prompt, type any number between 0 and 23, and then press Enter.
8.  At the **Day** prompt, type any number between 1 and 7 (where day 1 is Sunday, and day 7 is Saturday), and then press Enter.
9.  After a few moments, the predicted speed at that time for that camera will be displayed.
10. Run the application again, and experiment with other cameras, dates, and times.
11. Close Visual Studio.
12. In Internet Explorer, close the Power BI and web services tabs.

>**Result**: At the end of this exercise, you will have created a trained model, deployed this model as a web service, and then used this service in an application to generate traffic speed predictions for particular camera locations, at particular times of day, and for particular days of the week.

## Exercise 3: Assess SQL Data Warehouse query performance and optimize database configuration

### Scenario

You want to find the registration numbers of all cars that have never been caught speeding. One way to do this is to perform a query that finds the registration of every vehicle, and then remove the registration numbers of vehicles that have been caught speeding from this set. You will be performing this query at regular intervals, so it’s important that the data is structured to optimize the processes performed to find the required information. The details of all vehicle registrations are held in the VehicleOwner table in the data warehouse; the vehicle speed information is stored in the VehicleSpeed table (from the previous exercises).

The main tasks for this exercise are as follows:
1. Assess performance of a baseline query
2. Assess query performance when using replicated tables
3. Assess query performance when distributing data by vehicle number
4. Assess query performance when using columnstore
5. Assess query performance when distributing linked data to the same node

#### Task 1: Assess performance of a baseline query
1.  Using SQL Server Management Studio, ensure that you are still connected to **trafficserver*&lt;your name&gt;&lt;date&gt;***, and that **trafficwarehouse** is selected, and then open **E:\\Labfiles\\Lab08\\Exercise3\\Exercise3.sql**.
2.  Execute the following command:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
3.  Wait for the query to finish; the query should fetch 7,187,718 rows.
4.  Repeat the query another couple of times.
5.  In the Azure portal, open the **Monitoring** blade for **trafficwarehouse**, and open the **Query Activity** graph.
6.  Order the queries list by time—with the most recent at the top—and then open the most recent query that was run by the Student login.
7.  Look at the query text; note that the text matches the query you performed previously.
8.  Open the query plan; note that the plan lists five steps (numbered 0 to 4) that are the steps performed by SQL Data Warehouse to run the query.
9.  Open the first **OnOperation** operation, and then look at the query step command; note that the query step text shows that SQL Data Warehouse creates a temporary table for storing vehicle registration numbers, and that this table is created on every node in the data warehouse.
10.  Now open the **BroadcastMoveOperation** operation, and look at the query step command; note that the Query Text step shows that **VehicleOwner** data for vehicles that have been caught speeding is copied from every node in the data warehouse to the temporary table. This is a potentially expensive operation, because it could involve moving lots of data between nodes.
11.  Open the **ReturnOperation** operation, and then look at the query step command; note that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding. The details from each node are aggregated and returned as the overall results.
12. Finally, open the second **OnOperation** operation, and then look at the query step command; note that the query step text deletes the temporary table from each node when the query has finished.
13. The temporary table and data broadcast operations are required because the vehicle registration information and the vehicle speed information are both distributed throughout the nodes in the data warehouse. While this distribution enables SQL Data Warehouse to manage the load more evenly and reduce the chances of contention, it causes extra work and data movement for some queries. If you perform these queries often, you should consider modifying the distribution policies of the tables used.

#### Task 2: Assess query performance when using replicated tables
1.  In SQL Server Management Studio, execute the following command:
	```
	SELECT T.name, P.distribution_policy_desc
	FROM sys.pdw_table_distribution_properties P
	JOIN sys.tables T
	ON P.object_id = T.object_id
	GO
	```
2.  This query displays the name of each table in the data warehouse and the distribution policy it uses. Note that the **VehicleOwner** table implements the **ROUND\_ROBIN** policy. The **VehicleSpeed** table is hashed (you should recall that the **CameraID** is used as the hash column).
3.  Execute the following command:
	```
	DBCC PDW_SHOWSPACEUSED('VehicleOwner')
	GO
	```
4.  This command shows how much space the **VehicleOwner** table occupies. The statistics in the various "space" columns are all measured in KB. The **VehicleOwner** table currently consumes between 12 and 12.5 MB in each node. In general, if a relatively static fact table is smaller than 2 GB, consider implementing it as a replicated table. This might reduce the chances of data movement, because the same data is automatically available on every node.
5.  Execute the following command:
	```
	CREATE TABLE VehicleOwner2
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	WITH
	(
	DISTRIBUTION = REPLICATE
	)
	GO
	INSERT INTO VehicleOwner2
	SELECT *
	FROM VehicleOwner
	GO
	```
6.  These commands create a new version of the **VehicleOwner** table (VehicleOwner2) that is replicated. The **INSERT** statement copies the data into this new table.
7.  Execute the following command:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
 Verify that this query still returns 7,187,718 rows. However, it might not be any faster than the previous query (it might actually be a little slower, at least for the first time).
9.  Execute the query another couple of times.
10.  On the **Azure Portal Monitoring** blade for trafficwarehouse, reopen the **Query Activity** graph to refresh the data.
11.  Order the queries list by time with the most recent at the top, and open the most recent query that was run by the Student login.
12.  Look at the query text, and verify that the text matches the query you performed previously.
13.  Open the query plan, and note that the plan still comprises five steps (numbered 0 to 4).
14.  Open the first **OnOperation** operation, and look at the query step command; note that the query step text shows that SQL Data Warehouse is still creating a temporary table for storing vehicle registration numbers on every node in the data warehouse.
15.  Open the **BroadcastMoveOperation** operation, and then look at the query step command; note that the Query Text step shows that **VehicleOwner** data for vehicles that have been caught speeding is still being copied from every node in the data warehouse to the temporary table.
16.  Open the **ReturnOperation** operation, and then look at the query step command; note that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding. Note that one reason why the performance has not improved—and possibly why it has likely got worse—is because the **ReturnOperation** in Step 3 has to compare data in the temporary table against more data in each node. This is because the **VehicleRegistration** table in each node now contains the complete set of vehicle registrations rather than a smaller subset.
17. Open the second **OnOperation** operation, and then look at the query step command; note that the query step text deletes the temporary table from each node when the query has finished.
18. The problem is that, although the **VehicleOwner** data is replicated to each node, the **VehicleSpeed** data is distributed by speed camera ID. One possible idea is that you need to reorganize the speed data by vehicle registration number to ensure that the records for a single vehicle are all held together in the same node.

#### Task 3: Assess query performance when distributing data by vehicle number
1.  In SQL Server Management Studio, execute the following commands:
	```
	CREATE TABLE VehicleSpeed2
	(
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate DATETIME NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH
	(
	HEAP,
	DISTRIBUTION = HASH(VehicleRegistration),
	PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
	)
	GO
	INSERT INTO VehicleSpeed2
	SELECT *
	FROM VehicleSpeed
	GO
	```
These statements create a copy of the **VehicleSpeed** table that is distributed by **VehicleRegistration** number rather than camera ID. The table is created as a **HEAP** (as was the original table).
2.  Execute the following command:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed2 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
This is the same query as before, except that it now references the reorganized version of the **VehicleSpeed** table.
3.  Repeat the query another couple of times.
4.  On the **trafficwarehouse** blade, reopen the **Query Activity** graph to refresh the data.
5.  Order the queries list by time with the most recent at the top, and open the most recent query that was run by the Student login.
6.  Look at the query text, and verify that the text matches the query you performed previously.
7.  Open the query plan, and note that the plan still comprises five steps (numbered 0 to 4).
8.  Open the first **OnOperation** operation, and then look at the query step command; note that the query step text shows that SQL Data Warehouse is still creating a temporary table for storing vehicle registration numbers on every node in the data warehouse.
9.  Open the **BroadcastMoveOperation** operation, and then look at the query step command; note that the Query Text step shows that **VehicleOwner** data for vehicles that have been caught speeding is still being copied from every node in the data warehouse to the temporary table.
10.  Open the **ReturnOperation** operation, and then look at the query step command; note again that this step uses the data in the temporary table in each node to find vehicles that have not been caught speeding.
11.  Open the second **OnOperation** operation, and then look at the query step command; note that the query step text deletes the temporary table from each node when the query has finished.
12. You suspect that implementing the **VehicleSpeed** table as a heap might be the problem. Perhaps it’s better to index this data as a **ColumnStore** structure.

#### Task 4: Assess query performance when using columnstore
1.  In SQL Server Management Studio, execute the following command:
	```
	CREATE TABLE VehicleSpeed3
	(
	CameraID VARCHAR(10) NOT NULL,
	SpeedLimit INT NOT NULL,
	Speed INT NOT NULL,
	VehicleRegistration VARCHAR(7) NOT NULL,
	WhenDate DATETIME NOT NULL,
	WhenMonth INT NOT NULL
	)
	WITH
	(
	DISTRIBUTION = HASH(VehicleRegistration),
	PARTITION (WhenMonth RANGE FOR VALUES(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
	)
	GO
	INSERT INTO VehicleSpeed3
	SELECT *
	FROM VehicleSpeed
	GO
	```
2.  Execute the following command:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner2 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed3 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
Note that this time the speed might be much slower than in previous examples.
3.  Repeat the query another couple of times.
4.  On the **trafficwarehouse** blade, reopen the **Query Activity** graph to refresh the data.
5.  Order the queries list by time with the most recent at the top, and open the most recent query that was run by the Student login.
6.  Look at the query text, and verify that the text matches the query you performed previously.
7.  Open the query plan, and note that the plan still comprises five steps (numbered 0 to 4).
8.  Open the first **OnOperation** operation, and look at the query step command; note that the query step text shows that SQL Data Warehouse is still creating a temporary table.
9.  Open the **BroadcastMoveOperation** operation, and then look at the query step command; note that the Query Text step shows that data is still being copied from every node in the data warehouse to the temporary table.
10.  Open the **ReturnOperation** operation, and look at the query step command; note again that this step uses the data in the temporary table in each node.
11.  Open the second **OnOperation** operation, and look at the query step command; note that the query step text deletes the temporary table from each node when the query has finished.
12. You realize that the issue might arise because of the way in which the query is performed. The query optimizer has to drive the query from the replicated table to find all **VehicleOwner** records that have no matching speed records. The query fetches the speed data from all nodes to compare against a single instance of the replicated vehicle registration table. If the query could drive from the **VehicleSpeed** table instead, it could quickly look up the necessary data locally in each node and avoid the data movement. The idea, therefore, is to ensure that the data in the **VehicleOwner** and **VehicleSpeed** tables are distributed in the same way.

#### Task 5: Assess query performance when distributing linked data to the same node
1.  In SQL Server Management Studio, execute the following command:
	```
	CREATE TABLE VehicleOwner3
	(
	VehicleRegistration VARCHAR(7) NOT NULL,
	Title VARCHAR(30) NOT NULL,
	Forename VARCHAR(30) NOT NULL,
	Surname VARCHAR(30) NOT NULL,
	AddressLine1 VARCHAR(50) NOT NULL,
	AddressLine2 VARCHAR(50) NOT NULL,
	AddressLine3 VARCHAR(50) NOT NULL,
	AddressLine4 VARCHAR(50) NOT NULL
	)
	WITH
	(
	DISTRIBUTION = HASH(VehicleRegistration)
	)
	GO
	INSERT INTO VehicleOwner3
	SELECT *
	FROM VehicleOwner
	GO
	```
These commands create another version of the **VehicleOwner** table, hashed by registration number. This matches the distribution policy of the **VehicleSpeed** table, so vehicle ownership and speed records for any given vehicle should now be held in the same node in the data warehouse.
2.  Execute the following command:
	```
	SELECT V.VehicleRegistration
	FROM dbo.VehicleOwner3 V
	WHERE V.VehicleRegistration NOT IN
	(
	SELECT S.VehicleRegistration
	FROM dbo.VehicleSpeed3 S
	WHERE S.Speed > S.SpeedLimit
	)
	ORDER BY V.VehicleRegistration
	GO
	```
3.  Repeat the query another couple of times.
4.  On the **Azure Portal Monitoring** blade, reopen the **Query Activity graph**, to refresh the data.
5.  Order the queries by time, with the most recent at the top, and open the most recent query that was run by the Student login.
6.  Look at the query text, and verify that the text matches the query you performed in Step 3.
7.  Look at the query plan, and note that this time the plan contains a single **ReturnOperation** step.
8.  Open the **ReturnOperation** operation, and look at the query step command; note that this step shows that the data is retrieved from each node without creating a temporary table or moving data between nodes.

>**Result**: At the end of this exercise, you will have run a series of queries against SQL Data Warehouse, assessed how each query is processed, and reconfigured the data structure several times to see the impact on the query process.

## Exercise 4: Configure SQL Data Warehouse auditing and analyze threats

### Scenario

You want to ensure that the traffic data stored in SQL Data Warehouse, which includes personal and confidential details, is properly protected from both accidental and malicious threats; you will, therefore, configure SQL Data Warehouse auditing and look at how to protect against various threats.

The main tasks for this exercise are as follows:
1. Enable auditing and threat detection
2. Generate audit and threat test data
3. View audit logs and alerts
4. Monitor login failures
5. Lab cleanup

#### Task 1: Enable auditing and threat detection
1.  Using the Azure portal, enable auditing and threat detection for the **trafficwarehouse** database.
2.  Create a new storage account for holding audit records, called **auditdata*&lt;your name&gt;&lt;date&gt;***.
3.  View the types of events that are audited, and the types of threats that might be detected.
4.  Enable all audited events, and all threat detection types.
5.  Configure the system to send alerts to your own email address, or the email address of your Azure Learning Pass account, and to service and co-administrators.

#### Task 2: Generate audit and threat test data
1.  Using Visual Studio 2017, open the **Threaten** solution in the **E:\\Labfiles\\Lab08\\Exercise4** folder.
2.  Edit the **app.config** file, and in the **Server settings** section, insert your database server name.
3.  View the code in the **RunQuery** method; this is a classic example of a SQL injection vulnerability. The code runs a SELECT statement to retrieve data using criteria that is specified by the user. The code makes no attempt to validate the user input, and the user can provide a string that contains all sorts of “nasty stuff”.
4.  Run the application, without debugging.
5.  At the prompt, type **AAA 111**. The application will attempt to find how many stolen vehicle records there are for this registration (0).
6.  Run the application again, and enter **HSZ 883**. This vehicle has been reported stolen once.
7.  Run the application again, and enter **H%**. This time the application tells you how many times vehicles that have a registration number that begins with the letter H have been stolen (59372).
8.  Run the application again, and enter the following text on a single line:
	```
	H%'; DROP TABLE Test; --
	```
This query returns the same number of rows, but also causes a DROP TABLE command to be run; the DROP TABLE command is effectively appended to the end of the SELECT statement. The first quote in the data finishes off the string expected by the SELECT statement. The semicolon starts a new statement, and the "--" starts a comment, which causes the final quote character in the string added by the application to be ignored, so that it doesn't cause a syntax error that would make the command fail.
9.  This is extremely dangerous. The application has effectively provided a back door into your database and a malicious user could access your data.
10.  Fortunately, enabling threat detection causes SQL Data Warehouse to recognize this situation, and the injected command (DROP TABLE Test) will not be run.

#### Task 3: View audit logs and alerts
1.  Open your email. You should have a message from SQL Data Warehouse warning you that a potential SQL injection attack has occurred.
2.  Note that this email includes information on:
 -   The target Azure subscription
 -   The target Azure SQL Server and the database
 -   The IP address of the source of the attack
 -   The date and time of the attack
 -   Potential causes and recommendations
3.  In the Azure portal, view the audit logs for **trafficwarehouse**; you should see a list of all recent actions and queries that have been performed.
4.  Near the top of the list, you should see an entry with event type **DataAccess**, and another of type **SchemaChanges**; the **ACTION STATUS** of both records should be **Failure**. This indicates that SQL Data Warehouse detected a problem and did not run these statements.
5.  Open the **SchemaChanges** record, and examine the statement, which should look like this:
	```
	SELECT COUNT(*) FROM dbo.StolenVehicle WHERE VehicleRegistration LIKE 'H%'; DROP TABLE Test; -- '
	```
6.  You see how the user input has been appended to the query to perform the DROP TABLE operation.
7.  Examine older statements from previous runs of the application. These should all have an action status of success, and indicate operations that were performed successfully.

#### Task 4: Monitor login failures
1.  Return to Visual Studio, and edit the **app.config** file to change the **Password** setting to a random value.
2.  Run the application again, and enter **H%** when prompted; the application should stop with a “Login failed” exception.
3.  In the Azure portal, refresh the audit records list to see the most recent records.
4.  Verify that you now see a Login record with an Action Status of failure.
5.  Open the failure record; note that the record details include the IP address of the client application. Such information is useful so that, if necessary, you could block this IP address in the firewall of the database server.

#### Task 5: Lab cleanup
1.  In the Azure portal, close the **Audit** record, and **Audit records** blades for trafficwarehouse, and then pause the **trafficwarehouse** database.
2.  Close Visual Studio.

>**Result**: At the end of this exercise, you will have enabled auditing, and used a sample application that includes a SQL injection vulnerability to attempt to attack the data warehouse. You will also have examined the audit logs, including identifying login failures.

**Question**: If you have an example big data application in your own organization, can you list any strategies that are currently used to minimize data movement?

**Question**: Based on your own experience, and your organization’s data structures, can you think of scenarios where you would *not* enable auditing and/or threat detection?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
