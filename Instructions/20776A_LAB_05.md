
# Module 5: Processing big data using Azure Data Lake Analytics

# Lab: Processing big data using Data Lake Analytics

### Scenario
You work for Adatum as a data engineer, and you have been asked to build a traffic surveillance system for traffic police. This system must be able to analyze significant amounts of dynamically streamed data, captured from speed cameras and automatic number plate recognition (ANPR) devices, and then crosscheck the outputs against large volumes of reference data holding vehicle, driver, and location information. Fixed roadside cameras, hand-held cameras (held by traffic police), and mobile cameras (in police patrol cars) are used to monitor traffic speeds and raise an alert if a vehicle is travelling too quickly for the local speed limit. The cameras also have built-in ANPR software that can read vehicle registration plates.

For this phase of the project, you are going to use Data Lake Analytics to calculate the average speeds detected by speed cameras, use data joins with a Data Lake Analytics job to generate speeding notices linked to vehicle owner information, scale up the system to use Data Lake Analytics speed camera data stored in the cloud in Azure SQL Database, and use Power BI to present the average speeds using map visualizations.

### Objectives
After completing this lab, you will be able to:
-   Create and test a Data Lake Analytics job.
-   Use Data Lake Analytics with data joins.
-   Use Data Lake Analytics with SQL Database.
-   Use Data Lake Analytics to categorize data and present results using Power BI.

### Lab Setup
Estimated time: 90 minutes
Virtual machine: **20776A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Create and test a Data Lake Analytics job

### Scenario
You are going to use Data Lake Analytics to calculate the average speeds detected by speed cameras, across a city area. In this exercise, you will use Data Lake Analytics to analyze speed camera data, and calculate the rolling average speed over the previous 25 observations for each speed camera.

The main tasks for this exercise are as follows:
1. Prepare data files for a local Data Lake Analytics instance
2. Create a new Data Lake project
3. Run a Data Lake Analytics job locally
4. Create a Data Lake Analytics account
5. Prepare data files for a cloud Data Lake Analytics instance
6. Run a Data Lake Analytics job in the cloud
7. Edit a Data Lake Analytics job to add rolling averages
8. Run the Data Lake Analytics updated job

#### Task 1: Prepare data files for a local Data Lake Analytics instance
1.  Using File Explorer, go to the **C:\\Users\\Student\\AppData\\Local\\USQLDataRoot** folder. This is the folder used by the local instance of ADLA for holding ADLS data during testing.
2.  Copy the **speeds** folder and **subfolders** from **D:\\Labfiles\\Lab05\\## Exercise 1** into the **USQLDataRoot** folder. This folder contains sample speed camera data for three hours (examine the folders and the file contents—the files are in CSV format).

#### Task 2: Create a new Data Lake project
1.  Using Visual Studio 2017, create a new Data Lake project named **CameraSpeeds** using the U-SQL project template.
2.  The job will analyze the data in a single file, **speeds1.csv**, in the **/speeds/2017/08/30/16 folder**. You create an extractor to retrieve the data from this file (this requires defining fields for the contents of the file).
3.  Your Script.usql should be similar to the following (this code can be copied from **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob1.usql**):
	```
	@cameraData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime
	FROM "/speeds/2017/08/30/16/speeds1.csv"
	USING Extractors.Csv(skipFirstNRows: 1);

	@avgspeeds =
	SELECT CameraID, AVG(Speed) AS AvgSpeed
	FROM @cameraData
	GROUP BY CameraID;

	OUTPUT @avgspeeds
	TO "/AverageSpeeds.csv"
	USING Outputters.Csv(outputHeader: true);
	```
Note that, in the first statement (@cameraData), because the source data first line contains headers, you need a command to skip over it. In the second statement (@avgspeeds), the code creates a rowset that reads the file, and calculates the average speed reported by each speed camera. In the third statement (@avgspeeds), the code saves the results to a CSV file named AverageSpeeds.csv, and includes the column header data.

#### Task 3: Run a Data Lake Analytics job locally
1.  Submit the job using the (Local) ADLA account in Visual Studio, and examine the simple job graph (hover the mouse over the SV1 Extract step to see the processing details).
2.  In the Local Run Results pane, open the **AverageSpeeds.csv** file to see the data generated by the job. It should contain 50 lines (one for each camera), displaying the camera ID and the average speed reported.

#### Task 4: Create a Data Lake Analytics account
1.  Using the Azure portal, create a new ADLA account, using the following details:
 -   **Name**: speedsdla*&lt;your name&gt;&lt;date&gt;*
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: use the same location as you used for the Data Lake Store that you created in Lab 2
 -   **Data Lake Store**: use the Data Lake Store that you created in Lab 2 (adls*&lt;your name&gt;&lt;date&gt;*)
 -   Leave all other settings at their defaults
2.  Wait until the account has deployed before continuing with the lab.

#### Task 5: Prepare data files for a cloud Data Lake Analytics instance
-   Upload the speeds folder tree and data files from the **E:\\Labfiles\\Lab05\\Exercise1** folder to the ADLS account (/speeds should be in the root folder). Do this manually in Data Explorer.

#### Task 6: Run a Data Lake Analytics job in the cloud
1.  In Visual Studio, in the toolbar, select the new ADLA account and submit the job to run in the cloud (you might need to log in to your Azure account using Server Explorer first).
2.  Verify that it runs as before.
3.  Use Data Explorer to verify that the AverageSpeeds.csv file has been created and that it contains the same data as before.

#### Task 7: Edit a Data Lake Analytics job to add rolling averages
1.  In Visual Studio, edit **Script.usql** to create another rowset that calculates the average speed over the last 25 observations for each speed camera.
2.  Edit **Script.usql** to output the **CameraID**, **observation time**, and **speed observed at that time** in addition to the **rolling average**.
3.  Your **Script.usql** pane should now be similar to the following (this code can be copied from **E:\\Labfiles\\Lab05\\Exercise1\\SpeedsJob2.usql**):
	```
	@rollingAvgSpeeds =
	SELECT CameraID, Time, Speed, AVG(Speed) OVER (PARTITION BY CameraID ORDER BY Time ROWS 25 PRECEDING) AS RollingAverage
	FROM @cameraData;
	OUTPUT @rollingAvgSpeeds
	TO "/RollingAverageSpeeds.csv"
	USING Outputters.Csv(outputHeader: true);
	```
Note that the first statement (@rollingAvgSpeeds) creates another rowset that calculates the average speed over the last 25 observations for each speed camera.
The second statement (OUTPUT @rollingAvgSpeeds) outputs the CameraID, observation time, and speed observed at that time—in addition to the rolling average—and writes the results out to another CSV file (with header).

#### Task 8: Run the Data Lake Analytics updated job
1.  Submit the job using the ADLA account. Notice that the job graph is more complex, and that the two rowsets are generated in parallel.
2.  Close the **Job View** panes, and then close the solution.
3.  Use Data Explorer in the ADLS account to examine the **RollingAverageSpeeds.csv** file. View the contents of this file.

>**Result**: At the end of this exercise, you will have prepared data files for a local Data Lake Analytics instance, created a new Data Lake project, and run this Data Lake Analytics job locally. Also, you will have created a Data Lake Analytics account, prepared data files for a cloud Data Lake Analytics instance, run this Data Lake Analytics job in the cloud—then edited the job to add rolling averages, and run the updated job.

## Exercise 2: Use Data Lake Analytics with data joins

### Scenario
You need to be able to generate and send out speeding tickets that are addressed to the registered owners of vehicles that have been caught exceeding the local speed limit by the cameras. In this exercise, you will add functionality to your Data Lake Analytics job to generate fines/summonses from speed data and vehicle owner address information.

The main tasks for this exercise are as follows:
1. Prepare data files for a local Data Lake Analytics instance
2. Create a new Data Lake project
3. Test the Data Lake Analytics job locally
4. Modify the Data Lake Analytics job to use joins
5. Test the modified Data Lake Analytics job locally
6. Run the updated Data Lake Analytics job in the cloud

#### Task 1: Prepare data files for a local Data Lake Analytics instance
1.  In the **C:\\Users\\Student\\AppData\\Local\\USQLDataRoot** folder, create a new folder called **vehicles**, and copy the **ownerdata.csv** file from **E:\\Labfiles\\Lab05\\Exercise2** into this folder.
2.  Examine this file—it contains the names and addresses of the owner for each vehicle.

#### Task 2: Create a new Data Lake project
1.  Using Visual Studio 2017, create a new Data Lake project named **SpeedAnalyzer** using the U-SQL project template.
2.  At the start of the script, add a statement to create a new database named **VehicleData** (if it doesn't already exist).
3.  Add a statement to create a table for holding speed camera information, and to index and hash the data by the speed camera ID.
4.  Add a statement to remove any existing data from the table.
5.  The job must only process CSV files for a specific hour on a selected date (this will be a batch job, and it must not process files that are still being written to by ASA); add a statement to declare a variable that uses pattern matching to specify the file name.
6.  Add a statement to create an extractor to retrieve speed data from the selected CSV file; this requires defining virtual fields for the date, hour, and filename components of the file in addition to the fields within the file (the first line contains headers, so skip over it).
7.  Add a statement to create a rowset that fetches the data for the specified date and time, and to ensure that only CSV files are read (this is specified using the Filename variable).
8.  Add a statement to declare variables that specify the date and hour for the data to process (there are three hours of data—16-18, for August 30).
9.  Add a statement to save the data to the Speeds table in the database for further analysis later.
10. By the end of this task, your **Script.usql** should be similar to the following (the code for this exercise can be copied from **E:\\Labfiles\\Lab05\\Exercise2\\VehiclesJobCmds.txt**):
	```
	CREATE DATABASE IF NOT EXISTS VehicleData;

	CREATE TABLE IF NOT EXISTS VehicleData.dbo.Speeds
	(
	CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	INDEX speedidx
	CLUSTERED (CameraID ASC)
	DISTRIBUTED BY HASH (CameraID)
	);

	TRUNCATE TABLE VehicleData.dbo.Speeds;

	DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{Filename}";

	@speedData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	Date DateTime,
	Hour int,
	Filename string
	FROM @speedCameraData
	USING Extractors.Csv(skipFirstNRows: 1);

	DECLARE @selectedDate DateTime = new DateTime(2017, 8, 30);
	DECLARE @hour int = 16; // Could also be 17 or 18
	@speedRecords =

	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
	FROM @speedData
	WHERE Date == @selectedDate AND Hour == @hour AND Filename LIKE "%.csv";

	INSERT INTO VehicleData.dbo.Speeds (CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time)
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, LocationLatitude, LocationLongitude, Time
	FROM @speedRecords;
	```


#### Task 3: Test the Data Lake Analytics job locally
1.  Run the job locally, and verify that it executes successfully.
2.  Using Server Explorer, expand **Azure**, expand **Data Lake Analytics**, expand **(Local)**, expand **U-SQL Databases**, expand **VehicleData**, expand **Tables**, right-click **dbo.Speeds**, and then click **Preview**. Verify that the Speeds table contains the data copied from the CSV file (it will be in a different order, due to the index).
3.  Note that the preview has run another Data Lake Analytics job that fetches the data in the table and returns it as a TSV file.

#### Task 4: Modify the Data Lake Analytics job to use joins
1.  Edit **Script.usql** to add a statement to create a table for recording fines and summonses.
2.  Add a statement to define a rowset that fetches the vehicle owner data from the **ownerdata.csv** file (and skip the headers in the first line).
3.  Add a statement to create a rowset that joins the speed data with the vehicle owner data, so that the application will know where to send fines/summonses. The code should combine owner title and names into a single string, and concatenate the address information into another single string (with semicolons between each line of the address).
4.  Add a statement to process each row of data from the speed camera as follows:
 -   If the speed shown is less than the speed limit plus 5 percent, do not raise a fixed penalty or summons.
 -   If the speed shown is between the speed limit plus 5 percent and the speed limit plus 30 mph, generate a fixed penalty fine.
 -   If the speed is greater than the speed limit plus 30 mph, then issue a summons to appear in court.
5.  Add a statement to save the details for fines and summonses to the Fines table in the database, and do not record data for legal traffic speeds.
6.  Add a statement to copy the data to an output file; note that you cannot read data from the VehicleData.dbo.Fines table as it could be mutating.
7.  By the end of this task, your **Script.usql** should be similar to the following:
	```
	CREATE TABLE IF NOT EXISTS VehicleData.dbo.Fines
	(
	CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	TimeCaught DateTime,
	PenaltyType string, // "P" for a Fixed Penalty (£100), "S" for a Summons (for excessive speeds, fine amount to be determined by a magistrate)
	PenaltyIssuedWhen DateTime,
	PenaltyIssuedTo string, // Full name of owner, including title
	OffenderAddress string, // Full address of the owner
	INDEX finesidx
	CLUSTERED (CameraID ASC)
	DISTRIBUTED BY HASH (CameraID)
	);

	@vehicleOwnerData =
	EXTRACT VehicleRegistration string,
	Title string,
	Forename string,
	Surname string,
	AddressLine1 string,
	AddressLine2 string,
	AddressLine3 string,
	AddressLine4 string
	FROM "/vehicles/ownerdata.csv"
	USING Extractors.Csv(skipFirstNRows: 1);
	@speedsAndOwners =
	SELECT S.CameraID AS CameraID, S.VehicleRegistration AS VehicleRegistration, S.Speed AS Speed, S.SpeedLimit AS SpeedLimit,
	S.Time AS TimeCaught, O.Title + " " + O.Forename + " " + O.Surname AS PenaltyIssuedTo,
	O.AddressLine1 + "; " + O.AddressLine2 + "; " + O.AddressLine3 + "; " + O.AddressLine4 AS OffenderAddress
	FROM @speedRecords AS S
	JOIN @vehicleOwnerData AS O
	ON S.VehicleRegistration == O.VehicleRegistration;

	@fines =
	SELECT *, DateTime.UtcNow AS PenaltyIssuedWhen,
	Speed >= SpeedLimit * 1.05 && Speed <= SpeedLimit + 30 ? "P" :
	Speed >= SpeedLimit + 30 ? "S" : "" AS PenaltyType
	FROM @speedsAndOwners;

	INSERT INTO VehicleData.dbo.Fines (CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress)
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
	FROM @fines
	WHERE !String.IsNullOrEmpty(PenaltyType);

	@results =
	SELECT CameraID, VehicleRegistration, Speed, SpeedLimit, TimeCaught, PenaltyType, PenaltyIssuedWhen, PenaltyIssuedTo, OffenderAddress
	FROM @fines
	WHERE !String.IsNullOrEmpty(PenaltyType);

	OUTPUT @results
	TO "/FinesAndSummonses.csv"
	USING Outputters.Csv(outputHeader: true);
	```

#### Task 5: Test the modified Data Lake Analytics job locally
1.  Run the job locally, and verify that it executes successfully.
2.  Examine the job graph and note that there are now more processing steps shown.
3.  Using Server Explorer, verify that the Fines table contains the details of any fines and summonses.

#### Task 6: Run the updated Data Lake Analytics job in the cloud
1.  Using the Azure portal, go to the ADLS account for the ADLA account created in Exercise 1.
2.  Use Data Explorer to create a new folder named **/vehicles** and upload the **ownerdata.csv** file into this folder.
3.  In Visual Studio, run the job again, using your ADLA account.
4.  Verify that it executes successfully.
5.  When the job has completed, examine the job graph again by hovering the mouse over the processing steps.
6.  In the Azure portal, go to the ADLS account and use Data Explorer to view the **FinesAndSummonses.csv** file generated by the job.
7.  In Visual Studio, use Server Explorer to examine the contents of the Speeds and Fines tables in the database created in the catalog for the ADLA account. You should see the same data as before, when the job was run locally.
8.  Verify that the Speeds table contains the same data as when the job was run locally.
9.  Verify that the Fines table contains the same details of fines and summonses.
>**Result**: At the end of this exercise, you will have prepared data files for a local Data Lake Analytics instance, created a new Data Lake project and tested this job locally, then modified the job to use joins, and tested the job locally before running it in the cloud.

## Exercise 3: Use Data Lake Analytics with SQL Database

### Scenario
You need to scale up the speed camera system to use speed camera data stored in the cloud in SQL Database, and then ensure that Data Lake Analytics can use this data in a secure manner. In this exercise, you will replace the “temporary” vehicle owner CSV file used in the previous exercise with data held in a SQL Database. You will create a data source for this database, using stored database credentials, and update the job to use this database rather than the CSV file.

The main tasks for this exercise are as follows:
1. Create a SQL Database
2. Upload data to SQL Database and add an index
3. Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell
4. Configure a Visual Studio-based Data Lake Analytics job to use stored credentials
5. Run a Data Lake Analytics job using stored credentials to access data in SQL Database

#### Task 1: Create a SQL Database
1.  Using the Azure portal, create a new SQL Database, with the following details:
 -   **Database name**: VehicleDB
 -   **Resource group (Use existing)**: CamerasRG
 -   **Location**: select the same location as you used for the Data Lake Store
 -   **Source**: Blank database
 -   **Server**: New server, with the following details:
 -   **Server name**: camerasdbs&lt;your name&gt;&lt;date&gt;
 -   **Server admin login**: student
 -   **Password**: Pa55w.rd
 -   **Location**: select the same location as you used for the Data Lake Store that you created in Exercise 1
 -   Ensure that the **Allow azure services to access server** check box is selected
 -   **Pricing tier**: Basic
 -   Leave all other settings at their defaults
2.  Wait until the database and server have deployed before continuing with the lab.
3.  Configure the server firewall to add your Client IP address, and enable the **Allow access to Azure Services** setting.

#### Task 2: Upload data to SQL Database and add an index
1.  Use SQL Server Management Studio to upload the Vehicle Owner data to the database **E:\\Labfiles\\Lab05\\Exercise3\\ownerdata.csv**; note that the first data row contains column names.
2.  Verify that the transfer is successful (the operation should upload 1782 records to the database). If you get data truncation warnings, you can ignore them.
3.  Verify that the data has been uploaded by viewing the top 1000 rows in the table.
4.  Add an index to the **dbo.ownerdata**, using the following SQL command (the SQL code can be copied from **E:\\Labfiles\\Lab05\\Exercise3\\Index.txt**):
	```
	CREATE INDEX RegIdx
	ON [dbo].[ownerdata]
	(
	[Vehicle Registration]
	)
	GO
	```

#### Task 3: Store a SQL Database credential in the Data Lake Analytics catalog using PowerShell
1.  Run **E:\\Labfiles\\Lab05\\Exercise3\\Setup.cmd**, as administrator.
2.  Start PowerShell ISE as administrator, and open **E:\\Labfiles\\Lab05\\Exercise3\\CreateCredential.ps1**, to open the following script:
	```
	# Install Azure PowerShell modules
	Install-Module AzureRM -AllowClobber;
	Import-Module AzureRM;

	# Login to Azure
	Login-AzureRmAccount;

	# Create credential
	New-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -CredentialName "VehicleOwnerDataCredential" -Credential (Get-Credential) -DatabaseHost "<name of your Azure SQL Database Server>.database.windows.net" -Port 1433;

	# Remove credential
	# Remove-AzureRmDataLakeAnalyticsCatalogCredential -AccountName "<name of your ADLA account>" -DatabaseName "VehicleData" -Name "VehicleOwnerDataCredential";
	```
3.  Edit the script, replacing **&lt;name of your ADLA account&gt;** with **speedsdla*&lt;your name&gt;&lt;date&gt;***.
4.  Edit the script, replacing **&lt;name of your Azure SQL Database Server&gt;** with **camerasdbs*&lt;your name&gt;&lt;date&gt;***.
5.  Save the updated script.
6.  Execute lines 1-3, to install the latest AzureRM cmdlets.
7.  If you get a warning message about installing modules from an untrusted repository, click **Yes to All**.
8.  Execute lines 4-6, to log in to Azure, using the details of the Microsoft account that is associated with your Azure Learning Pass subscription.
9.  Execute lines 7-9, to create a credential in your ADLA catalog that contains the connection information for connecting to your SQL Database.
10.  When prompted, enter the following credentials:
 -   **User name**: student
 -   **Password**: Pa55w.rd
11.  Note that these credentials, for accessing the database server, are stored and encrypted as part of the credential in the catalog.
12.  Note also that, in the script, the -DatabaseName parameter is the name of the database in the ADLA catalog that will hold the credential, and is *not* the name of the SQL database to which the credential connects!
13.  Note also that the script shows the Remove-AzureRmDataLakeAnalyticsCatalogCredential command (commented out); this is how you remove a credential from the catalog. You could use this command if you make a mistake when creating the credential; you can then recreate the credential with the correct information.

#### Task 4: Configure a Visual Studio-based Data Lake Analytics job to use stored credentials
1.  Using Visual Studio, open the **SpeedAnalyzer** starter project in the **E:\\Labfiles\\Lab05\\Exercise3** folder. This project is a sample solution for the U-SQL job that you created in Exercise 2.
2.  In **Script.usql**, locate the comment **// TODO: Switch the context to the VehicleData database in the catalog**, then on the next line, add the following USE statement for the credential store (the U-SQL commands can be copied from **E:\\Labfiles\\Lab05\\Exercise3\\UsqlCmds.txt**):
	```
	USE DATABASE VehicleData;
	```
3.  Locate the comment **// TODO: Drop the VehicleOwner Data Source if it already exists**, then on the next line, add the following statement:
	```
	DROP DATA SOURCE IF EXISTS VehicleOwner;
	```
4.  Locate the comment **// TODO: Create the VehicleOwner data source**, then on the next line, add the following statement to create the data source:
	```
	CREATE DATA SOURCE VehicleOwner
	FROM AZURESQLDB
	WITH (
	PROVIDER_STRING = "Database=VehicleDB;Trusted_Connection=False;Encrypt=True;",
	CREDENTIAL = VehicleOwnerDataCredential ,
	REMOTABLE_TYPES = (bool, byte, sbyte, short, int, long, decimal, float, double, string, DateTime)
	);
	```
5.  Locate the comment **// TODO: Fetch the vehicle owner data from the SQL database**, then on the next line, add the following statement to create a rowset from the dbo.ownerdata table held by the SQL database, using the data source you just created:
	```
	@vehicleOwnerData =
	SELECT [Vehicle Registration] AS VehicleRegistration, [Title] AS Title, [Forename] AS Forename, [Surname] AS Surname,
	[Address Line 1] AS AddressLine1, [Address Line 2] AS AddressLine2, [Address Line 3] AS AddressLine3, [Address Line 4] AS AddressLine4
	FROM EXTERNAL VehicleOwner
	LOCATION "dbo.ownerdata";
	```
6.  Leave the remainder of the script as it is; it is unchanged from Exercise 2, and performs the same tasks, but now uses the data retrieved from the SQL database instead of a flat file.

#### Task 5: Run a Data Lake Analytics job using stored credentials to access data in SQL Database
1.  In Visual Studio, submit the job using your ADLA account; note that this script must be run in the cloud, because the credential was created in the ADLA account, not locally.
2.  Verify that it runs successfully and, when the job has completed, examine the job graph by hovering the mouse over the processing steps.
3.  Use Data Explorer to view the **FinesAndSummonses.csv** file generated by the job, and verify that it contains the same data as before.
4.  In Visual Studio, use Server Explorer to examine the contents of the Speeds and Fines tables in the database created in the catalog for the ADLA account. You should see the same data as before, when the job was run locally using the data from the CSV file.
>**Result**: At the end of this exercise, you will have created a SQL Database, uploaded data to SQL Database and added an index. You will also have stored a SQL Database credential in the Data Lake Analytics catalog using PowerSheII, configured a Data Lake Analytics job to use stored credentials, and run this job using stored credentials to access data in SQL Database.

## Exercise 4: Use Data Lake Analytics to categorize data and present results using Power BI

### Scenario
You need to be able to present average speed data on a digital map, so that it’s easy to see traffic patterns across a city area. In this exercise, you will generate analytics for speed cameras, showing the number of cars that passed each camera in different speed “buckets” (< 10 mph, 10-29 mph, 30-49 mph, 50-69 mph, 70-99 mph, and 100+ mph); this analysis should incorporate current and historical data from the speed cameras. You will then visualize the data using the ARCGis map control in Power BI.

The main tasks for this exercise are as follows:
1. Create a new Data Lake project
2. Test the Data Lake Analytics job locally then run it in the cloud
3. Use Power BI to visualize the data
4. Lab closedown

#### Task 1: Create a new Data Lake project
1.  Using Visual Studio, create a new U-SQL project named **SpeedCameraAnalytics**.
2.  Define a rowset variable that extracts the data from the speed camera data files (uploaded in Exercise 1) under the **/speeds** folder. These files are organized by date and time, but you should ensure that you read every available file. Remember that each file is in CSV format, and has a header.
3.  Your **Script.usql** code should be similar to the following code (all the code in this exercise can be copied from **E:\\Labfiles\\Lab05\\Exercise4\\CameraJobCmds.txt**):
	```
	// Capture the historical and current data from all available files containing speed data
	DECLARE @speedCameraData string = @"/speeds/{Date:yyyy}/{Date:MM}/{Date:dd}/{Hour}/{*}.csv";

	// Specify the format of the data in the CSV files
	@speedData =
	EXTRACT CameraID string,
	VehicleRegistration string,
	Speed int,
	SpeedLimit int,
	LocationLatitude double,
	LocationLongitude double,
	Time DateTime,
	Date DateTime,
	Hour int
	FROM @speedCameraData
	USING Extractors.Csv(skipFirstNRows: 1);
	```
4.  Create a rowset that retrieves the data in the CameraID and Speed fields from these files; your **Script.usql** code should be similar to the following:
	```
	// Generate a rowset that reads the relevant data from the CSV files
	@cameraData =
	SELECT CameraID, Speed, LocationLatitude, LocationLongitude
	FROM @speedData;
	```
5.  Define a rowset that calculates, for each camera, the number of vehicles that were travelling at less than 10 mph (this is the first "bucket"); your **Script.usql** code should be similar to the following:
	```
	// Define a rowset that calculates the number of vehicles for each camera that were travelling at less than 10 mph (this is the first "bucket")
	@lessThan10 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS LessThan10
	FROM @cameraData
	WHERE Speed < 10
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
6.  Define another rowset that, for each camera, calculates the number of vehicles that were travelling at between 10 and 29 mph (this is the second "bucket); your **Script.usql** code should be similar to the following:
	```
	// Define a rowset that calculates the number of vehicles for each camera that were travelling at between 10 and 29 mph (this is the second "bucket)
	@between10And29 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between10And29
	FROM @cameraData
	WHERE Speed BETWEEN 10 AND 29
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
7.  Repeat the previous code for the following speed ranges—30-49 mph, 50-69 mph, 70-99 mph, 100 mph and more; your **Script.usql** code should be similar to the following:
	```
	// Define rowsets for the following speed ranges; 30-49 mph, 50-69 mph, 70-99 mph, 100 mph and over
	@between30And49 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between30And49
	FROM @cameraData
	WHERE Speed BETWEEN 30 AND 49
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

	@between50And69 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between50And69
	FROM @cameraData
	WHERE Speed BETWEEN 50 AND 69
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

	@between70And99 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Between70And99
	FROM @cameraData
	WHERE Speed BETWEEN 70 AND 99
	GROUP BY CameraID, LocationLatitude, LocationLongitude;

	@over99 =
	SELECT CameraID, LocationLatitude, LocationLongitude, COUNT(*) AS Over99
	FROM @cameraData
	WHERE Speed > 99
	GROUP BY CameraID, LocationLatitude, LocationLongitude;
	```
8.  Create another rowset that joins the buckets together over the CameraID column. You must use an OUTER join in case the bucket for a particular speed camera is empty (you don't want to discard all the data in the other buckets for that camera); your **Script.usql** code should be similar to the following:
	```
	// Join the speed buckets together, this requires an OUTER join, as one or more buckets for a camera could be null (empty)

	@speedAnalysis =
	SELECT a.CameraID, a.LocationLatitude, a.LocationLongitude, a.LessThan10, b.Between10And29, c.Between30And49, d.Between50And69, e.Between70And99, f.Over99
	FROM @lessThan10 AS a
	LEFT JOIN @between10And29 AS b
	ON a.CameraID == b.CameraID
	LEFT JOIN @between30And49 AS c
	ON a.CameraID == c.CameraID
	LEFT JOIN @between50And69 AS d
	ON a.CameraID == d.CameraID
	LEFT JOIN @between70And99 AS e
	ON a.CameraID == e.CameraID
	LEFT JOIN @over99 AS f
	ON a.CameraID == f.CameraID;
	Save the results to a CSV file named "/SpeedAnalysis.csv". Include a header; your Script.usql code should be similar to the following:
	OUTPUT @speedAnalysis
	TO "/SpeedAnalysis.csv"
	USING Outputters.Csv(outputHeader: true);
	```

#### Task 2: Test the Data Lake Analytics job locally then run it in the cloud
1.  Submit the job locally, and verify that it runs successfully.
2.  Examine the job graph to see the various processing steps.
3.  Examine the results in the **SpeedAnalysis.csv** file. The results should be a tabular list of cameras and the number of cars in each speed bucket.
4.  Run the job again, using your ADLA account.
5.  When the job has completed, examine the job graph again.

#### Task 3: Use Power BI to visualize the data
1.  Use Power BI Desktop to connect to your Data Lake Store, using the following URL (replacing **&lt;name of your Data Lake Store&gt;** with **adls*&lt;your name&gt;&lt;date&gt;***):
	```
	adl://<Your ADLS account>.azuredatalakestore.net/SpeedAnalysis.csv
	```
2.  If prompted to log in to Azure, authenticate using the Microsoft account that is associated with your Azure Learning Pass subscription.
3.  In Query Editor, ensure that the content is identified as Binary (the data in the file should appear).
4.  In Power BI, use the ArcGIS Maps for Power BI Visualization.
5.  Drag **LocationLatitude** to the **Latitude** box.
6.  Drag **LocationLongitude** to the **Longitude** box.
7.  Drag **CameraID** to the **Location** box.
8.  To make the names more meaningful, rename **LessThan10**, to **Less than 10 mph**.
9.  Rename each of the **BetweenXAndY** fields to **Between X and Y mph** (for example, Between10And29 will be renamed to Between 10 and 29 mph).
10.  Rename **Over99** to **Over 99 mph**.
11. Add the **Less than 10 mph** field, all the **Between X and Y mph fields**, and the **Over 99 mph** field to the **Tooltips** (in order, with Less than 10 at the top).
12. You should now have all the speed fields listed under **Tooltips**, and in ascending speed order.
13. Use Focus Mode, and view the map. Zoom in and hover over any speed camera to see the detected speed values for that camera.
14. Save the report as **SpeedCameraAnalysis.pbix**.

#### Task 4: Lab closedown
1.  Close Windows PowerShell ISE.
2.  Do not remove the Azure resources (resource group, Stream Analytics jobs, event hub, IoT hub, and storage); these resources will be used in later labs.
3.  Close Internet Explorer.

>**Result**: At the end of this exercise, you will have created a new Data Lake project, tested the job locally, run it in the cloud, and then used Power BI to visualize the data.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
